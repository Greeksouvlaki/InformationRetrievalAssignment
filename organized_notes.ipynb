{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8f1d149",
   "metadata": {},
   "source": [
    "\n",
    "# Εργασία Εργαστηρίου: Δημιουργία Μηχανής Αναζήτησης\n",
    "\n",
    "# Δημήτρης Σκούφης - 21390317\n",
    "\n",
    "## Περιγραφή\n",
    "Στην εργασία αυτή, αναπτύσσουμε μια απλή μηχανή αναζήτησης με σκοπό την κατανόηση των θεμελιωδών εννοιών της ανάκτησης πληροφορίας, της ευρετηρίασης, \n",
    "της κατάταξης και της αναζήτησης. Το notebook περιλαμβάνει τα βήματα:\n",
    "1. Συλλογή δεδομένων από το Wikipedia.\n",
    "2. Προεπεξεργασία κειμένου.\n",
    "3. Προετοιμασία για ευρετηρίαση (θα ακολουθήσει).\n",
    "\n",
    "---\n",
    "## Βήμα 1: Συλλογή Δεδομένων\n",
    "\n",
    "**Σκοπός:** Συλλογή άρθρων από το Wikipedia, τα οποία θα αποτελέσουν το dataset για τη μηχανή αναζήτησης.\n",
    "\n",
    "**Περιγραφή:**\n",
    "- Χρησιμοποιούμε τη βιβλιοθήκη `BeautifulSoup` για τη λήψη περιεχομένου από άρθρα.\n",
    "- Αποθηκεύουμε το περιεχόμενο σε μορφή JSON για μελλοντική χρήση.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Βήμα 1: Δημιουργία Dataset\n",
    "\n",
    "Ακολουθεί η λίστα με τα URLs των άρθρων από το Wikipedia, τα οποία επιλέχθηκαν για να παρέχουν ποικιλία σε κατηγορίες όπως επιστήμη, ιστορία, και τεχνολογία.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Λίστα URLs για συλλογή άρθρων από Wikipedia\n",
    "wikipedia_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Quantum_mechanics\",\n",
    "    \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "    \"https://en.wikipedia.org/wiki/Space_exploration\",\n",
    "    \"https://en.wikipedia.org/wiki/Computer_network\",\n",
    "    \"https://en.wikipedia.org/wiki/French_Revolution\",\n",
    "    \"https://en.wikipedia.org/wiki/World_War_II\",\n",
    "    \"https://en.wikipedia.org/wiki/Ancient_Greece\",\n",
    "    \"https://en.wikipedia.org/wiki/Renaissance\",\n",
    "    \"https://en.wikipedia.org/wiki/Seven_Wonders_of_the_World\",\n",
    "    \"https://en.wikipedia.org/wiki/Great_Barrier_Reef\",\n",
    "    \"https://en.wikipedia.org/wiki/Amazon_rainforest\",\n",
    "    \"https://en.wikipedia.org/wiki/Sahara\",\n",
    "    \"https://en.wikipedia.org/wiki/Classical_music\",\n",
    "    \"https://en.wikipedia.org/wiki/Impressionism\",\n",
    "    \"https://en.wikipedia.org/wiki/Film\",\n",
    "    \"https://en.wikipedia.org/wiki/Homer\",\n",
    "    \"https://en.wikipedia.org/wiki/Evolution\",\n",
    "    \"https://en.wikipedia.org/wiki/Human_anatomy\",\n",
    "    \"https://en.wikipedia.org/wiki/Climate\",\n",
    "    \"https://en.wikipedia.org/wiki/Biodiversity\",\n",
    "    \"https://en.wikipedia.org/wiki/Sustainable_development\",\n",
    "    \"https://en.wikipedia.org/wiki/Globalization\",\n",
    "    \"https://en.wikipedia.org/wiki/Blockchain\",\n",
    "    \"https://en.wikipedia.org/wiki/Financial_crisis_of_2007–2008\",\n",
    "    \"https://en.wikipedia.org/wiki/Vaccine\",\n",
    "    \"https://en.wikipedia.org/wiki/Mental_health\",\n",
    "    \"https://en.wikipedia.org/wiki/Nutrition\",\n",
    "    \"https://en.wikipedia.org/wiki/Pandemic\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Δημιουργία Web Crawler\n",
    "\n",
    "**Περιγραφή:** Το παρακάτω script χρησιμοποιεί τη βιβλιοθήκη `BeautifulSoup` για να συλλέξει δεδομένα από τη λίστα των URLs. \n",
    "Το περιεχόμενο κάθε άρθρου (τίτλος, URL, περιεχόμενο) αποθηκεύεται σε ένα αρχείο JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Λίστα URLs για συλλογή άρθρων από Wikipedia\n",
    "wikipedia_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Quantum_mechanics\",\n",
    "    \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "    \"https://en.wikipedia.org/wiki/Space_exploration\",\n",
    "    \"https://en.wikipedia.org/wiki/Computer_network\",\n",
    "    \"https://en.wikipedia.org/wiki/French_Revolution\",\n",
    "    \"https://en.wikipedia.org/wiki/World_War_II\",\n",
    "    \"https://en.wikipedia.org/wiki/Ancient_Greece\",\n",
    "    \"https://en.wikipedia.org/wiki/Renaissance\",\n",
    "    \"https://en.wikipedia.org/wiki/Seven_Wonders_of_the_World\",\n",
    "    \"https://en.wikipedia.org/wiki/Great_Barrier_Reef\",\n",
    "    \"https://en.wikipedia.org/wiki/Amazon_rainforest\",\n",
    "    \"https://en.wikipedia.org/wiki/Sahara\",\n",
    "    \"https://en.wikipedia.org/wiki/Classical_music\",\n",
    "    \"https://en.wikipedia.org/wiki/Impressionism\",\n",
    "    \"https://en.wikipedia.org/wiki/Film\",\n",
    "    \"https://en.wikipedia.org/wiki/Homer\",\n",
    "    \"https://en.wikipedia.org/wiki/Evolution\",\n",
    "    \"https://en.wikipedia.org/wiki/Human_anatomy\",\n",
    "    \"https://en.wikipedia.org/wiki/Climate\",\n",
    "    \"https://en.wikipedia.org/wiki/Biodiversity\",\n",
    "    \"https://en.wikipedia.org/wiki/Sustainable_development\",\n",
    "    \"https://en.wikipedia.org/wiki/Globalization\",\n",
    "    \"https://en.wikipedia.org/wiki/Blockchain\",\n",
    "    \"https://en.wikipedia.org/wiki/Financial_crisis_of_2007–2008\",\n",
    "    \"https://en.wikipedia.org/wiki/Vaccine\",\n",
    "    \"https://en.wikipedia.org/wiki/Mental_health\",\n",
    "    \"https://en.wikipedia.org/wiki/Nutrition\",\n",
    "    \"https://en.wikipedia.org/wiki/Pandemic\"\n",
    "]\n",
    "\n",
    "\n",
    "# Συνάρτηση για συλλογή δεδομένων από άρθρο Wikipedia\n",
    "def fetch_wikipedia_article(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Έλεγχος αν η σελίδα είναι προσβάσιμη\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Τίτλος άρθρου\n",
    "        title = soup.find('h1').text\n",
    "\n",
    "        # Περιεχόμενο άρθρου (παράγραφοι)\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = '\\n'.join([p.text for p in paragraphs if p.text])\n",
    "\n",
    "        return {'title': title, 'url': url, 'content': content}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Συλλογή δεδομένων από τα URLs\n",
    "collected_articles = []\n",
    "\n",
    "for url in wikipedia_urls:\n",
    "    article = fetch_wikipedia_article(url)\n",
    "    if article:\n",
    "        collected_articles.append(article)\n",
    "\n",
    "# Αποθήκευση δεδομένων σε JSON αρχείο\n",
    "output_file = 'wikipedia_articles.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(collected_articles, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Articles saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000994ea",
   "metadata": {},
   "source": [
    "\n",
    "## Αποτελέσματα Βήματος 1\n",
    "\n",
    "Η εκτέλεση του παραπάνω κώδικα δημιούργησε το αρχείο `wikipedia_articles.json` με τα εξής χαρακτηριστικά:\n",
    "- Αποθηκεύει δεδομένα από 28 άρθρα.\n",
    "- Περιλαμβάνει πληροφορίες όπως τίτλος, URL και περιεχόμενο.\n",
    "\n",
    "Παρακάτω φαίνεται ένα στιγμιότυπο της επιτυχούς εκτέλεσης:\n",
    "\n",
    "![success!](images/Στιγμιότυπο%20οθόνης%202025-01-11%20151157.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η εκτέλεση έγινε με επιτυχία και δημιουργήθηκε το αρχείο wikipedia_articles.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![success!](images/Στιγμιότυπο%20οθόνης%202025-01-11%20151157.png \"soup is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 2:  Προεπεξεργασία Κειμένου (Text Processing)\n",
    "\n",
    "# Καθαρισμός και επεξεργασία κειμένου για να είναι έτοιμο για αναζήτηση\n",
    "\n",
    "Θα εφαρμόσω tokenization, lemmatization, stop-word removal, καθαρισμός ειδικών χαρακτήρων"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Βήμα 2: Προεπεξεργασία Κειμένου\n",
    "\n",
    "**Σκοπός:** Καθαρισμός και επεξεργασία κειμένου ώστε να είναι κατάλληλο για αναζήτηση και ευρετηρίαση.\n",
    "\n",
    "**Εργασίες που εκτελούνται:**\n",
    "1. Μετατροπή κειμένου σε πεζά.\n",
    "2. Αφαίρεση ειδικών χαρακτήρων.\n",
    "3. Διαχωρισμός σε tokens (tokenization).\n",
    "4. Αφαίρεση λέξεων-κλειδιών (stop-word removal).\n",
    "5. Εύρεση βασικής μορφής λέξεων (lemmatization).\n",
    "\n",
    "Παρακάτω εφαρμόζονται τα παραπάνω βήματα στα δεδομένα που συλλέξαμε.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53813e9",
   "metadata": {},
   "source": [
    "\n",
    "## Αποτελέσματα Βήματος 2\n",
    "\n",
    "Μετά την επεξεργασία, δημιουργούνται:\n",
    "1. **`processed_articles.json`**: Περιέχει τα καθαρισμένα δεδομένα με tokens για κάθε άρθρο.\n",
    "2. **`articles_metadata.json`**: Περιέχει μεταδεδομένα όπως:\n",
    "   - Αριθμός tokens\n",
    "   - Αριθμός μοναδικών tokens\n",
    "\n",
    "Παρακάτω φαίνονται παραδείγματα αποτελεσμάτων:\n",
    "- **Πριν την επεξεργασία:** Το κείμενο περιλαμβάνει ειδικούς χαρακτήρες, κεφαλαία κ.λπ.\n",
    "- **Μετά την επεξεργασία:** Το κείμενο καθαρίζεται και διαχωρίζεται σε tokens.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
