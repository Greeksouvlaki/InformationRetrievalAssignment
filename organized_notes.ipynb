{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8f1d149",
   "metadata": {},
   "source": [
    "\n",
    "# Εργασία Εργαστηρίου: Δημιουργία Μηχανής Αναζήτησης\n",
    "\n",
    "# Δημήτρης Σκούφης - 21390317\n",
    "\n",
    "## Περιγραφή\n",
    "Στην εργασία αυτή, αναπτύσσουμε μια απλή μηχανή αναζήτησης με σκοπό την κατανόηση των θεμελιωδών εννοιών της ανάκτησης πληροφορίας, της ευρετηρίασης, \n",
    "της κατάταξης και της αναζήτησης. Το notebook περιλαμβάνει τα βήματα:\n",
    "1. Συλλογή δεδομένων από το Wikipedia.\n",
    "2. Προεπεξεργασία κειμένου.\n",
    "3. Προετοιμασία για ευρετηρίαση (θα ακολουθήσει).\n",
    "\n",
    "---\n",
    "## Βήμα 1: Συλλογή Δεδομένων\n",
    "\n",
    "**Σκοπός:** Συλλογή άρθρων από το Wikipedia, τα οποία θα αποτελέσουν το dataset για τη μηχανή αναζήτησης.\n",
    "\n",
    "**Περιγραφή:**\n",
    "- Χρησιμοποιούμε τη βιβλιοθήκη `BeautifulSoup` για τη λήψη περιεχομένου από άρθρα.\n",
    "- Αποθηκεύουμε το περιεχόμενο σε μορφή JSON για μελλοντική χρήση.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Βήμα 1: Δημιουργία Dataset\n",
    "\n",
    "Ακολουθεί η λίστα με τα URLs των άρθρων από το Wikipedia, τα οποία επιλέχθηκαν για να παρέχουν ποικιλία σε κατηγορίες όπως επιστήμη, ιστορία, και τεχνολογία.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Λίστα URLs για συλλογή άρθρων από Wikipedia\n",
    "wikipedia_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Quantum_mechanics\",\n",
    "    \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "    \"https://en.wikipedia.org/wiki/Space_exploration\",\n",
    "    \"https://en.wikipedia.org/wiki/Computer_network\",\n",
    "    \"https://en.wikipedia.org/wiki/French_Revolution\",\n",
    "    \"https://en.wikipedia.org/wiki/World_War_II\",\n",
    "    \"https://en.wikipedia.org/wiki/Ancient_Greece\",\n",
    "    \"https://en.wikipedia.org/wiki/Renaissance\",\n",
    "    \"https://en.wikipedia.org/wiki/Seven_Wonders_of_the_World\",\n",
    "    \"https://en.wikipedia.org/wiki/Great_Barrier_Reef\",\n",
    "    \"https://en.wikipedia.org/wiki/Amazon_rainforest\",\n",
    "    \"https://en.wikipedia.org/wiki/Sahara\",\n",
    "    \"https://en.wikipedia.org/wiki/Classical_music\",\n",
    "    \"https://en.wikipedia.org/wiki/Impressionism\",\n",
    "    \"https://en.wikipedia.org/wiki/Film\",\n",
    "    \"https://en.wikipedia.org/wiki/Homer\",\n",
    "    \"https://en.wikipedia.org/wiki/Evolution\",\n",
    "    \"https://en.wikipedia.org/wiki/Human_anatomy\",\n",
    "    \"https://en.wikipedia.org/wiki/Climate\",\n",
    "    \"https://en.wikipedia.org/wiki/Biodiversity\",\n",
    "    \"https://en.wikipedia.org/wiki/Sustainable_development\",\n",
    "    \"https://en.wikipedia.org/wiki/Globalization\",\n",
    "    \"https://en.wikipedia.org/wiki/Blockchain\",\n",
    "    \"https://en.wikipedia.org/wiki/Financial_crisis_of_2007–2008\",\n",
    "    \"https://en.wikipedia.org/wiki/Vaccine\",\n",
    "    \"https://en.wikipedia.org/wiki/Mental_health\",\n",
    "    \"https://en.wikipedia.org/wiki/Nutrition\",\n",
    "    \"https://en.wikipedia.org/wiki/Pandemic\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Δημιουργία Web Crawler\n",
    "\n",
    "**Περιγραφή:** Το παρακάτω script χρησιμοποιεί τη βιβλιοθήκη `BeautifulSoup` για να συλλέξει δεδομένα από τη λίστα των URLs. \n",
    "Το περιεχόμενο κάθε άρθρου (τίτλος, URL, περιεχόμενο) αποθηκεύεται σε ένα αρχείο JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Λίστα URLs για συλλογή άρθρων από Wikipedia\n",
    "wikipedia_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Quantum_mechanics\",\n",
    "    \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "    \"https://en.wikipedia.org/wiki/Space_exploration\",\n",
    "    \"https://en.wikipedia.org/wiki/Computer_network\",\n",
    "    \"https://en.wikipedia.org/wiki/French_Revolution\",\n",
    "    \"https://en.wikipedia.org/wiki/World_War_II\",\n",
    "    \"https://en.wikipedia.org/wiki/Ancient_Greece\",\n",
    "    \"https://en.wikipedia.org/wiki/Renaissance\",\n",
    "    \"https://en.wikipedia.org/wiki/Seven_Wonders_of_the_World\",\n",
    "    \"https://en.wikipedia.org/wiki/Great_Barrier_Reef\",\n",
    "    \"https://en.wikipedia.org/wiki/Amazon_rainforest\",\n",
    "    \"https://en.wikipedia.org/wiki/Sahara\",\n",
    "    \"https://en.wikipedia.org/wiki/Classical_music\",\n",
    "    \"https://en.wikipedia.org/wiki/Impressionism\",\n",
    "    \"https://en.wikipedia.org/wiki/Film\",\n",
    "    \"https://en.wikipedia.org/wiki/Homer\",\n",
    "    \"https://en.wikipedia.org/wiki/Evolution\",\n",
    "    \"https://en.wikipedia.org/wiki/Human_anatomy\",\n",
    "    \"https://en.wikipedia.org/wiki/Climate\",\n",
    "    \"https://en.wikipedia.org/wiki/Biodiversity\",\n",
    "    \"https://en.wikipedia.org/wiki/Sustainable_development\",\n",
    "    \"https://en.wikipedia.org/wiki/Globalization\",\n",
    "    \"https://en.wikipedia.org/wiki/Blockchain\",\n",
    "    \"https://en.wikipedia.org/wiki/Financial_crisis_of_2007–2008\",\n",
    "    \"https://en.wikipedia.org/wiki/Vaccine\",\n",
    "    \"https://en.wikipedia.org/wiki/Mental_health\",\n",
    "    \"https://en.wikipedia.org/wiki/Nutrition\",\n",
    "    \"https://en.wikipedia.org/wiki/Pandemic\"\n",
    "]\n",
    "\n",
    "\n",
    "# Συνάρτηση για συλλογή δεδομένων από άρθρο Wikipedia\n",
    "def fetch_wikipedia_article(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Έλεγχος αν η σελίδα είναι προσβάσιμη\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Τίτλος άρθρου\n",
    "        title = soup.find('h1').text\n",
    "\n",
    "        # Περιεχόμενο άρθρου (παράγραφοι)\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = '\\n'.join([p.text for p in paragraphs if p.text])\n",
    "\n",
    "        return {'title': title, 'url': url, 'content': content}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Συλλογή δεδομένων από τα URLs\n",
    "collected_articles = []\n",
    "\n",
    "for url in wikipedia_urls:\n",
    "    article = fetch_wikipedia_article(url)\n",
    "    if article:\n",
    "        collected_articles.append(article)\n",
    "\n",
    "# Αποθήκευση δεδομένων σε JSON αρχείο\n",
    "output_file = 'wikipedia_articles.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(collected_articles, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Articles saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000994ea",
   "metadata": {},
   "source": [
    "\n",
    "## Αποτελέσματα Βήματος 1\n",
    "\n",
    "Η εκτέλεση του παραπάνω κώδικα δημιούργησε το αρχείο `wikipedia_articles.json` με τα εξής χαρακτηριστικά:\n",
    "- Αποθηκεύει δεδομένα από 28 άρθρα.\n",
    "- Περιλαμβάνει πληροφορίες όπως τίτλος, URL και περιεχόμενο.\n",
    "\n",
    "Παρακάτω φαίνεται ένα στιγμιότυπο της επιτυχούς εκτέλεσης:\n",
    "\n",
    "![success!](images/Στιγμιότυπο%20οθόνης%202025-01-11%20151157.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η εκτέλεση έγινε με επιτυχία και δημιουργήθηκε το αρχείο wikipedia_articles.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![success!](images/Στιγμιότυπο%20οθόνης%202025-01-11%20151157.png \"soup is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 2:  Προεπεξεργασία Κειμένου (Text Processing)\n",
    "\n",
    "# Καθαρισμός και επεξεργασία κειμένου για να είναι έτοιμο για αναζήτηση\n",
    "\n",
    "Θα εφαρμόσω tokenization, lemmatization, stop-word removal, καθαρισμός ειδικών χαρακτήρων"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Βήμα 2: Προεπεξεργασία Κειμένου\n",
    "\n",
    "**Σκοπός:** Καθαρισμός και επεξεργασία κειμένου ώστε να είναι κατάλληλο για αναζήτηση και ευρετηρίαση.\n",
    "\n",
    "**Εργασίες που εκτελούνται:**\n",
    "1. Μετατροπή κειμένου σε πεζά.\n",
    "2. Αφαίρεση ειδικών χαρακτήρων.\n",
    "3. Διαχωρισμός σε tokens (tokenization).\n",
    "4. Αφαίρεση λέξεων-κλειδιών (stop-word removal).\n",
    "5. Εύρεση βασικής μορφής λέξεων (lemmatization).\n",
    "\n",
    "Παρακάτω εφαρμόζονται τα παραπάνω βήματα στα δεδομένα που συλλέξαμε.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53813e9",
   "metadata": {},
   "source": [
    "\n",
    "## Αποτελέσματα Βήματος 2\n",
    "\n",
    "Μετά την επεξεργασία, δημιουργούνται:\n",
    "1. **`processed_articles.json`**: Περιέχει τα καθαρισμένα δεδομένα με tokens για κάθε άρθρο.\n",
    "2. **`articles_metadata.json`**: Περιέχει μεταδεδομένα όπως:\n",
    "   - Αριθμός tokens\n",
    "   - Αριθμός μοναδικών tokens\n",
    "\n",
    "Παρακάτω φαίνονται παραδείγματα αποτελεσμάτων:\n",
    "- **Πριν την επεξεργασία:** Το κείμενο περιλαμβάνει ειδικούς χαρακτήρες, κεφαλαία κ.λπ.\n",
    "- **Μετά την επεξεργασία:** Το κείμενο καθαρίζεται και διαχωρίζεται σε tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![success!](images/Βήμα2.png \"proccessing is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 3: Ευρετήριο (Indexing)\n",
    "\n",
    "1. Σχεδιασμός και υλοποιήση: Δημιουργία δομής δεδομένων για αντιστοίχιση λέξεων με τα έγγραφα στα οποία εμφανίζονται\n",
    "\n",
    "2. Αποθήκευση: Υλοποίηση αποδοτικής μεθόδου αποθήκευσης για την παραπάνω δομή"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Σχεδιασμός Ευρετηρίου\n",
    "\n",
    "Το ανεστραμμένο ευρετήριο θα είναι μια Python dict όπου:\n",
    "\n",
    "Τα κλειδιά θα είναι οι μοναδικές λέξεις (tokens) από τα προεπεξεργασμένα άρθρα.\n",
    "Οι τιμές θα είναι λίστες με ID άρθρων που περιέχουν αυτή τη λέξη."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Φόρτωση των προεπεξεργασμένων άρθρων\n",
    "with open('processed_articles.json', 'r', encoding='utf-8') as file:\n",
    "    processed_articles = json.load(file)\n",
    "\n",
    "# Δημιουργία του ανεστραμμένου ευρετηρίου\n",
    "inverted_index = defaultdict(list)\n",
    "\n",
    "for article_id, article in enumerate(processed_articles):\n",
    "    for token in set(article['tokens']):\n",
    "        inverted_index[token].append(article_id)\n",
    "\n",
    "# Αποθήκευση του ευρετηρίου σε αρχείο\n",
    "with open('inverted_index.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(inverted_index, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Inverted index created and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Αποτελέσματα βήματος 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Περιγραφή Σχεδιασμού: Η χρήση defaultdict επιτρέπει εύκολη εισαγωγή νέων λέξεων χωρίς πρόσθετους ελέγχους\n",
    "2. Αποδοτικότητα: Το ευρετήριο αποθηκεύεται σε JSON μορφή για να είναι φορητό και αναγνώσιμο"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Παραδείγματα Δεδομένων\n",
    "\n",
    "* Λέξη: \"quantum\" -> [0, 1, 2] (IDs άρθρων που περιέχουν τη λέξη)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![success!](images/Βήμα3.png \"proccessing is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Αλλαγή Dataset\n",
    "\n",
    "## Άρθρα για το Marvel Cinematic Universe\n",
    "\n",
    "Αποφάσισα να αλλάξω Dataset γιατί άρχισε να αποκτάει ενδιαφέρον"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS D:\\Downloads\\Uni\\Ανάκτηση Πληροφορίας\\Εργασία> python search_interface.py\n",
    "Welcome to the Search Engine!\n",
    "Enter your query (or type 'exit' to quit):\n",
    "\n",
    "> hulk\n",
    "\n",
    "Choose retrieval method:\n",
    "1. Boolean Retrieval (AND, OR, NOT)\n",
    "2. Vector Space Model (VSM)\n",
    "3. Okapi BM25\n",
    "\n",
    "Enter your choice (1/2/3): 2\n",
    "VSM Query Tokens: ['hulk']\n",
    "\n",
    "Top Results:\n",
    "- Marvel Cinematic Universe (Score: 1.0000)\n",
    "- Marvel Studios (Score: 1.0000)\n",
    "- Iron Man (2008 film) (Score: 1.0000)\n",
    "- The Incredible Hulk (film) (Score: 1.0000)\n",
    "- Thor (film) (Score: 1.0000)\n",
    "\n",
    "> iron man\n",
    "\n",
    "Choose retrieval method:\n",
    "1. Boolean Retrieval (AND, OR, NOT)\n",
    "2. Vector Space Model (VSM)\n",
    "3. Okapi BM25\n",
    "\n",
    "Enter your choice (1/2/3): 1\n",
    "\n",
    "Available Boolean operations: AND, OR, NOT\n",
    "Enter Boolean operation: AND\n",
    "Query Tokens: ['iron', 'man']\n",
    "\n",
    "Top Results:\n",
    "- Marvel Cinematic Universe\n",
    "- Marvel Studios\n",
    "- Iron Man (2008 film)\n",
    "- The Incredible Hulk (film)\n",
    "- Iron Man 2\n",
    "- Thor (film)\n",
    "- Captain America: The First Avenger\n",
    "- The Avengers (2012 film)\n",
    "- Iron Man 3\n",
    "- Thor: The Dark World\n",
    "- Captain America: The Winter Soldier\n",
    "- Guardians of the Galaxy (film)\n",
    "- Avengers: Age of Ultron\n",
    "- Ant-Man (film)\n",
    "- Captain America: Civil War\n",
    "- Guardians of the Galaxy Vol. 2\n",
    "- Spider-Man: Homecoming\n",
    "- Black Panther (film)\n",
    "- Avengers: Infinity War\n",
    "- Captain Marvel (film)\n",
    "- Avengers: Endgame\n",
    "- Spider-Man: Far From Home\n",
    "- Black Widow (2021 film)\n",
    "- Shang-Chi and the Legend of the Ten Rings\n",
    "- Eternals (film)\n",
    "- Doctor Strange in the Multiverse of Madness\n",
    "- WandaVision\n",
    "- The Falcon and the Winter Soldier\n",
    "- Moon Knight (miniseries)\n",
    "- Ironheart (miniseries)\n",
    "\n",
    ">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
