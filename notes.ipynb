{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Εργασία Εργαστηρίου - Σχόλια\n",
    "\n",
    "Δημήτρης Σκούφης - 21390317 - ΧΠ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 1: Δημιουργία Dataset\n",
    "\n",
    "Βρήκα πολύ βασικά άρθρα για ορισμένες κατηγορίες καθώς η κλίμακα της μηχανής αναζήτησης θα είναι πολύ μικρή"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Λίστα URLs για συλλογή άρθρων από Wikipedia\n",
    "wikipedia_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Quantum_mechanics\",\n",
    "    \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "    \"https://en.wikipedia.org/wiki/Space_exploration\",\n",
    "    \"https://en.wikipedia.org/wiki/Computer_network\",\n",
    "    \"https://en.wikipedia.org/wiki/French_Revolution\",\n",
    "    \"https://en.wikipedia.org/wiki/World_War_II\",\n",
    "    \"https://en.wikipedia.org/wiki/Ancient_Greece\",\n",
    "    \"https://en.wikipedia.org/wiki/Renaissance\",\n",
    "    \"https://en.wikipedia.org/wiki/Seven_Wonders_of_the_World\",\n",
    "    \"https://en.wikipedia.org/wiki/Great_Barrier_Reef\",\n",
    "    \"https://en.wikipedia.org/wiki/Amazon_rainforest\",\n",
    "    \"https://en.wikipedia.org/wiki/Sahara\",\n",
    "    \"https://en.wikipedia.org/wiki/Classical_music\",\n",
    "    \"https://en.wikipedia.org/wiki/Impressionism\",\n",
    "    \"https://en.wikipedia.org/wiki/Film\",\n",
    "    \"https://en.wikipedia.org/wiki/Homer\",\n",
    "    \"https://en.wikipedia.org/wiki/Evolution\",\n",
    "    \"https://en.wikipedia.org/wiki/Human_anatomy\",\n",
    "    \"https://en.wikipedia.org/wiki/Climate\",\n",
    "    \"https://en.wikipedia.org/wiki/Biodiversity\",\n",
    "    \"https://en.wikipedia.org/wiki/Sustainable_development\",\n",
    "    \"https://en.wikipedia.org/wiki/Globalization\",\n",
    "    \"https://en.wikipedia.org/wiki/Blockchain\",\n",
    "    \"https://en.wikipedia.org/wiki/Financial_crisis_of_2007–2008\",\n",
    "    \"https://en.wikipedia.org/wiki/Vaccine\",\n",
    "    \"https://en.wikipedia.org/wiki/Mental_health\",\n",
    "    \"https://en.wikipedia.org/wiki/Nutrition\",\n",
    "    \"https://en.wikipedia.org/wiki/Pandemic\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Δημιουργία Web Crawler προγράμματος\n",
    "\n",
    "Έγραψα αυτό το script που με την χρήση του BeautifulSoup Library μπορεί και αποθηκεύει τα δεδομένα αυτών των άρθρων σε JSON αρχείο"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Λίστα URLs για συλλογή άρθρων από Wikipedia\n",
    "wikipedia_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Quantum_mechanics\",\n",
    "    \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "    \"https://en.wikipedia.org/wiki/Space_exploration\",\n",
    "    \"https://en.wikipedia.org/wiki/Computer_network\",\n",
    "    \"https://en.wikipedia.org/wiki/French_Revolution\",\n",
    "    \"https://en.wikipedia.org/wiki/World_War_II\",\n",
    "    \"https://en.wikipedia.org/wiki/Ancient_Greece\",\n",
    "    \"https://en.wikipedia.org/wiki/Renaissance\",\n",
    "    \"https://en.wikipedia.org/wiki/Seven_Wonders_of_the_World\",\n",
    "    \"https://en.wikipedia.org/wiki/Great_Barrier_Reef\",\n",
    "    \"https://en.wikipedia.org/wiki/Amazon_rainforest\",\n",
    "    \"https://en.wikipedia.org/wiki/Sahara\",\n",
    "    \"https://en.wikipedia.org/wiki/Classical_music\",\n",
    "    \"https://en.wikipedia.org/wiki/Impressionism\",\n",
    "    \"https://en.wikipedia.org/wiki/Film\",\n",
    "    \"https://en.wikipedia.org/wiki/Homer\",\n",
    "    \"https://en.wikipedia.org/wiki/Evolution\",\n",
    "    \"https://en.wikipedia.org/wiki/Human_anatomy\",\n",
    "    \"https://en.wikipedia.org/wiki/Climate\",\n",
    "    \"https://en.wikipedia.org/wiki/Biodiversity\",\n",
    "    \"https://en.wikipedia.org/wiki/Sustainable_development\",\n",
    "    \"https://en.wikipedia.org/wiki/Globalization\",\n",
    "    \"https://en.wikipedia.org/wiki/Blockchain\",\n",
    "    \"https://en.wikipedia.org/wiki/Financial_crisis_of_2007–2008\",\n",
    "    \"https://en.wikipedia.org/wiki/Vaccine\",\n",
    "    \"https://en.wikipedia.org/wiki/Mental_health\",\n",
    "    \"https://en.wikipedia.org/wiki/Nutrition\",\n",
    "    \"https://en.wikipedia.org/wiki/Pandemic\"\n",
    "]\n",
    "\n",
    "\n",
    "# Συνάρτηση για συλλογή δεδομένων από άρθρο Wikipedia\n",
    "def fetch_wikipedia_article(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Έλεγχος αν η σελίδα είναι προσβάσιμη\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Τίτλος άρθρου\n",
    "        title = soup.find('h1').text\n",
    "\n",
    "        # Περιεχόμενο άρθρου (παράγραφοι)\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = '\\n'.join([p.text for p in paragraphs if p.text])\n",
    "\n",
    "        return {'title': title, 'url': url, 'content': content}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Συλλογή δεδομένων από τα URLs\n",
    "collected_articles = []\n",
    "\n",
    "for url in wikipedia_urls:\n",
    "    article = fetch_wikipedia_article(url)\n",
    "    if article:\n",
    "        collected_articles.append(article)\n",
    "\n",
    "# Αποθήκευση δεδομένων σε JSON αρχείο\n",
    "output_file = 'wikipedia_articles.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(collected_articles, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Articles saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η εκτέλεση έγινε με επιτυχία και δημιουργήθηκε το αρχείο wikipedia_articles.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![success!](images/Στιγμιότυπο%20οθόνης%202025-01-11%20151157.png \"soup is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Βήμα 2:  Προεπεξεργασία Κειμένου (Text Processing)\n",
    "\n",
    "# Καθαρισμός και επεξεργασία κειμένου για να είναι έτοιμο για αναζήτηση\n",
    "\n",
    "Θα εφαρμόσω tokenization, lemmatization, stop-word removal, καθαρισμός ειδικών χαρακτήρων"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Προεπεξεργασία κειμένου\n",
    "def preprocess_text(content):\n",
    "    content = content.lower()\n",
    "    content = re.sub(r'[^a-z\\s]', '', content)\n",
    "    tokens = word_tokenize(content)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Φόρτωση δεδομένων από το αρχείο JSON που δημιουργήθηκε στο Βήμα 1\n",
    "input_file = 'wikipedia_articles.json'\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    wikipedia_articles = json.load(file)\n",
    "\n",
    "processed_articles = []\n",
    "metadata = []\n",
    "\n",
    "# Εφαρμογή προεπεξεργασίας σε κάθε άρθρο\n",
    "for article in wikipedia_articles:\n",
    "    processed_content = preprocess_text(article['content'])\n",
    "    processed_articles.append({\n",
    "        'title': article['title'],\n",
    "        'url': article['url'],\n",
    "        'tokens': processed_content\n",
    "    })\n",
    "    metadata.append({\n",
    "        'title': article['title'],\n",
    "        'url': article['url'],\n",
    "        'token_count': len(processed_content),\n",
    "        'unique_token_count': len(set(processed_content))\n",
    "    })\n",
    "\n",
    "# Αποθήκευση καθαρισμένων δεδομένων σε JSON\n",
    "output_file = 'processed_articles.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(processed_articles, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Αποθήκευση μεταδεδομένων σε JSON\n",
    "metadata_file = 'articles_metadata.json'\n",
    "with open(metadata_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(metadata, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Data processing complete. Processed articles and metadata saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Η επεξεργασία ήταν επιτυχής!\n",
    "\n",
    "όπως μπορούμε να δούμε στο παρακάτω παράδειγμα"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![success!](images/Βήμα2.png \"proccessing is ready\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
