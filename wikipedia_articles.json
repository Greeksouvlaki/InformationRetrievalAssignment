[
    {
        "title": "Quantum mechanics",
        "url": "https://en.wikipedia.org/wiki/Quantum_mechanics",
        "content": "\n\nQuantum mechanics is a fundamental theory that describes the behavior of nature at and below the scale of atoms.[2]: 1.1  It is the foundation of all quantum physics, which includes quantum chemistry, quantum field theory, quantum technology, and quantum information science.\n\nQuantum mechanics can describe many systems that classical physics cannot. Classical physics can describe many aspects of nature at an ordinary (macroscopic and (optical) microscopic) scale, but is not sufficient for describing them at very small submicroscopic (atomic and subatomic) scales. Most theories in classical physics can be derived from quantum mechanics as an approximation, valid at large (macroscopic/microscopic) scale.[3]\n\nQuantum systems have bound states that are quantized to discrete values of energy, momentum, angular momentum, and other quantities, in contrast to classical systems where these quantities can be measured continuously. Measurements of quantum systems show characteristics of both particles and waves (wave–particle duality), and there are limits to how accurately the value of a physical quantity can be predicted prior to its measurement, given a complete set of initial conditions (the uncertainty principle).\n\nQuantum mechanics arose gradually from theories to explain observations that could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and the correspondence between energy and frequency in Albert Einstein's 1905 paper, which explained the photoelectric effect. These early attempts to understand microscopic phenomena, now known as the \"old quantum theory\", led to the full development of quantum mechanics in the mid-1920s by Niels Bohr, Erwin Schrödinger, Werner Heisenberg, Max Born, Paul Dirac and others. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical entity called the wave function provides information, in the form of probability amplitudes, about what measurements of a particle's energy, momentum, and other physical properties may yield.\n\nQuantum mechanics allows the calculation of properties and behaviour of physical systems. It is typically applied to microscopic systems: molecules, atoms and sub-atomic particles. It has been demonstrated to hold for complex molecules with thousands of atoms,[4] but its application to human beings raises philosophical problems, such as Wigner's friend, and its application to the universe as a whole remains speculative.[5] Predictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy. For example, the refinement of quantum mechanics for the interaction of light and matter, known as quantum electrodynamics (QED), has been shown to agree with experiment to within 1 part in 1012 when predicting the magnetic properties of an electron.[6]\n\nA fundamental feature of the theory is that it usually cannot predict with certainty what will happen, but only give probabilities. Mathematically, a probability is found by taking the square of the absolute value of a complex number, known as a probability amplitude. This is known as the Born rule, named after physicist Max Born. For example, a quantum particle like an electron can be described by a wave function, which associates to each point in space a probability amplitude. Applying the Born rule to these amplitudes gives a probability density function for the position that the electron will be found to have when an experiment is performed to measure it. This is the best the theory can do; it cannot say for certain where the electron will be found. The Schrödinger equation relates the collection of probability amplitudes that pertain to one moment of time to the collection of probability amplitudes that pertain to another.[7]: 67–87 \n\nOne consequence of the mathematical rules of quantum mechanics is a tradeoff in predictability between measurable quantities. The most famous form of this uncertainty principle says that no matter how a quantum particle is prepared or how carefully experiments upon it are arranged, it is impossible to have a precise prediction for a measurement of its position and also at the same time for a measurement of its momentum.[7]: 427–435 \n\nAnother consequence of the mathematical rules of quantum mechanics is the phenomenon of quantum interference, which is often illustrated with the double-slit experiment. In the basic version of this experiment, a coherent light source, such as a laser beam, illuminates a plate pierced by two parallel slits, and the light passing through the slits is observed on a screen behind the plate.[8]: 102–111 [2]: 1.1–1.8  The wave nature of light causes the light waves passing through the two slits to interfere, producing bright and dark bands on the screen – a result that would not be expected if light consisted of classical particles.[8] However, the light is always found to be absorbed at the screen at discrete points, as individual particles rather than waves; the interference pattern appears via the varying density of these particle hits on the screen. Furthermore, versions of the experiment that include detectors at the slits find that each detected photon passes through one slit (as would a classical particle), and not through both slits (as would a wave).[8]: 109 [9][10] However, such experiments demonstrate that particles do not form the interference pattern if one detects which slit they pass through.  This behavior is known as wave–particle duality. In addition to light, electrons, atoms, and molecules are all found to exhibit the same dual behavior when fired towards a double slit.[2]\n\nAnother non-classical phenomenon predicted by quantum mechanics is quantum tunnelling: a particle that goes up against a potential barrier can cross it, even if its kinetic energy is smaller than the maximum of the potential.[11] In classical mechanics this particle would be trapped. Quantum tunnelling has several important consequences, enabling radioactive decay, nuclear fusion in stars, and applications such as scanning tunnelling microscopy, tunnel diode and tunnel field-effect transistor.[12][13]\n\nWhen quantum systems interact, the result can be the creation of quantum entanglement: their properties become so intertwined that a description of the whole solely in terms of the individual parts is no longer possible. Erwin Schrödinger called entanglement \"...the characteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought\".[14] Quantum entanglement enables quantum computing and is part of quantum communication protocols, such as quantum key distribution and superdense coding.[15] Contrary to popular misconception, entanglement does not allow sending signals faster than light, as demonstrated by the no-communication theorem.[15]\n\nAnother possibility opened by entanglement is testing for \"hidden variables\", hypothetical properties more fundamental than the quantities addressed in quantum theory itself, knowledge of which would allow more exact predictions than quantum theory provides. A collection of results, most significantly Bell's theorem, have demonstrated that broad classes of such hidden-variable theories are in fact incompatible with quantum physics. According to Bell's theorem, if nature actually operates in accord with any theory of local hidden variables, then the results of a Bell test will be constrained in a particular, quantifiable way. Many Bell tests have been performed and they have shown results incompatible with the constraints imposed by local hidden variables.[16][17]\n\nIt is not possible to present these concepts in more than a superficial way without introducing the mathematics involved; understanding quantum mechanics requires not only manipulating complex numbers, but also linear algebra, differential equations, group theory, and other more advanced subjects.[18][19] Accordingly, this article will present a mathematical formulation of quantum mechanics and survey its application to some useful and oft-studied examples.\n\nIn the mathematically rigorous formulation of quantum mechanics, the state of a quantum mechanical system is a vector \n\n\n\nψ\n\n\n{\\displaystyle \\psi }\n\n belonging to a (separable) complex Hilbert space \n\n\n\n\n\nH\n\n\n\n\n{\\displaystyle {\\mathcal {H}}}\n\n. This vector is postulated to be normalized under the Hilbert space inner product, that is, it obeys \n\n\n\n⟨\nψ\n,\nψ\n⟩\n=\n1\n\n\n{\\displaystyle \\langle \\psi ,\\psi \\rangle =1}\n\n, and it is well-defined up to a complex number of modulus 1 (the global phase), that is, \n\n\n\nψ\n\n\n{\\displaystyle \\psi }\n\n and \n\n\n\n\ne\n\ni\nα\n\n\nψ\n\n\n{\\displaystyle e^{i\\alpha }\\psi }\n\n represent the same physical system. In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system – for example, for describing position and momentum the Hilbert space is the space of complex square-integrable functions \n\n\n\n\nL\n\n2\n\n\n(\n\nC\n\n)\n\n\n{\\displaystyle L^{2}(\\mathbb {C} )}\n\n, while the Hilbert space for the spin of a single proton is simply the space of two-dimensional complex vectors \n\n\n\n\n\nC\n\n\n2\n\n\n\n\n{\\displaystyle \\mathbb {C} ^{2}}\n\n with the usual inner product.\n\nPhysical quantities of interest – position, momentum, energy, spin – are represented by observables, which are Hermitian (more precisely, self-adjoint) linear operators acting on the Hilbert space. A quantum state can be an eigenvector of an observable, in which case it is called an eigenstate, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. More generally, a quantum state will be a linear combination of the eigenstates, known as a quantum superposition. When an observable is measured, the result will be one of its eigenvalues with probability given by the Born rule: in the simplest case the eigenvalue \n\n\n\nλ\n\n\n{\\displaystyle \\lambda }\n\n is non-degenerate and the probability is given by \n\n\n\n\n|\n\n⟨\n\n\n\nλ\n→\n\n\n\n,\nψ\n⟩\n\n\n|\n\n\n2\n\n\n\n\n{\\displaystyle |\\langle {\\vec {\\lambda }},\\psi \\rangle |^{2}}\n\n, where \n\n\n\n\n\n\nλ\n→\n\n\n\n\n\n{\\displaystyle {\\vec {\\lambda }}}\n\n is its associated eigenvector. More generally, the eigenvalue is degenerate and the probability is given by \n\n\n\n⟨\nψ\n,\n\nP\n\nλ\n\n\nψ\n⟩\n\n\n{\\displaystyle \\langle \\psi ,P_{\\lambda }\\psi \\rangle }\n\n, where \n\n\n\n\nP\n\nλ\n\n\n\n\n{\\displaystyle P_{\\lambda }}\n\n is the projector onto its associated eigenspace. In the continuous case, these formulas give instead the probability density.\n\nAfter the measurement, if result \n\n\n\nλ\n\n\n{\\displaystyle \\lambda }\n\n was obtained, the quantum state is postulated to collapse to \n\n\n\n\n\n\nλ\n→\n\n\n\n\n\n{\\displaystyle {\\vec {\\lambda }}}\n\n, in the non-degenerate case, or to \n\n\n\n\nP\n\nλ\n\n\nψ\n\n\n/\n\n\n\n\n\n⟨\nψ\n,\n\nP\n\nλ\n\n\nψ\n⟩\n\n\n\n\n{\\textstyle P_{\\lambda }\\psi {\\big /}\\!{\\sqrt {\\langle \\psi ,P_{\\lambda }\\psi \\rangle }}}\n\n, in the general case. The probabilistic nature of quantum mechanics thus stems from the act of measurement. This is one of the most difficult aspects of quantum systems to understand. It was the central topic in the famous Bohr–Einstein debates, in which the two scientists attempted to clarify these fundamental principles by way of thought experiments. In the decades after the formulation of quantum mechanics, the question of what constitutes a \"measurement\" has been extensively studied. Newer interpretations of quantum mechanics have been formulated that do away with the concept of \"wave function collapse\" (see, for example, the many-worlds interpretation). The basic idea is that when a quantum system interacts with a measuring apparatus, their respective wave functions become entangled so that the original quantum system ceases to exist as an independent entity (see Measurement in quantum mechanics[20]).\n\nThe time evolution of a quantum state is described by the Schrödinger equation:\n\nHere \n\n\n\nH\n\n\n{\\displaystyle H}\n\n denotes the Hamiltonian, the observable corresponding to the total energy of the system, and \n\n\n\nℏ\n\n\n{\\displaystyle \\hbar }\n\n is the reduced Planck constant. The constant \n\n\n\ni\nℏ\n\n\n{\\displaystyle i\\hbar }\n\n is introduced so that the Hamiltonian is reduced to the classical Hamiltonian in cases where the quantum system can be approximated by a classical system; the ability to make such an approximation in certain limits is called the correspondence principle.\n\nThe solution of this differential equation is given by\n\nThe operator \n\n\n\nU\n(\nt\n)\n=\n\ne\n\n−\ni\nH\nt\n\n/\n\nℏ\n\n\n\n\n{\\displaystyle U(t)=e^{-iHt/\\hbar }}\n\n is known as the time-evolution operator, and has the crucial property that it is unitary. This time evolution is deterministic in the sense that – given an initial quantum state \n\n\n\nψ\n(\n0\n)\n\n\n{\\displaystyle \\psi (0)}\n\n – it makes a definite prediction of what the quantum state \n\n\n\nψ\n(\nt\n)\n\n\n{\\displaystyle \\psi (t)}\n\n will be at any later time.[21]\n\nSome wave functions produce probability distributions that are independent of time, such as eigenstates of the Hamiltonian.[7]: 133–137  Many systems that are treated dynamically in classical mechanics are described by such \"static\" wave functions. For example, a single electron in an unexcited atom is pictured classically as a particle moving in a circular trajectory around the atomic nucleus, whereas in quantum mechanics, it is described by a static wave function surrounding the nucleus. For example, the electron wave function for an unexcited hydrogen atom is a spherically symmetric function known as an s orbital (Fig. 1).\n\nAnalytic solutions of the Schrödinger equation are known for very few relatively simple model Hamiltonians including the quantum harmonic oscillator, the particle in a box, the dihydrogen cation, and the hydrogen atom. Even the helium atom – which contains just two electrons – has defied all attempts at a fully analytic treatment, admitting no solution in closed form.[22][23][24]\n\nHowever, there are techniques for finding approximate solutions. One method, called perturbation theory, uses the analytic result for a simple quantum mechanical model to create a result for a related but more complicated model by (for example) the addition of a weak potential energy.[7]: 793  Another approximation method applies to systems for which quantum mechanics produces only small deviations from classical behavior. These deviations can then be computed based on the classical motion.[7]: 849 \n\nOne consequence of the basic quantum formalism is the uncertainty principle. In its most familiar form, this states that no preparation of a quantum particle can imply simultaneously precise predictions both for a measurement of its position and for a measurement of its momentum.[25][26] Both position and momentum are observables, meaning that they are represented by Hermitian operators. The position operator \n\n\n\n\n\n\nX\n^\n\n\n\n\n\n{\\displaystyle {\\hat {X}}}\n\n and momentum operator \n\n\n\n\n\n\nP\n^\n\n\n\n\n\n{\\displaystyle {\\hat {P}}}\n\n do not commute, but rather satisfy the canonical commutation relation:\n\nGiven a quantum state, the Born rule lets us compute expectation values for both \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nP\n\n\n{\\displaystyle P}\n\n, and moreover for powers of them. Defining the uncertainty for an observable by a standard deviation, we have\n\nand likewise for the momentum:\n\nThe uncertainty principle states that\n\nEither standard deviation can in principle be made arbitrarily small, but not both simultaneously.[27] This inequality generalizes to arbitrary pairs of self-adjoint operators \n\n\n\nA\n\n\n{\\displaystyle A}\n\n and \n\n\n\nB\n\n\n{\\displaystyle B}\n\n. The commutator of these two operators is\n\nand this provides the lower bound on the product of standard deviations:\n\nAnother consequence of the canonical commutation relation is that the position and momentum operators are Fourier transforms of each other, so that a description of an object according to its momentum is the Fourier transform of its description according to its position. The fact that dependence in momentum is the Fourier transform of the dependence in position means that the momentum operator is equivalent (up to an \n\n\n\ni\n\n/\n\nℏ\n\n\n{\\displaystyle i/\\hbar }\n\n factor) to taking the derivative according to the position, since in Fourier analysis differentiation corresponds to multiplication in the dual space. This is why in quantum equations in position space, the momentum \n\n\n\n\np\n\ni\n\n\n\n\n{\\displaystyle p_{i}}\n\n is replaced by \n\n\n\n−\ni\nℏ\n\n\n∂\n\n∂\nx\n\n\n\n\n\n{\\displaystyle -i\\hbar {\\frac {\\partial }{\\partial x}}}\n\n, and in particular in the non-relativistic Schrödinger equation in position space the momentum-squared term is replaced with a Laplacian times \n\n\n\n−\n\nℏ\n\n2\n\n\n\n\n{\\displaystyle -\\hbar ^{2}}\n\n.[25]\n\nWhen two different quantum systems are considered together, the Hilbert space of the combined system is the tensor product of the Hilbert spaces of the two components. For example, let A and B be two quantum systems, with Hilbert spaces \n\n\n\n\n\n\nH\n\n\n\nA\n\n\n\n\n{\\displaystyle {\\mathcal {H}}_{A}}\n\n and \n\n\n\n\n\n\nH\n\n\n\nB\n\n\n\n\n{\\displaystyle {\\mathcal {H}}_{B}}\n\n, respectively. The Hilbert space of the composite system is then\n\nIf the state for the first system is the vector \n\n\n\n\nψ\n\nA\n\n\n\n\n{\\displaystyle \\psi _{A}}\n\n and the state for the second system is \n\n\n\n\nψ\n\nB\n\n\n\n\n{\\displaystyle \\psi _{B}}\n\n, then the state of the composite system is\n\nNot all states in the joint Hilbert space \n\n\n\n\n\n\nH\n\n\n\nA\nB\n\n\n\n\n{\\displaystyle {\\mathcal {H}}_{AB}}\n\n can be written in this form, however, because the superposition principle implies that linear combinations of these \"separable\" or \"product states\" are also valid. For example, if \n\n\n\n\nψ\n\nA\n\n\n\n\n{\\displaystyle \\psi _{A}}\n\n and \n\n\n\n\nϕ\n\nA\n\n\n\n\n{\\displaystyle \\phi _{A}}\n\n are both possible states for system \n\n\n\nA\n\n\n{\\displaystyle A}\n\n, and likewise \n\n\n\n\nψ\n\nB\n\n\n\n\n{\\displaystyle \\psi _{B}}\n\n and \n\n\n\n\nϕ\n\nB\n\n\n\n\n{\\displaystyle \\phi _{B}}\n\n are both possible states for system \n\n\n\nB\n\n\n{\\displaystyle B}\n\n, then\n\nis a valid joint state that is not separable. States that are not separable are called entangled.[28][29]\n\nIf the state for a composite system is entangled, it is impossible to describe either component system A or system B by a state vector. One can instead define reduced density matrices that describe the statistics that can be obtained by making measurements on either component system alone. This necessarily causes a loss of information, though: knowing the reduced density matrices of the individual systems is not enough to reconstruct the state of the composite system.[28][29] Just as density matrices specify the state of a subsystem of a larger system, analogously, positive operator-valued measures (POVMs) describe the effect on a subsystem of a measurement performed on a larger system. POVMs are extensively used in quantum information theory.[28][30]\n\nAs described above, entanglement is a key feature of models of measurement processes in which an apparatus becomes entangled with the system being measured. Systems interacting with the environment in which they reside generally become entangled with that environment, a phenomenon known as quantum decoherence. This can explain why, in practice, quantum effects are difficult to observe in systems larger than microscopic.[31]\n\nThere are many mathematically equivalent formulations of quantum mechanics. One of the oldest and most common is the \"transformation theory\" proposed by Paul Dirac, which unifies and generalizes the two earliest formulations of quantum mechanics – matrix mechanics (invented by Werner Heisenberg) and wave mechanics (invented by Erwin Schrödinger).[32] An alternative formulation of quantum mechanics is Feynman's path integral formulation, in which a quantum-mechanical amplitude is considered as a sum over all possible classical and non-classical paths between the initial and final states. This is the quantum-mechanical counterpart of the action principle in classical mechanics.[33]\n\nThe Hamiltonian \n\n\n\nH\n\n\n{\\displaystyle H}\n\n is known as the generator of time evolution, since it defines a unitary time-evolution operator \n\n\n\nU\n(\nt\n)\n=\n\ne\n\n−\ni\nH\nt\n\n/\n\nℏ\n\n\n\n\n{\\displaystyle U(t)=e^{-iHt/\\hbar }}\n\n for each value of \n\n\n\nt\n\n\n{\\displaystyle t}\n\n. From this relation between \n\n\n\nU\n(\nt\n)\n\n\n{\\displaystyle U(t)}\n\n and \n\n\n\nH\n\n\n{\\displaystyle H}\n\n, it follows that any observable \n\n\n\nA\n\n\n{\\displaystyle A}\n\n that commutes with \n\n\n\nH\n\n\n{\\displaystyle H}\n\n will be conserved: its expectation value will not change over time.[7]: 471  This statement generalizes, as mathematically, any Hermitian operator \n\n\n\nA\n\n\n{\\displaystyle A}\n\n can generate a family of unitary operators parameterized by a variable \n\n\n\nt\n\n\n{\\displaystyle t}\n\n. Under the evolution generated by \n\n\n\nA\n\n\n{\\displaystyle A}\n\n, any observable \n\n\n\nB\n\n\n{\\displaystyle B}\n\n that commutes with \n\n\n\nA\n\n\n{\\displaystyle A}\n\n will be conserved. Moreover, if \n\n\n\nB\n\n\n{\\displaystyle B}\n\n is conserved by evolution under \n\n\n\nA\n\n\n{\\displaystyle A}\n\n, then \n\n\n\nA\n\n\n{\\displaystyle A}\n\n is conserved under the evolution generated by \n\n\n\nB\n\n\n{\\displaystyle B}\n\n. This implies a quantum version of the result proven by Emmy Noether in classical (Lagrangian) mechanics: for every differentiable symmetry of a Hamiltonian, there exists a corresponding conservation law.\n\nThe simplest example of a quantum system with a position degree of freedom is a free particle in a single spatial dimension. A free particle is one which is not subject to external influences, so that its Hamiltonian consists only of its kinetic energy:\n\nThe general solution of the Schrödinger equation is given by\n\nwhich is a superposition of all possible plane waves \n\n\n\n\ne\n\ni\n(\nk\nx\n−\n\n\n\nℏ\n\nk\n\n2\n\n\n\n\n2\nm\n\n\n\nt\n)\n\n\n\n\n{\\displaystyle e^{i(kx-{\\frac {\\hbar k^{2}}{2m}}t)}}\n\n, which are eigenstates of the momentum operator with momentum \n\n\n\np\n=\nℏ\nk\n\n\n{\\displaystyle p=\\hbar k}\n\n. The coefficients of the superposition are \n\n\n\n\n\n\nψ\n^\n\n\n\n(\nk\n,\n0\n)\n\n\n{\\displaystyle {\\hat {\\psi }}(k,0)}\n\n, which is the Fourier transform of the initial quantum state \n\n\n\nψ\n(\nx\n,\n0\n)\n\n\n{\\displaystyle \\psi (x,0)}\n\n.\n\nIt is not possible for the solution to be a single momentum eigenstate, or a single position eigenstate, as these are not normalizable quantum states.[note 1] Instead, we can consider a Gaussian wave packet:\n\nwhich has Fourier transform, and therefore momentum distribution\n\nWe see that as we make \n\n\n\na\n\n\n{\\displaystyle a}\n\n smaller the spread in position gets smaller, but the spread in momentum gets larger. Conversely, by making \n\n\n\na\n\n\n{\\displaystyle a}\n\n larger we make the spread in momentum smaller, but the spread in position gets larger. This illustrates the uncertainty principle.\n\nAs we let the Gaussian wave packet evolve in time, we see that its center moves through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more and more uncertain. The uncertainty in momentum, however, stays constant.[34]\n\nThe particle in a one-dimensional potential energy box is the most mathematically simple example where restraints lead to the quantization of energy levels. The box is defined as having zero potential energy everywhere inside a certain region, and therefore infinite potential energy everywhere outside that region.[25]: 77–78  For the one-dimensional case in the \n\n\n\nx\n\n\n{\\displaystyle x}\n\n direction, the time-independent Schrödinger equation may be written\n\nWith the differential operator defined by\n\nwith state \n\n\n\nψ\n\n\n{\\displaystyle \\psi }\n\n in this case having energy \n\n\n\nE\n\n\n{\\displaystyle E}\n\n coincident with the kinetic energy of the particle.\n\nThe general solutions of the Schrödinger equation for the particle in a box are\n\nor, from Euler's formula,\n\nThe infinite potential walls of the box determine the values of \n\n\n\nC\n,\nD\n,\n\n\n{\\displaystyle C,D,}\n\n and \n\n\n\nk\n\n\n{\\displaystyle k}\n\n at \n\n\n\nx\n=\n0\n\n\n{\\displaystyle x=0}\n\n and \n\n\n\nx\n=\nL\n\n\n{\\displaystyle x=L}\n\n where \n\n\n\nψ\n\n\n{\\displaystyle \\psi }\n\n must be zero. Thus, at \n\n\n\nx\n=\n0\n\n\n{\\displaystyle x=0}\n\n,\n\nand \n\n\n\nD\n=\n0\n\n\n{\\displaystyle D=0}\n\n. At \n\n\n\nx\n=\nL\n\n\n{\\displaystyle x=L}\n\n,\n\nin which \n\n\n\nC\n\n\n{\\displaystyle C}\n\n cannot be zero as this would conflict with the postulate that \n\n\n\nψ\n\n\n{\\displaystyle \\psi }\n\n has norm 1. Therefore, since \n\n\n\nsin\n⁡\n(\nk\nL\n)\n=\n0\n\n\n{\\displaystyle \\sin(kL)=0}\n\n, \n\n\n\nk\nL\n\n\n{\\displaystyle kL}\n\n must be an integer multiple of \n\n\n\nπ\n\n\n{\\displaystyle \\pi }\n\n,\n\nThis constraint on \n\n\n\nk\n\n\n{\\displaystyle k}\n\n implies a constraint on the energy levels, yielding\n\nA finite potential well is the generalization of the infinite potential well problem to potential wells having finite depth. The finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well. Another related problem is that of the rectangular potential barrier, which furnishes a model for the quantum tunneling effect that plays an important role in the performance of modern technologies such as flash memory and scanning tunneling microscopy.\n\nAs in the classical case, the potential for the quantum harmonic oscillator is given by[7]: 234 \n\nThis problem can either be treated by directly solving the Schrödinger equation, which is not trivial, or by using the more elegant \"ladder method\" first proposed by Paul Dirac. The eigenstates are given by\n\nwhere Hn are the Hermite polynomials\n\nand the corresponding energy levels are\n\nThis is another example illustrating the discretization of energy for bound states.\n\nThe Mach–Zehnder interferometer (MZI) illustrates the concepts of superposition and interference with linear algebra in dimension 2, rather than differential equations. It can be seen as a simplified version of the double-slit experiment, but it is of interest in its own right, for example in the delayed choice quantum eraser, the Elitzur–Vaidman bomb tester, and in studies of quantum entanglement.[35][36]\n\nWe can model a photon going through the interferometer by considering that at each point it can be in a superposition of only two paths: the \"lower\" path which starts from the left, goes straight through both beam splitters, and ends at the top, and the \"upper\" path which starts from the bottom, goes straight through both beam splitters, and ends at the right. The quantum state of the photon is therefore a vector \n\n\n\nψ\n∈\n\n\nC\n\n\n2\n\n\n\n\n{\\displaystyle \\psi \\in \\mathbb {C} ^{2}}\n\n that is a superposition of the \"lower\" path \n\n\n\n\nψ\n\nl\n\n\n=\n\n\n(\n\n\n\n1\n\n\n\n\n0\n\n\n\n)\n\n\n\n\n{\\displaystyle \\psi _{l}={\\begin{pmatrix}1\\\\0\\end{pmatrix}}}\n\n and the \"upper\" path \n\n\n\n\nψ\n\nu\n\n\n=\n\n\n(\n\n\n\n0\n\n\n\n\n1\n\n\n\n)\n\n\n\n\n{\\displaystyle \\psi _{u}={\\begin{pmatrix}0\\\\1\\end{pmatrix}}}\n\n, that is, \n\n\n\nψ\n=\nα\n\nψ\n\nl\n\n\n+\nβ\n\nψ\n\nu\n\n\n\n\n{\\displaystyle \\psi =\\alpha \\psi _{l}+\\beta \\psi _{u}}\n\n for complex \n\n\n\nα\n,\nβ\n\n\n{\\displaystyle \\alpha ,\\beta }\n\n. In order to respect the postulate that \n\n\n\n⟨\nψ\n,\nψ\n⟩\n=\n1\n\n\n{\\displaystyle \\langle \\psi ,\\psi \\rangle =1}\n\n we require that \n\n\n\n\n|\n\nα\n\n\n|\n\n\n2\n\n\n+\n\n|\n\nβ\n\n\n|\n\n\n2\n\n\n=\n1\n\n\n{\\displaystyle |\\alpha |^{2}+|\\beta |^{2}=1}\n\n.\n\nBoth beam splitters are modelled as the unitary matrix \n\n\n\nB\n=\n\n\n1\n\n2\n\n\n\n\n\n(\n\n\n\n1\n\n\ni\n\n\n\n\ni\n\n\n1\n\n\n\n)\n\n\n\n\n{\\displaystyle B={\\frac {1}{\\sqrt {2}}}{\\begin{pmatrix}1&i\\\\i&1\\end{pmatrix}}}\n\n, which means that when a photon meets the beam splitter it will either stay on the same path with a probability amplitude of \n\n\n\n1\n\n/\n\n\n\n2\n\n\n\n\n{\\displaystyle 1/{\\sqrt {2}}}\n\n, or be reflected to the other path with a probability amplitude of \n\n\n\ni\n\n/\n\n\n\n2\n\n\n\n\n{\\displaystyle i/{\\sqrt {2}}}\n\n. The phase shifter on the upper arm is modelled as the unitary matrix \n\n\n\nP\n=\n\n\n(\n\n\n\n1\n\n\n0\n\n\n\n\n0\n\n\n\ne\n\ni\nΔ\nΦ\n\n\n\n\n\n)\n\n\n\n\n{\\displaystyle P={\\begin{pmatrix}1&0\\\\0&e^{i\\Delta \\Phi }\\end{pmatrix}}}\n\n, which means that if the photon is on the \"upper\" path it will gain a relative phase of \n\n\n\nΔ\nΦ\n\n\n{\\displaystyle \\Delta \\Phi }\n\n, and it will stay unchanged if it is in the lower path.\n\nA photon that enters the interferometer from the left will then be acted upon with a beam splitter \n\n\n\nB\n\n\n{\\displaystyle B}\n\n, a phase shifter \n\n\n\nP\n\n\n{\\displaystyle P}\n\n, and another beam splitter \n\n\n\nB\n\n\n{\\displaystyle B}\n\n, and so end up in the state\n\nand the probabilities that it will be detected at the right or at the top are given respectively by\n\nOne can therefore use the Mach–Zehnder interferometer to estimate the phase shift by estimating these probabilities.\n\nIt is interesting to consider what would happen if the photon were definitely in either the \"lower\" or \"upper\" paths between the beam splitters. This can be accomplished by blocking one of the paths, or equivalently by removing the first beam splitter (and feeding the photon from the left or the bottom, as desired). In both cases, there will be no interference between the paths anymore, and the probabilities are given by \n\n\n\np\n(\nu\n)\n=\np\n(\nl\n)\n=\n1\n\n/\n\n2\n\n\n{\\displaystyle p(u)=p(l)=1/2}\n\n, independently of the phase \n\n\n\nΔ\nΦ\n\n\n{\\displaystyle \\Delta \\Phi }\n\n. From this we can conclude that the photon does not take one path or another after the first beam splitter, but rather that it is in a genuine quantum superposition of the two paths.[37]\n\nQuantum mechanics has had enormous success in explaining many of the features of our universe, with regard to small-scale and discrete quantities and interactions which cannot be explained by classical methods.[note 2] Quantum mechanics is often the only theory that can reveal the individual behaviors of the subatomic particles that make up all forms of matter (electrons, protons, neutrons, photons, and others). Solid-state physics and materials science are dependent upon quantum mechanics.[38]\n\nIn many aspects, modern technology operates at a scale where quantum effects are significant. Important applications of quantum theory include quantum chemistry, quantum optics, quantum computing, superconducting magnets, light-emitting diodes, the optical amplifier and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy.[39] Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-molecule DNA.\n\nThe rules of quantum mechanics assert that the state space of a system is a Hilbert space and that observables of the system are Hermitian operators acting on vectors in that space – although they do not tell us which Hilbert space or which operators. These can be chosen appropriately in order to obtain a quantitative description of a quantum system, a necessary step in making physical predictions. An important guide for making these choices is the correspondence principle, a heuristic which states that the predictions of quantum mechanics reduce to those of classical mechanics in the regime of large quantum numbers.[40] One can also start from an established classical model of a particular system, and then try to guess the underlying quantum model that would give rise to the classical model in the correspondence limit. This approach is known as quantization.[41]: 299 [42]\n\nWhen quantum mechanics was originally formulated, it was applied to models whose correspondence limit was non-relativistic classical mechanics. For instance, the well-known model of the quantum harmonic oscillator uses an explicitly non-relativistic expression for the kinetic energy of the oscillator, and is thus a quantum version of the classical harmonic oscillator.[7]: 234 \n\nComplications arise with chaotic systems, which do not have good quantum numbers, and quantum chaos studies the relationship between classical and quantum descriptions in these systems.[41]: 353 \n\nQuantum decoherence is a mechanism through which quantum systems lose coherence, and thus become incapable of displaying many typically quantum effects: quantum superpositions become simply probabilistic mixtures, and quantum entanglement becomes simply classical correlations.[7]: 687–730  Quantum coherence is not typically evident at macroscopic scales, though at temperatures approaching absolute zero quantum behavior may manifest macroscopically.[note 3]\n\nMany macroscopic properties of a classical system are a direct consequence of the quantum behavior of its parts. For example, the stability of bulk matter (consisting of atoms and molecules which would quickly collapse under electric forces alone), the rigidity of solids, and the mechanical, thermal, chemical, optical and magnetic properties of matter are all results of the interaction of electric charges under the rules of quantum mechanics.[43]\n\nEarly attempts to merge quantum mechanics with special relativity involved the replacement of the Schrödinger equation with a covariant equation such as the Klein–Gordon equation or the Dirac equation. While these theories were successful in explaining many experimental results, they had certain unsatisfactory qualities stemming from their neglect of the relativistic creation and annihilation of particles. A fully relativistic quantum theory required the development of quantum field theory, which applies quantization to a field (rather than a fixed set of particles). The first complete quantum field theory, quantum electrodynamics, provides a fully quantum description of the electromagnetic interaction. Quantum electrodynamics is, along with general relativity, one of the most accurate physical theories ever devised.[44][45]\n\nThe full apparatus of quantum field theory is often unnecessary for describing electrodynamic systems. A simpler approach, one that has been used since the inception of quantum mechanics, is to treat charged particles as quantum mechanical objects being acted on by a classical electromagnetic field. For example, the elementary quantum model of the hydrogen atom describes the electric field of the hydrogen atom using a classical \n\n\n\n\n−\n\ne\n\n2\n\n\n\n/\n\n(\n4\nπ\n\nϵ\n\n\n\n\n0\n\n\n\n\nr\n)\n\n\n\n{\\displaystyle \\textstyle -e^{2}/(4\\pi \\epsilon _{_{0}}r)}\n\n Coulomb potential.[7]: 285  Likewise, in a Stern–Gerlach experiment, a charged particle is modeled as a quantum system, while the background magnetic field is described classically.[41]: 26  This \"semi-classical\" approach fails if quantum fluctuations in the electromagnetic field play an important role, such as in the emission of photons by charged particles.\n\nQuantum field theories for the strong nuclear force and the weak nuclear force have also been developed. The quantum field theory of the strong nuclear force is called quantum chromodynamics, and describes the interactions of subnuclear particles such as quarks and gluons. The weak nuclear force and the electromagnetic force were unified, in their quantized forms, into a single quantum field theory (known as electroweak theory), by the physicists Abdus Salam, Sheldon Glashow and Steven Weinberg.[46]\n\nEven though the predictions of both quantum theory and general relativity have been supported by rigorous and repeated empirical evidence, their abstract formalisms contradict each other and they have proven extremely difficult to incorporate into one consistent, cohesive model. Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory of quantum gravity is an important issue in physical cosmology and the search by physicists for an elegant \"Theory of Everything\" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th- and 21st-century physics. This TOE would combine not only the models of subatomic physics but also derive the four fundamental forces of nature from a single force or phenomenon.[47]\n\nOne proposal for doing so is string theory, which posits that the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In string theory, one of the many vibrational states of the string corresponds to the graviton, a quantum mechanical particle that carries gravitational force.[48][49]\n\nAnother popular theory is loop quantum gravity (LQG), which describes quantum properties of gravity and is thus a theory of quantum spacetime. LQG is an attempt to merge and adapt standard quantum mechanics and standard general relativity. This theory describes space as an extremely fine fabric \"woven\" of finite loops called spin networks. The evolution of a spin network over time is called a spin foam. The characteristic length scale of a spin foam is the Planck length, approximately 1.616×10−35 m, and so lengths shorter than the Planck length are not physically meaningful in LQG.[50]\n\nSince its inception, the many counter-intuitive aspects and results of quantum mechanics have provoked strong philosophical debates and many interpretations. The arguments centre on the probabilistic nature of quantum mechanics, the difficulties with wavefunction collapse and the related measurement problem, and quantum nonlocality. Perhaps the only consensus that exists about these issues is that there is no consensus. Richard Feynman once said, \"I think I can safely say that nobody understands quantum mechanics.\"[51] According to Steven Weinberg, \"There is now in my opinion no entirely satisfactory interpretation of quantum mechanics.\"[52]\n\nThe views of Niels Bohr, Werner Heisenberg and other physicists are often grouped together as the \"Copenhagen interpretation\".[53][54] According to these views, the probabilistic nature of quantum mechanics is not a temporary feature which will eventually be replaced by a deterministic theory, but is instead a final renunciation of the classical idea of \"causality\". Bohr in particular emphasized that any well-defined application of the quantum mechanical formalism must always make reference to the experimental arrangement, due to the complementary nature of evidence obtained under different experimental situations. Copenhagen-type interpretations were adopted by Nobel laureates in quantum physics, including Bohr,[55] Heisenberg,[56] Schrödinger,[57] Feynman,[2] and Zeilinger[58] as well as 21st-century researchers in quantum foundations.[59]\n\nAlbert Einstein, himself one of the founders of quantum theory, was troubled by its apparent failure to respect some cherished metaphysical principles, such as determinism and locality. Einstein's long-running exchanges with Bohr about the meaning and status of quantum mechanics are now known as the Bohr–Einstein debates. Einstein believed that underlying quantum mechanics must be a theory that explicitly forbids action at a distance. He argued that quantum mechanics was incomplete, a theory that was valid but not fundamental, analogous to how thermodynamics is valid, but the fundamental theory behind it is statistical mechanics. In 1935, Einstein and his collaborators Boris Podolsky and Nathan Rosen published an argument that the principle of locality implies the incompleteness of quantum mechanics, a thought experiment later termed the Einstein–Podolsky–Rosen paradox.[note 4] In 1964, John Bell showed that EPR's principle of locality, together with determinism, was actually incompatible with quantum mechanics: they implied constraints on the correlations produced by distance systems, now known as Bell inequalities, that can be violated by entangled particles.[64] Since then several experiments have been performed to obtain these correlations, with the result that they do in fact violate Bell inequalities, and thus falsify the conjunction of locality with determinism.[16][17]\n\nBohmian mechanics shows that it is possible to reformulate quantum mechanics to make it deterministic, at the price of making it explicitly nonlocal. It attributes not only a wave function to a physical system, but in addition a real position, that evolves deterministically under a nonlocal guiding equation. The evolution of a physical system is given at all times by the Schrödinger equation together with the guiding equation; there is never a collapse of the wave function. This solves the measurement problem.[65]\n\nEverett's many-worlds interpretation, formulated in 1956, holds that all the possibilities described by quantum theory simultaneously occur in a multiverse composed of mostly independent parallel universes.[66] This is a consequence of removing the axiom of the collapse of the wave packet. All possible states of the measured system and the measuring apparatus, together with the observer, are present in a real physical quantum superposition. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we do not observe the multiverse as a whole, but only one parallel universe at a time. Exactly how this is supposed to work has been the subject of much debate. Several attempts have been made to make sense of this and derive the Born rule,[67][68] with no consensus on whether they have been successful.[69][70][71]\n\nRelational quantum mechanics appeared in the late 1990s as a modern derivative of Copenhagen-type ideas,[72] and QBism was developed some years later.[73]\n\nQuantum mechanics was developed in the early decades of the 20th century, driven by the need to explain phenomena that, in some cases, had been observed in earlier times. Scientific inquiry into the wave nature of light began in the 17th and 18th centuries, when scientists such as Robert Hooke, Christiaan Huygens and Leonhard Euler proposed a wave theory of light based on experimental observations.[74] In 1803 English polymath Thomas Young described the famous double-slit experiment.[75] This experiment played a major role in the general acceptance of the wave theory of light.\n\nDuring the early 19th century, chemical research by John Dalton and Amedeo Avogadro lent weight to the atomic theory of matter, an idea that James Clerk Maxwell, Ludwig Boltzmann and others built upon to establish the kinetic theory of gases. The successes of kinetic theory gave further credence to the idea that matter is composed of atoms, yet the theory also had shortcomings that would only be resolved by the development of quantum mechanics.[76] While the early conception of atoms from Greek philosophy had been that they were indivisible units – the word \"atom\" deriving from the Greek for \"uncuttable\" –  the 19th century saw the formulation of hypotheses about subatomic structure. One important discovery in that regard was Michael Faraday's 1838 observation of a glow caused by an electrical discharge inside a glass tube containing gas at low pressure. Julius Plücker, Johann Wilhelm Hittorf and Eugen Goldstein carried on and improved upon Faraday's work, leading to the identification of cathode rays, which J. J. Thomson found to consist of subatomic particles that would be called electrons.[77][78]\n\nThe black-body radiation problem was discovered by Gustav Kirchhoff in 1859. In 1900, Max Planck proposed the hypothesis that energy is radiated and absorbed in discrete \"quanta\" (or energy packets), yielding a calculation that precisely matched the observed patterns of black-body radiation.[79] The word quantum derives from the Latin, meaning \"how great\" or \"how much\".[80] According to Planck, quantities of energy could be thought of as divided into \"elements\" whose size (E) would be proportional to their frequency (ν):\n\nwhere h is the Planck constant. Planck cautiously insisted that this was only an aspect of the processes of absorption and emission of radiation and was not the physical reality of the radiation.[81] In fact, he considered his quantum hypothesis a mathematical trick to get the right answer rather than a sizable discovery.[82] However, in 1905 Albert Einstein interpreted Planck's quantum hypothesis realistically and used it to explain the photoelectric effect, in which shining light on certain materials can eject electrons from the material. Niels Bohr then developed Planck's ideas about radiation into a model of the hydrogen atom that successfully predicted the spectral lines of hydrogen.[83] Einstein further developed this idea to show that an electromagnetic wave such as light could also be described as a particle (later called the photon), with a discrete amount of energy that depends on its frequency.[84] In his paper \"On the Quantum Theory of Radiation\", Einstein expanded on the interaction between energy and matter to explain the absorption and emission of energy by atoms. Although overshadowed at the time by his general theory of relativity, this paper articulated the mechanism underlying the stimulated emission of radiation,[85] which became the basis of the laser.[86]\n\nThis phase is known as the old quantum theory. Never complete or self-consistent, the old quantum theory was rather a set of heuristic corrections to classical mechanics.[87][88] The theory is now understood as a semi-classical approximation to modern quantum mechanics.[89][90] Notable results from this period include, in addition to the work of Planck, Einstein and Bohr mentioned above, Einstein and Peter Debye's work on the specific heat of solids, Bohr and Hendrika Johanna van Leeuwen's proof that classical physics cannot account for diamagnetism, and Arnold Sommerfeld's extension of the Bohr model to include special-relativistic effects.[87][91]\n\nIn the mid-1920s quantum mechanics was developed to become the standard formulation for atomic physics. In 1923, the French physicist Louis de Broglie put forward his theory of matter waves by stating that particles can exhibit wave characteristics and vice versa. Building on de Broglie's approach, modern quantum mechanics was born in 1925, when the German physicists Werner Heisenberg, Max Born, and Pascual Jordan[92][93] developed matrix mechanics and the Austrian physicist Erwin Schrödinger invented wave mechanics. Born introduced the probabilistic interpretation of Schrödinger's wave function in July 1926.[94] Thus, the entire field of quantum physics emerged, leading to its wider acceptance at the Fifth Solvay Conference in 1927.[95]\n\nBy 1930, quantum mechanics had been further unified and formalized by David Hilbert, Paul Dirac and John von Neumann[96] with greater emphasis on measurement, the statistical nature of our knowledge of reality, and philosophical speculation about the 'observer'. It has since permeated many disciplines, including quantum chemistry, quantum electronics, quantum optics, and quantum information science. It also provides a useful framework for many features of the modern periodic table of elements, and describes the behaviors of atoms during chemical bonding and the flow of electrons in computer semiconductors, and therefore plays a crucial role in many modern technologies. While quantum mechanics was constructed to describe the world of the very small, it is also needed to explain some macroscopic phenomena such as superconductors[97] and superfluids.[98]\n\nThe following titles, all by working physicists, attempt to communicate quantum theory to lay people, using a minimum of technical apparatus.\n\nMore technical:\n"
    },
    {
        "title": "Artificial intelligence",
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "content": "\n\nArtificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1] Such machines may be called AIs.\n\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[2][3]\n\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics.[a] General intelligence—the ability to complete any task performed by a human on an at least equal level—is among the field's long-term goals.[4] To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.[b] AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.[5]\n\nArtificial intelligence was founded as an academic discipline in 1956,[6] and the field went through multiple cycles of optimism throughout its history,[7][8] followed by periods of disappointment and loss of funding, known as AI winters.[9][10] Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques.[11] This growth accelerated further after 2017 with the transformer architecture,[12] and by the early 2020s many billions of dollars were being invested in AI and the field experienced rapid ongoing progress in what has become known as the AI boom. The emergence of advanced generative AI in the midst of the AI boom and its ability to create and modify content exposed several unintended consequences and harms in the present and raised concerns about the risks of AI and its long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.[a]\n\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[13] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.[14]\n\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow.[15] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[16] Accurate and efficient reasoning is an unsolved problem.\n\nKnowledge representation and knowledge engineering[17] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,[18] scene interpretation,[19] clinical decision support,[20] knowledge discovery (mining \"interesting\" and actionable inferences from large databases),[21] and other areas.[22]\n\nA knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge.[23] Knowledge bases need to represent things such as objects, properties, categories, and relations between objects;[24] situations, events, states, and time;[25] causes and effects;[26] knowledge about knowledge (what we know about what other people know);[27] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[28] and many other aspects and domains of knowledge.\n\nAmong the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous);[29] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally).[16] There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.[c]\n\nAn \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen.[d][32] In automated planning, the agent has a specific goal.[33] In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.[34]\n\nIn classical planning, the agent knows exactly what the effect of any action will be.[35] In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.[36]\n\nIn some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences.[37] Information value theory can be used to weigh the value of exploratory or experimental actions.[38] The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\n\nA Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.[39]\n\nGame theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.[40]\n\nMachine learning is the study of programs that can improve their performance on a given task automatically.[41] It has been a part of AI from the beginning.[e]\n\nThere are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.[44] Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).[45]\n\nIn reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\".[46] Transfer learning is when the knowledge gained from one problem is applied to a new problem.[47] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.[48]\n\nComputational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[49]\n\nNatural language processing (NLP)[50] allows programs to read, write and communicate in human languages such as English. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.[51]\n\nEarly work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation[f] unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem[29]). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\n\nModern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning),[52] transformers (a deep learning architecture using an attention mechanism),[53] and others.[54] In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text,[55][56] and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.[57]\n\nMachine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.[58]\n\nThe field includes speech recognition,[59] image classification,[60] facial recognition, object recognition,[61]object tracking,[62] and robotic perception.[63]\n\nAffective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood.[65] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.\n\nHowever, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents.[66] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.[67]\n\nA machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.[4]\n\nAI research uses a wide variety of techniques to accomplish the goals above.[b]\n\nAI can solve many problems by intelligently searching through many possible solutions.[68] There are two very different kinds of search used in AI: state space search and local search.\n\nState space search searches through a tree of possible states to try to find a goal state.[69] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[70]\n\nSimple exhaustive searches[71] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes.[15] \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.[72]\n\nAdversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.[73]\n\nLocal search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.[74]\n\nGradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks.[75]\n\nAnother type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.[76]\n\nDistributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[77]\n\nFormal logic is used for reasoning and knowledge representation.[78]\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\")[79] and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").[80]\n\nDeductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises).[81] Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\n\nGiven a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem.[82] In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.[83]\n\nInference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.[84]\n\nFuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.[85]\n\nNon-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.[28] Other specialized versions of logic have been developed to describe many complex domains.\n\nMany problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[86] Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[87] and information value theory.[88] These tools include models such as Markov decision processes,[89] dynamic decision networks,[90] game theory and mechanism design.[91]\n\nBayesian networks[92] are a tool that can be used for reasoning (using the Bayesian inference algorithm),[g][94] learning (using the expectation–maximization algorithm),[h][96] planning (using decision networks)[97] and perception (using dynamic Bayesian networks).[90]\n\nProbabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[90]\n\nThe simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers[98] are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[45]\n\nThere are many kinds of classifiers in use.[99] The decision tree is the simplest and most widely used symbolic machine learning algorithm.[100] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[101]\nThe naive Bayes classifier is reportedly the \"most widely used learner\"[102] at Google, due in part to its scalability.[103]\nNeural networks are also used as classifiers.[104]\n\nAn artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.[104]\n\nLearning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.[105] Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.[106]\n\nIn feedforward neural networks the signal passes in only one direction.[107] Recurrent neural networks feed the output signal back into the input, which allows short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks.[108] Perceptrons[109] use only a single layer of neurons; deep learning[110] uses multiple layers. Convolutional neural networks strengthen the connection between neurons that are \"close\" to each other—this is especially important in image processing, where a local set of neurons must identify an \"edge\" before the network can identify an object.[111]\n\nDeep learning[110] uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.[112]\n\nDeep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification,[113] and others. The reason that deep learning performs so well in so many applications is not known as of 2023.[114] The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)[i] but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.[j]\n\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pretrained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\", although this can be reduced with RLHF and quality data. They are used in chatbots, which allow people to ask a question or request a task in simple text.[122][123]\n\nCurrent models and services include Gemini (formerly Bard), ChatGPT, Grok, Claude, Copilot, and LLaMA.[124] Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.[125]\n\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.[126] Specialized programming languages such as Prolog were used in early AI research,[127] but general-purpose programming languages like Python have become predominant.[128]\n\nThe transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster.[129]\n\nAI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok). The deployment of AI may be overseen by a Chief automation officer (CAO).\n\nThe application of AI in medicine and medical research has the potential to increase patient care and quality of life.[130] Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.[131][132]\n\nFor medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication.[133] It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.[133] New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[134] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.[135] In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.[136][137]\n\nApplications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer prediction,[138] AI-integrated sex toys (e.g., teledildonics),[139] AI-generated sexual education content,[140] and AI agents that simulate sexual and romantic partners (e.g., Replika).[141]  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.[142]\n\nAI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.[143][144]\n\nGame playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques.[145] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[146] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[147] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world.[148] Other programs handle imperfect-information games, such as the poker-playing program Pluribus.[149] DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games.[150] In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map.[151] In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning.[152] In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.[153]\n\nIn mathematics, special forms of formal step-by-step reasoning are used.[154] In contrast, LLMs such as GPT-4 Turbo, Gemini Ultra, Claude Opus, LLaMa-2 or Mistral Large are working with probabilistic models, which can produce wrong answers in the form of hallucinations. Therefore, they need not only a large database of mathematical problems to learn from but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections.[155] A 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.[156]\n\nAlternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as Alpha Tensor, Alpha Geometry and Alpha Proof all from Google DeepMind,[157] Llemma from eleuther[158] or Julius.[159]\n\nWhen natural language is used to describe mathematical problems, converters transform such prompts into a formal language such as Lean to define mathematical tasks.\n\nSome models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.[160]\n\nFinance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.[161]\n\nWorld Pensions experts like Nicolas Firzli insist it may be too early to see the emergence of highly innovative AI-informed financial products and services: \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"[162]\n\nVarious countries are deploying AI military applications.[163] The main applications enhance command and control, communications, sensors, integration and interoperability.[164] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[163] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams.[164]\n\nAI has been used in military operations in Iraq, Syria, Israel and Ukraine.[163][165][166][167]\n\nIn the early 2020s, generative AI gained widespread prominence. GenAI is AI capable of generating text, images, videos, or other data using generative models,[168][169] often in response to prompts.[170][171]\n\nIn March 2023, 58% of U.S. adults had heard about ChatGPT and 14% had tried it.[172] The increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon, as well as the usage in professional creative arts.[173][174]\n\nArtificial intelligent (AI) agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.[175][176][177]\n\nThere are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes.[178] A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\n\nAI applications for evacuation and disaster management are growing. AI has been used to investigate if and how people evacuated in large scale and small scale evacuations using historical data from GPS, videos or social media. Further, AI can provide real time information on the real time evacuation conditions.[179][180][181]\n\nIn agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\n\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\n\nDuring the 2024 Indian elections, US$50 millions was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.[182]\n\nAI has potential benefits and potential risks.[183] AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\".[184] However, as the use of AI has become widespread, several unintended consequences and risks have been identified.[185] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.[186]\n\nMachine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\n\nAI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.\n\nSensitive user data collected may include online activity records, geolocation data, video or audio.[187] For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them.[188] Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.[189]\n\nAI developers argue that this is the only way to deliver valuable applications. and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy.[190] Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"[191]\n\nGenerative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\".[192][193] Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file.[194] In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI.[195][196] Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.[197]\n\nThe commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft.[198][199][200] Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.[201][202]\n\nIn January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use.[203] This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.[204]\n\nProdigious power consumption by AI is responsible for the growth of fossil fuels use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.[205]\n\nA 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means.[206] Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.[207]\n\nIn 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for $650 Million (US).[208] Nvidia CEO Jen-Hsun Huang said nuclear power is a good option for the data centers.[209]\n\nIn September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at $1.6 billion (US) and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act.[210] The US government and the state of Michigan are investing almost $2 billion (US) to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon spinoff of Constellation.[211]\n\nAfter the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages.[212] Taiwan aims to phase out nuclear power by 2025.[212] On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.[212]\n\nAlthough most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for generative AI.[213] Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.[213]\n\nOn 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center.[214] \nAccording to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.[214]\n\nYouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation.[215] This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government.[216] The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took steps to mitigate the problem [citation needed].\n\nIn 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films, or human writing. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda.[217] AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.[218]\n\nMachine learning applications will be biased[k] if they learn from biased data.[220] The developers may not be aware that the bias exists.[221] Bias can be introduced by the way training data is selected and by the way a model is deployed.[222][220] If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.[223] The field of fairness studies how to prevent harms from algorithmic biases.\n\nOn June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people,[224] a problem called \"sample size disparity\".[225] Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.[226]\n\nCOMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend.[227] In 2017, several researchers[l] showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.[229]\n\nA program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\".[230] Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"[231]\n\nCriticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist.[232] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.[m]\n\nBias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.[225]\n\nThere are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.[219]\n\nAt its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.[dubious – discuss][234]\n\nMany AI systems are so complex that their designers cannot explain how they reach their decisions.[235] Particularly with deep neural networks, in which there are a large amount of non-linear relationships between inputs and outputs. But some popular explainability techniques exist.[236]\n\nIt is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale.[237] Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.[238]\n\nPeople who have been harmed by an algorithm's decision have a right to an explanation.[239] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists.[n] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.[240]\n\nDARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.[241]\n\nSeveral approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output.[242] LIME can locally approximate a model's outputs with a simpler, interpretable model.[243] Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.[244] Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning.[245] For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.[246]\n\nArtificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\n\nA lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision.[o] Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction.[248] Even when used in conventional warfare, it is unlikely that they will be unable to reliably choose targets and could potentially kill an innocent person.[248] In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed.[249] By 2015, over fifty countries were reported to be researching battlefield robots.[250]\n\nAI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware.[251] All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.[252][253]\n\nThere many other ways that AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.[254]\n\nEconomists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.[255]\n\nIn the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI.[256] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[257] Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\".[p][259] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.[255] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.[260][261]\n\nUnlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\".[262] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[263]\n\nFrom the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.[264]\n\nIt has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\".[265] This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character.[q] These sci-fi scenarios are misleading in several ways.\n\nFirst, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager).[267] Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\"[268] In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".[269]\n\nSecond, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.[270]\n\nThe opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI.[271] Personalities such as Stephen Hawking, Bill Gates, and Elon Musk,[272] as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.\n\nIn May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google.\"[273] He notably mentioned risks of an AI takeover,[274] and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.[275]\n\nIn 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".[276]\n\nSome other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\"[277] While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"[278][279] Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\"[280] Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\"[281] In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine.[282] However, after 2016, the study of current and future risks and possible solutions became a serious area of research.[283]\n\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[284]\n\nMachines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[285]\nThe field of machine ethics is also called computational morality,[285]\nand was founded at an AAAI symposium in 2005.[286]\n\nOther approaches include Wendell Wallach's \"artificial moral agents\"[287] and Stuart J. Russell's three principles for developing provably beneficial machines.[288]\n\nActive organizations in the AI open-source community include Hugging Face,[289] Google,[290] EleutherAI and Meta.[291] Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight,[292][293] meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case.[294] Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.[295]\n\nArtificial Intelligence projects can have their ethical permissibility tested while designing, developing, and implementing an AI system. An AI framework such as the Care and Act Framework containing the SUM values—developed by the Alan Turing Institute tests projects in four main areas:[296][297]\n\nOther developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others;[298] however, these principles do not go without their criticisms, especially regards to the people chosen contributes to these frameworks.[299]\n\nPromotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.[300]\n\nThe UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under a MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.[301]\n\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms.[302] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[303] According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.[304][305] Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[306] Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[306] The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[306] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[307] In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.[308] In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics.[309] In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.[310]\n\nIn a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\".[304] A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.[311] In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".[312][313]\n\nIn November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[314] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.[315][316] In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.[317][318]\n\nThe study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning.[319][320] This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\".[r] They developed several areas of research that would become part of AI,[322] such as McCullouch and Pitts design for \"artificial neurons\" in 1943,[115] and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.[323][320]\n\nThe field of AI research was founded at a workshop at Dartmouth College in 1956.[s][6] The attendees became the leaders of AI research in the 1960s.[t] They and their students produced programs that the press described as \"astonishing\":[u] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[v][7] Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.[320]\n\nResearchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field.[327] In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".[328] In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".[329] They had, however, underestimated the difficulty of the problem.[w] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill[331] and ongoing pressure from the U.S. Congress to fund more productive projects.[332] Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether.[333] The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.[9]\n\nIn the early 1980s, AI research was revived by the commercial success of expert systems,[334] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[8] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[10]\n\nUp to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition,[335] and began to look into \"sub-symbolic\" approaches.[336] Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive.[x] Judea Pearl, Lofti Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic.[86][341] But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others.[342] In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.[343]\n\nAI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).[344] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).[345]\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.[4]\n\nDeep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.[11]\nFor many specific tasks, other methods were abandoned.[y]\nDeep learning's success was based on both hardware improvements (faster computers,[347] graphics processing units, cloud computing[348]) and access to large amounts of data[349] (including curated datasets,[348] such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.[z] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.[306]\n\nIn 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.[283]\n\nIn the late teens and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text.[350] ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months.[351] It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness.[352] These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about $50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\".[353] About 800,000 \"AI\"-related U.S. job openings existed in 2022.[354] According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.[355]\n\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines.[356] Another major focus has been whether machines can be conscious, and the associated ethical implications.[357] Many other topics in philosophy are relevant to AI, such as epistemology and free will.[358] Rapid advancements have intensified public discussions on the philosophy and ethics of AI.[357]\n\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"[359] He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".[359] He devised the Turing test, which measures the ability of a machine to simulate human conversation.[323] Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"[360]\n\nRussell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure.[1] However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\"[362] AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".[363]\n\nMcCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\".[364] Another AI founder, Marvin Minsky similarly describes it as \"the ability to solve hard problems\".[365] The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals.[1] These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible.\n\nAnother definition has been adopted by Google,[366] a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\n\nSome authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI,[367] with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".[368]\n\nNo established unifying theory or paradigm has guided AI research for most of its history.[aa] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\n\nSymbolic AI (or \"GOFAI\")[370] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"[371]\n\nHowever, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.[372] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.[373] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.[ab][16]\n\nThe issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[375][376] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\n\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s,[377] but eventually was seen as irrelevant. Modern AI has elements of both.\n\nFinding a provably correct or optimal solution is intractable for many important problems.[15] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.[378][379] General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.\n\nThe philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\"[380] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness.[381] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[382]\n\nComputationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[383]\n\nPhilosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[ac] Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.[387]\n\nIt is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree.[388] But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals.[389][390] Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights.[389] Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.[391]\n\nIn 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities.[392] Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own.[393][394]\n\nProgress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.[390][389]\n\nA superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.[379] If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".[395]\n\nHowever, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.[396]\n\nRobot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.[397]\n\nEdward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.[398]\n\nThought-capable artificial beings have appeared as storytelling devices since antiquity,[399] and have been a persistent theme in science fiction.[400]\n\nA common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[401]\n\nIsaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics;[402] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[403]\n\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[404]\n\nThe two most widely used textbooks in 2023 (see the Open Syllabus):\n\nThe four most widely used AI textbooks in 2008:\n\nOther textbooks:\n"
    },
    {
        "title": "Space exploration",
        "url": "https://en.wikipedia.org/wiki/Space_exploration",
        "content": "\n\nSpace exploration is the use of astronomy and space technology to explore outer space.[1] While the exploration of space is currently carried out mainly by astronomers with telescopes, its physical exploration is conducted both by uncrewed robotic space probes and human spaceflight. Space exploration, like its classical form astronomy, is one of the main sources for space science.\n\nWhile the observation of objects in space, known as astronomy, predates reliable recorded history, it was the development of large and relatively efficient rockets during the mid-twentieth century that allowed physical space exploration to become a reality. Common rationales for exploring space include advancing scientific research, national prestige, uniting different nations, ensuring the future survival of humanity, and developing military and strategic advantages against other countries.[2]\n\nThe early era of space exploration was driven by a \"Space Race\" between the Soviet Union and the United States. A driving force of the start of space exploration was during the Cold War. After the ability to create nuclear weapons, the narrative of defense/offense left land and the power to control the air the focus. Both the Soviet Union and the U.S. were racing to prove their superiority in technology through exploring space. In fact, the reason NASA was created was as a response to Sputnik I.[3]\n\nThe launch of the first human-made object to orbit Earth, the Soviet Union's Sputnik 1, on 4 October 1957, and the first Moon landing by the American Apollo 11 mission on 20 July 1969 are often taken as landmarks for this initial period. The Soviet space program achieved many of the first milestones, including the first living being in orbit in 1957, the first human spaceflight (Yuri Gagarin aboard Vostok 1) in 1961, the first spacewalk (by Alexei Leonov) on 18 March 1965, the first automatic landing on another celestial body in 1966, and the launch of the first space station (Salyut 1) in 1971. After the first 20 years of exploration, focus shifted from one-off flights to renewable hardware, such as the Space Shuttle program, and from competition to cooperation as with the International Space Station (ISS).\n\nWith the substantial completion of the ISS[4] following STS-133 in March 2011, plans for space exploration by the U.S. remained in flux. The Constellation program aiming for a return to the Moon by 2020[5] was judged unrealistic by an expert review panel reporting in 2009.[6] Constellation ultimately was replaced with the Artemis Program, of which the first mission occurred in 2022, with a planned crewed landing to occur with Artemis III.[7] The rise of the private space industry also began in earnest in the 2010s with the development of private launch vehicles, space capsules and satellite manufacturing.\n\nIn the 2000s, China initiated a successful crewed spaceflight program while India launched the Chandrayaan programme, while the European Union and Japan have also planned future crewed space missions. The two primary global programs gaining traction in the 2020s are the Chinese-led International Lunar Research Station and the US-led Artemis Program, with its plan to build the Lunar Gateway and the Artemis Base Camp, each having its own set of international partners.\n\nThe first telescope is said to have been invented in 1608 in the Netherlands by an eyeglass maker named Hans Lippershey, but their first recorded use in astronomy was by Galileo Galilei in 1609.[8] In 1668 Isaac Newton built his own reflecting telescope, the first fully functional telescope of this kind, and a landmark for future developments due to its superior features over the previous Galilean telescope.[9]\n\nA string of discoveries in the Solar System (and beyond) followed, then and in the next centuries: the mountains of the Moon, the phases of Venus, the main satellites of Jupiter and Saturn, the rings of Saturn, many comets, the asteroids, the new planets Uranus and Neptune, and many more satellites.\n\nThe Orbiting Astronomical Observatory 2 was the first space telescope launched 1968,[10] but the launching of Hubble Space Telescope in 1990[11] set a milestone. As of 1 December 2022, there were 5,284 confirmed exoplanets discovered. The Milky Way is estimated to contain 100–400 billion stars[12] and more than 100 billion planets.[13] There are at least 2 trillion galaxies in the observable universe.[14][15] HD1 is the most distant known object from Earth, reported as 33.4 billion light-years away.[16][17][18][19][20][21]\n\nMW 18014 was a German V-2 rocket test launch that took place on 20 June 1944, at the Peenemünde Army Research Center in Peenemünde. It was the first human-made object to reach outer space, attaining an apogee of 176 kilometers,[22] which is well above the Kármán line.[23] It was a vertical test launch. Although the rocket reached space, it did not reach orbital velocity, and therefore returned to Earth in an impact, becoming the first sub-orbital spaceflight.[24] In 1949, the Bumper-WAC reached an altitude of 393 kilometres (244 mi), becoming the first human-made object to enter space, according to NASA.[25]\n\nThe first successful orbital launch was of the Soviet uncrewed Sputnik 1 (\"Satellite 1\") mission on 4 October 1957. The satellite weighed about 83 kg (183 lb), and is believed to have orbited Earth at a height of about 250 km (160 mi). It had two radio transmitters (20 and 40 MHz), which emitted \"beeps\" that could be heard by radios around the globe. Analysis of the radio signals was used to gather information about the electron density of the ionosphere, while temperature and pressure data was encoded in the duration of radio beeps. The results indicated that the satellite was not punctured by a meteoroid. Sputnik 1 was launched by an R-7 rocket. It burned up upon re-entry on 3 January 1958.\n\nThe first successful human spaceflight was Vostok 1 (\"East 1\"), carrying the 27-year-old Russian cosmonaut, Yuri Gagarin, on 12 April 1961. The spacecraft completed one orbit around the globe, lasting about 1 hour and 48 minutes. Gagarin's flight resonated around the world; it was a demonstration of the advanced Soviet space program and it opened an entirely new era in space exploration: human spaceflight.\n\nThe first artificial object to reach another celestial body was Luna 2 reaching the Moon in 1959.[26] The first soft landing on another celestial body was performed by Luna 9 landing on the Moon on 3 February 1966.[27] Luna 10 became the first artificial satellite of the Moon, entering in a lunar orbit on 3 April 1966.[28]\n\nThe first crewed landing on another celestial body was performed by Apollo 11 on 20 July 1969, landing on the Moon. There have been a total of six spacecraft with humans landing on the Moon starting from 1969 to the last human landing in 1972.\n\nThe first interplanetary flyby was the 1961 Venera 1 flyby of Venus, though the 1962 Mariner 2 was the first flyby of Venus to return data (closest approach 34,773 kilometers). Pioneer 6 was the first satellite to orbit the Sun, launched on 16 December 1965. The other planets were first flown by in 1965 for Mars by Mariner 4, 1973 for Jupiter by Pioneer 10, 1974 for Mercury by Mariner 10, 1979 for Saturn by Pioneer 11, 1986 for Uranus by Voyager 2, 1989 for Neptune by Voyager 2. In 2015, the dwarf planets Ceres and Pluto were orbited by Dawn and passed by New Horizons, respectively. This accounts for flybys of each of the eight planets in the Solar System, the Sun, the Moon, and Ceres and Pluto (two of the five recognized dwarf planets).\n\nThe first interplanetary surface mission to return at least limited surface data from another planet was the 1970 landing of Venera 7, which returned data to Earth for 23 minutes from Venus. In 1975, Venera 9 was the first to return images from the surface of another planet, returning images from Venus. In 1971, the Mars 3 mission achieved the first soft landing on Mars returning data for almost 20 seconds. Later, much longer duration surface missions were achieved, including over six years of Mars surface operation by Viking 1 from 1975 to 1982 and over two hours of transmission from the surface of Venus by Venera 13 in 1982, the longest ever Soviet planetary surface mission. Venus and Mars are the two planets outside of Earth on which humans have conducted surface missions with uncrewed robotic spacecraft.\n\nSalyut 1 was the first space station of any kind, launched into low Earth orbit by the Soviet Union on 19 April 1971. The International Space Station (ISS) is currently the largest and oldest of the 2 current fully functional space stations, inhabited continuously since the year 2000. The other, Tiangong space station built by China, is now fully crewed and operational.\n\nVoyager 1 became the first human-made object to leave the Solar System into interstellar space on 25 August 2012. The probe passed the heliopause at 121 AU to enter interstellar space.[29]\n\nThe Apollo 13 flight passed the far side of the Moon at an altitude of 254 kilometers (158 miles; 137 nautical miles) above the lunar surface, and 400,171 km (248,655 mi) from Earth, marking the record for the farthest humans have ever traveled from Earth in 1970.\n\nAs of 26 November 2022[update] Voyager 1 was at a distance of 159 AU (23.8 billion km; 14.8 billion mi) from Earth.[30] It is the most distant human-made object from Earth.[31]\n\nStarting in the mid-20th century probes and then human missions were sent into Earth orbit, and then on to the Moon. Also, probes were sent throughout the known Solar System, and into Solar orbit. Uncrewed spacecraft have been sent into orbit around Saturn, Jupiter, Mars, Venus, and Mercury by the 21st century, and the most distance active spacecraft, Voyager 1 and 2 traveled beyond 100 times the Earth-Sun distance. The instruments were enough though that it is thought they have left the Sun's heliosphere, a sort of bubble of particles made in the Galaxy by the Sun's solar wind.\n\nThe Sun is a major focus of space exploration. Being above the atmosphere in particular and Earth's magnetic field gives access to the solar wind and infrared and ultraviolet radiations that cannot reach Earth's surface. The Sun generates most space weather, which can affect power generation and transmission systems on Earth and interfere with, and even damage, satellites and space probes. Numerous spacecraft dedicated to observing the Sun, beginning with the Apollo Telescope Mount, have been launched and still others have had solar observation as a secondary objective. Parker Solar Probe, launched in 2018, will approach the Sun to within 1/9th the orbit of Mercury.\n\nMercury remains the least explored of the Terrestrial planets. As of May 2013, the Mariner 10 and MESSENGER missions have been the only missions that have made close observations of Mercury. MESSENGER entered orbit around Mercury in March 2011, to further investigate the observations made by Mariner 10 in 1975 (Munsell, 2006b). A third mission to Mercury, scheduled to arrive in 2025, BepiColombo is to include two probes. BepiColombo is a joint mission between Japan and the European Space Agency. MESSENGER and BepiColombo are intended to gather complementary data to help scientists understand many of the mysteries discovered by Mariner 10's flybys.\n\nFlights to other planets within the Solar System are accomplished at a cost in energy, which is described by the net change in velocity of the spacecraft, or delta-v. Due to the relatively high delta-v to reach Mercury and its proximity to the Sun, it is difficult to explore and orbits around it are rather unstable.\n\nVenus was the first target of interplanetary flyby and lander missions and, despite one of the most hostile surface environments in the Solar System, has had more landers sent to it (nearly all from the Soviet Union) than any other planet in the Solar System. The first flyby was the 1961 Venera 1, though the 1962 Mariner 2 was the first flyby to successfully return data. Mariner 2 has been followed by several other flybys by multiple space agencies often as part of missions using a Venus flyby to provide a gravitational assist en route to other celestial bodies. In 1967, Venera 4 became the first probe to enter and directly examine the atmosphere of Venus. In 1970, Venera 7 became the first successful lander to reach the surface of Venus and by 1985 it had been followed by eight additional successful Soviet Venus landers which provided images and other direct surface data. Starting in 1975, with the Soviet orbiter Venera 9, some ten successful orbiter missions have been sent to Venus, including later missions which were able to map the surface of Venus using radar to pierce the obscuring atmosphere.\n\nSpace exploration has been used as a tool to understand Earth as a celestial object. Orbital missions can provide data for Earth that can be difficult or impossible to obtain from a purely ground-based point of reference.\n\nFor example, the existence of the Van Allen radiation belts was unknown until their discovery by the United States' first artificial satellite, Explorer 1. These belts contain radiation trapped by Earth's magnetic fields, which currently renders construction of habitable space stations above 1000 km impractical. Following this early unexpected discovery, a large number of Earth observation satellites have been deployed specifically to explore Earth from a space-based perspective. These satellites have significantly contributed to the understanding of a variety of Earth-based phenomena. For instance, the hole in the ozone layer was found by an artificial satellite that was exploring Earth's atmosphere, and satellites have allowed for the discovery of archeological sites or geological formations that were difficult or impossible to otherwise identify.\n\nThe Moon was the first celestial body to be the object of space exploration. It holds the distinctions of being the first remote celestial object to be flown by, orbited, and landed upon by spacecraft, and the only remote celestial object ever to be visited by humans.\n\nIn 1959, the Soviets obtained the first images of the far side of the Moon, never previously visible to humans. The U.S. exploration of the Moon began with the Ranger 4 impactor in 1962. Starting in 1966, the Soviets successfully deployed a number of landers to the Moon which were able to obtain data directly from the Moon's surface; just four months later, Surveyor 1 marked the debut of a successful series of U.S. landers. The Soviet uncrewed missions culminated in the Lunokhod program in the early 1970s, which included the first uncrewed rovers and also successfully brought lunar soil samples to Earth for study. This marked the first (and to date the only) automated return of extraterrestrial soil samples to Earth. Uncrewed exploration of the Moon continues with various nations periodically deploying lunar orbiters. China's Chang'e 4 in 2019 and Chang'e 6 in 2024 achieved the world's first landing and sample return on the far side of the Moon. India's Chandrayaan-3 in 2023 achieved the world's first landing on the lunar south pole region.\n\nCrewed exploration of the Moon began in 1968 with the Apollo 8 mission that successfully orbited the Moon, the first time any extraterrestrial object was orbited by humans. In 1969, the Apollo 11 mission marked the first time humans set foot upon another world. Crewed exploration of the Moon did not continue for long. The Apollo 17 mission in 1972 marked the sixth landing and the most recent human visit. Artemis II is scheduled to complete a crewed flyby of the Moon in 2025, and Artemis III will perform the first lunar landing since Apollo 17 with it scheduled for launch no earlier than 2026. Robotic missions are still pursued vigorously.\n\nThe exploration of Mars has been an important part of the space exploration programs of the Soviet Union (later Russia), the United States, Europe, Japan and India. Dozens of robotic spacecraft, including orbiters, landers, and rovers, have been launched toward Mars since the 1960s. These missions were aimed at gathering data about current conditions and answering questions about the history of Mars. The questions raised by the scientific community are expected to not only give a better appreciation of the Red Planet but also yield further insight into the past, and possible future, of Earth.\n\nThe exploration of Mars has come at a considerable financial cost with roughly two-thirds of all spacecraft destined for Mars failing before completing their missions, with some failing before they even began. Such a high failure rate can be attributed to the complexity and large number of variables involved in an interplanetary journey, and has led researchers to jokingly speak of The Great Galactic Ghoul[32] which subsists on a diet of Mars probes. This phenomenon is also informally known as the \"Mars Curse\".[33] In contrast to overall high failure rates in the exploration of Mars, India has become the first country to achieve success of its maiden attempt. India's Mars Orbiter Mission (MOM)[34][35][36] is one of the least expensive interplanetary missions ever undertaken with an approximate total cost of ₹ 450 Crore (US$73 million).[37][38] The first mission to Mars by any Arab country has been taken up by the United Arab Emirates. Called the Emirates Mars Mission, it was launched on 19 July 2020 and went into orbit around Mars on 9 February 2021. The uncrewed exploratory probe was named \"Hope Probe\" and was sent to Mars to study its atmosphere in detail.[39]\n\nThe Russian space mission Fobos-Grunt, which launched on 9 November 2011, experienced a failure leaving it stranded in low Earth orbit.[40] It was to begin exploration of the Phobos and Martian circumterrestrial orbit, and study whether the moons of Mars, or at least Phobos, could be a \"trans-shipment point\" for spaceships traveling to Mars.[41]\n\nUntil the advent of space travel, objects in the asteroid belt were merely pinpricks of light in even the largest telescopes, their shapes and terrain remaining a mystery. Several asteroids have now been visited by probes, the first of which was Galileo, which flew past two: 951 Gaspra in 1991, followed by 243 Ida in 1993. Both of these lay near enough to Galileo's planned trajectory to Jupiter that they could be visited at acceptable cost. The first landing on an asteroid was performed by the NEAR Shoemaker probe in 2000, following an orbital survey of the object, 433 Eros. The dwarf planet Ceres and the asteroid 4 Vesta, two of the three largest asteroids, were visited by NASA's Dawn spacecraft, launched in 2007.\n\nHayabusa was a robotic spacecraft developed by the Japan Aerospace Exploration Agency to return a sample of material from the small near-Earth asteroid 25143 Itokawa to Earth for further analysis. Hayabusa was launched on 9 May 2003 and rendezvoused with Itokawa in mid-September 2005. After arriving at Itokawa, Hayabusa studied the asteroid's shape, spin, topography, color, composition, density, and history. In November 2005, it landed on the asteroid twice to collect samples. The spacecraft returned to Earth on 13 June 2010.\n\nThe exploration of Jupiter has consisted solely of a number of automated NASA spacecraft visiting the planet since 1973. A large majority of the missions have been \"flybys\", in which detailed observations are taken without the probe landing or entering orbit; such as in Pioneer and Voyager programs. The Galileo and Juno spacecraft are the only spacecraft to have entered the planet's orbit. As Jupiter is believed to have only a relatively small rocky core and no real solid surface, a landing mission is precluded.\n\nReaching Jupiter from Earth requires a delta-v of 9.2 km/s,[42] which is comparable to the 9.7 km/s delta-v needed to reach low Earth orbit.[43] Fortunately, gravity assists through planetary flybys can be used to reduce the energy required at launch to reach Jupiter, albeit at the cost of a significantly longer flight duration.[42]\n\nJupiter has 95 known moons, many of which have relatively little known information about them.\n\nSaturn has been explored only through uncrewed spacecraft launched by NASA, including one mission (Cassini–Huygens) planned and executed in cooperation with other space agencies. These missions consist of flybys in 1979 by Pioneer 11, in 1980 by Voyager 1, in 1982 by Voyager 2 and an orbital mission by the Cassini spacecraft, which lasted from 2004 until 2017.\n\nSaturn has at least 62 known moons, although the exact number is debatable since Saturn's rings are made up of vast numbers of independently orbiting objects of varying sizes. The largest of the moons is Titan, which holds the distinction of being the only moon in the Solar System with an atmosphere denser and thicker than that of Earth. Titan holds the distinction of being the only object in the Outer Solar System that has been explored with a lander, the Huygens probe deployed by the Cassini spacecraft.\n\nThe exploration of Uranus has been entirely through the Voyager 2 spacecraft, with no other visits currently planned. Given its axial tilt of 97.77°, with its polar regions exposed to sunlight or darkness for long periods, scientists were not sure what to expect at Uranus. The closest approach to Uranus occurred on 24 January 1986. Voyager 2 studied the planet's unique atmosphere and magnetosphere. Voyager 2 also examined its ring system and the moons of Uranus including all five of the previously known moons, while discovering an additional ten previously unknown moons.\n\nImages of Uranus proved to have a uniform appearance, with no evidence of the dramatic storms or atmospheric banding evident on Jupiter and Saturn. Great effort was required to even identify a few clouds in the images of the planet. The magnetosphere of Uranus, however, proved to be unique, being profoundly affected by the planet's unusual axial tilt. In contrast to the bland appearance of Uranus itself, striking images were obtained of the Moons of Uranus, including evidence that Miranda had been unusually geologically active.\n\nThe exploration of Neptune began with the 25 August 1989 Voyager 2 flyby, the sole visit to the system as of 2025. The possibility of a Neptune Orbiter has been discussed, but no other missions have been given serious thought.\n\nAlthough the extremely uniform appearance of Uranus during Voyager 2's visit in 1986 had led to expectations that Neptune would also have few visible atmospheric phenomena, the spacecraft found that Neptune had obvious banding, visible clouds, auroras, and even a conspicuous anticyclone storm system rivaled in size only by Jupiter's Great Red Spot. Neptune also proved to have the fastest winds of any planet in the Solar System, measured as high as 2,100 km/h.[44] Voyager 2 also examined Neptune's ring and moon system. It discovered 900 complete rings and additional partial ring \"arcs\" around Neptune. In addition to examining Neptune's three previously known moons, Voyager 2 also discovered five previously unknown moons, one of which, Proteus, proved to be the last largest moon in the system. Data from Voyager 2 supported the view that Neptune's largest moon, Triton, is a captured Kuiper belt object.[45]\n\nThe dwarf planet Pluto presents significant challenges for spacecraft because of its great distance from Earth (requiring high velocity for reasonable trip times) and small mass (making capture into orbit difficult at present). Voyager 1 could have visited Pluto, but controllers opted instead for a close flyby of Saturn's moon Titan, resulting in a trajectory incompatible with a Pluto flyby. Voyager 2 never had a plausible trajectory for reaching Pluto.[46]\n\nAfter an intense political battle, a mission to Pluto dubbed New Horizons was granted funding from the United States government in 2003.[47] New Horizons was launched successfully on 19 January 2006. In early 2007 the craft made use of a gravity assist from Jupiter. Its closest approach to Pluto was on 14 July 2015; scientific observations of Pluto began five months prior to closest approach and continued for 16 days after the encounter.\n\nThe New Horizons mission also performed a flyby of the small planetesimal Arrokoth, in the Kuiper belt, in 2019. This was its first extended mission.[48]\n\nAlthough many comets have been studied from Earth sometimes with centuries-worth of observations, only a few comets have been closely visited. In 1985, the International Cometary Explorer conducted the first comet fly-by (21P/Giacobini-Zinner) before joining the Halley Armada studying the famous comet. The Deep Impact probe smashed into 9P/Tempel to learn more about its structure and composition and the Stardust mission returned samples of another comet's tail. The Philae lander successfully landed on Comet Churyumov–Gerasimenko in 2014 as part of the broader Rosetta mission.\n\nDeep space exploration is the branch of astronomy, astronautics and space technology that is involved with the exploration of distant regions of outer space.[49] Physical exploration of space is conducted both by human spaceflights (deep-space astronautics) and by robotic spacecraft.\n\nSome of the best candidates for future deep space engine technologies include anti-matter, nuclear power and beamed propulsion.[50] Beamed propulsion, appears to be the best candidate for deep space exploration presently available, since it uses known physics and known technology that is being developed for other purposes.[51]\n\nBreakthrough Starshot is a research and engineering project by the Breakthrough Initiatives to develop a proof-of-concept fleet of light sail spacecraft named StarChip,[52] to be capable of making the journey to the Alpha Centauri star system 4.37 light-years away. It was founded in 2016 by Yuri Milner, Stephen Hawking, and Mark Zuckerberg.[53][54]\n\nAn article in the science magazine Nature suggested the use of asteroids as a gateway for space exploration, with the ultimate destination being Mars. In order to make such an approach viable, three requirements need to be fulfilled: first, \"a thorough asteroid survey to find thousands of nearby bodies suitable for astronauts to visit\"; second, \"extending flight duration and distance capability to ever-increasing ranges out to Mars\"; and finally, \"developing better robotic vehicles and tools to enable astronauts to explore an asteroid regardless of its size, shape or spin\". Furthermore, using asteroids would provide astronauts with protection from galactic cosmic rays, with mission crews being able to land on them without great risk to radiation exposure.\n\nThe Artemis program is an ongoing crewed spaceflight program carried out by NASA, U.S. commercial spaceflight companies, and international partners such as ESA,[55] with the goal of landing \"the first woman and the next man\" on the Moon, specifically at the lunar south pole region. Artemis would be the next step towards the long-term goal of establishing a sustainable presence on the Moon, laying the foundation for private companies to build a lunar economy, and eventually sending humans to Mars.\n\nIn 2017, the lunar campaign was authorized by Space Policy Directive 1, using various ongoing spacecraft programs such as Orion, the Lunar Gateway, Commercial Lunar Payload Services, and adding an undeveloped crewed lander. The Space Launch System will serve as the primary launch vehicle for Orion, while commercial launch vehicles are planned for use to launch other elements of the campaign.[56] NASA requested $1.6 billion in additional funding for Artemis for fiscal year 2020,[57] while the U.S. Senate Appropriations Committee requested from NASA a five-year budget profile[58] which is needed for evaluation and approval by the U.S. Congress.[59][60] As of 2024, the first Artemis mission was launched in 2022 with the second mission, a crewed lunar flyby planned for 2025.[61] Construction on the Lunar Gateway is underway with initial capabilities set for the 2025–2027 timeframe.[62] The first CLPS lander landed in 2024, marking the first US spacecraft to land since Apollo 17.[63]\n\nThe research that is conducted by national space exploration agencies, such as NASA and Roscosmos, is one of the reasons supporters cite to justify government expenses. Economic analyses of the NASA programs often showed ongoing economic benefits (such as NASA spin-offs), generating many times the revenue of the cost of the program.[64] It is also argued that space exploration would lead to the extraction of resources on other planets and especially asteroids, which contain billions of dollars worth of minerals and metals. Such expeditions could generate substantial revenue.[65] In addition, it has been argued that space exploration programs help inspire youth to study in science and engineering.[66] Space exploration also gives scientists the ability to perform experiments in other settings and expand humanity's knowledge.[67]\n\nAnother claim is that space exploration is a necessity to humankind and that staying on Earth will eventually lead to extinction. Some of the reasons are lack of natural resources, comets, nuclear war, and worldwide epidemic. Stephen Hawking, renowned British theoretical physicist, said, \"I don't think the human race will survive the next thousand years, unless we spread into space. There are too many accidents that can befall life on a single planet. But I'm an optimist. We will reach out to the stars.\"[68] Author Arthur C. Clarke (1950) presented a summary of motivations for the human exploration of space in his non-fiction semi-technical monograph Interplanetary Flight.[69] He argued that humanity's choice is essentially between expansion off Earth into space, versus cultural (and eventually biological) stagnation and death.\n\nThese motivations could be attributed to one of the first rocket scientists in NASA, Wernher von Braun, and his vision of humans moving beyond Earth. The basis of this plan was to:\n\nDevelop multi-stage rockets capable of placing satellites, animals, and humans in space.\n\nDevelopment of large, winged reusable spacecraft capable of carrying humans and equipment into Earth orbit in a way that made space access routine and cost-effective.\n\nConstruction of a large, permanently occupied space station to be used as a platform both to observe Earth and from which to launch deep space expeditions.\n\nLaunching the first human flights around the Moon, leading to the first landings of humans on the Moon, with the intent of exploring that body and establishing permanent lunar bases.\n\n\nAssembly and fueling of spaceships in Earth orbit for the purpose of sending humans to Mars with the intent of eventually colonizing that planet.[70]\nKnown as the Von Braun Paradigm, the plan was formulated to lead humans in the exploration of space. Von Braun's vision of human space exploration served as the model for efforts in space exploration well into the twenty-first century, with NASA incorporating this approach into the majority of their projects.[70] The steps were followed out of order, as seen by the Apollo program reaching the moon before the space shuttle program was started, which in turn was used to complete the International Space Station. Von Braun's Paradigm formed NASA's drive for human exploration, in the hopes that humans discover the far reaches of the universe.\n\nNASA has produced a series of public service announcement videos supporting the concept of space exploration.[71]\n\nOverall, the U.S. public remains largely supportive of both crewed and uncrewed space exploration. According to an Associated Press Poll conducted in July 2003, 71% of U.S. citizens agreed with the statement that the space program is \"a good investment\", compared to 21% who did not.[72]\n\nSpace advocacy and space policy[73] regularly invokes exploration as a human nature.[74]\n\nSpaceflight is the use of space technology to achieve the flight of spacecraft into and through outer space.\n\nSpaceflight is used in space exploration, and also in commercial activities like space tourism and satellite telecommunications. Additional non-commercial uses of spaceflight include space observatories, reconnaissance satellites and other Earth observation satellites.\n\nA spaceflight typically begins with a rocket launch, which provides the initial thrust to overcome the force of gravity and propels the spacecraft from the surface of Earth. Once in space, the motion of a spacecraft—both when unpropelled and when under propulsion—is covered by the area of study called astrodynamics. Some spacecraft remain in space indefinitely, some disintegrate during atmospheric reentry, and others reach a planetary or lunar surface for landing or impact.\n\nSatellites are used for a large number of purposes. Common types include military (spy) and civilian Earth observation satellites, communication satellites, navigation satellites, weather satellites, and research satellites. Space stations and human spacecraft in orbit are also satellites.\n\nThe commercialization of space first started out with the launching of private satellites by NASA or other space agencies. Current examples of the commercial satellite use of space include satellite navigation systems, satellite television, satellite communications (such as internet services) and satellite radio. The next step of commercialization of space was seen as human spaceflight. Flying humans safely to and from space had become routine to NASA and Russia.[75] Reusable spacecraft were an entirely new engineering challenge, something only seen in novels and films like Star Trek and War of the Worlds. Astronaut Buzz Aldrin supported the use of making a reusable vehicle like the space shuttle. Aldrin held that reusable spacecraft were the key in making space travel affordable, stating that the use of \"passenger space travel is a huge potential market big enough to justify the creation of reusable launch vehicles\".[76] Space tourism is a next step in the use of reusable vehicles in the commercialization of space. The purpose of this form of space travel is personal pleasure.\n\nPrivate spaceflight companies such as SpaceX and Blue Origin, and commercial space stations such as the Axiom Space and the Bigelow Commercial Space Station have changed the cost and overall landscape of space exploration, and are expected to continue to do so in the near future.\n\nAstrobiology is the interdisciplinary study of life in the universe, combining aspects of astronomy, biology and geology.[77] It is focused primarily on the study of the origin, distribution and evolution of life. It is also known as exobiology (from Greek: έξω, exo, \"outside\").[78][79][80] The term \"Xenobiology\" has been used as well, but this is technically incorrect because its terminology means \"biology of the foreigners\".[81] Astrobiologists must also consider the possibility of life that is chemically entirely distinct from any life found on Earth.[82] In the Solar System, some of the prime locations for current or past astrobiology are on Enceladus, Europa, Mars, and Titan.[83]\n\nTo date, the longest human occupation of space is the International Space Station which has been in continuous use for 24 years, 69 days. Valeri Polyakov's record single spaceflight of almost 438 days aboard the Mir space station has not been surpassed. The health effects of space have been well documented through years of research conducted in the field of aerospace medicine. Analog environments similar to those experienced in space travel (like deep sea submarines), have been used in this research to further explore the relationship between isolation and extreme environments.[84] It is imperative that the health of the crew be maintained as any deviation from baseline may compromise the integrity of the mission as well as the safety of the crew, hence the astronauts must endure rigorous medical screenings and tests prior to embarking on any missions. However, it does not take long for the environmental dynamics of spaceflight to commence its toll on the human body; for example, space motion sickness (SMS) – a condition which affects the neurovestibular system and culminates in mild to severe signs and symptoms such as vertigo, dizziness, fatigue, nausea, and disorientation – plagues almost all space travelers within their first few days in orbit.[84] Space travel can also have an impact on the psyche of the crew members as delineated in anecdotal writings composed after their retirement. Space travel can adversely affect the body's natural biological clock (circadian rhythm); sleep patterns causing sleep deprivation and fatigue; and social interaction; consequently, residing in a Low Earth Orbit (LEO) environment for a prolonged amount of time can result in both mental and physical exhaustion.[84] Long-term stays in space reveal issues with bone and muscle loss in low gravity, immune system suppression, problems with eyesight, and radiation exposure. The lack of gravity causes fluid to rise upward which can cause pressure to build up in the eye, resulting in vision problems; the loss of bone minerals and densities; cardiovascular deconditioning; and decreased endurance and muscle mass.[85]\n\nRadiation is an insidious health hazard to space travelers as it is invisible and can cause cancer. When above the Earth's magnetic field, spacecraft are no longer protected from the sun's radiation; the danger of radiation is even more potent in deep space. The hazards of radiation can be ameliorated through protective shielding on the spacecraft, alerts, and dosimetry.[86]\n\nFortunately, with new and rapidly evolving technological advancements, those in Mission Control are able to monitor the health of their astronauts more closely using telemedicine. One may not be able to completely evade the physiological effects of space flight, but those effects can be mitigated. For example, medical systems aboard space vessels such as the International Space Station (ISS) are well equipped and designed to counteract the effects of lack of gravity and weightlessness; on-board treadmills can help prevent muscle loss and reduce the risk of developing premature osteoporosis.[84][86] Additionally, a crew medical officer is appointed for each ISS mission and a flight surgeon is available 24/7 via the ISS Mission Control Center located in Houston, Texas.[86] Although the interactions are intended to take place in real time, communications between the space and terrestrial crew may become delayed – sometimes by as much as 20 minutes[86] – as their distance from each other increases when the spacecraft moves further out of low Earth orbit; because of this the crew are trained and need to be prepared to respond to any medical emergencies that may arise on the vessel as the ground crew are hundreds of miles away.\n\nMany past and current concepts for the continued exploration and colonization of space focus on a return to the Moon as a \"steppingstone\" to the other planets, especially Mars. At the end of 2006, NASA announced they were planning to build a permanent Moon base with continual presence by 2024.[87]\n\nBeyond the technical factors that could make living in space more widespread, it has been suggested that the lack of private property, the inability or difficulty in establishing property rights in space, has been an impediment to the development of space for human habitation. Since the advent of space technology in the latter half of the twentieth century, the ownership of property in space has been murky, with strong arguments both for and against. In particular, the making of national territorial claims in outer space and on celestial bodies has been specifically proscribed by the Outer Space Treaty, which had been, as of 2012[update], ratified by all spacefaring nations.[88] Space colonization, also called space settlement and space humanization, would be the permanent autonomous (self-sufficient) human habitation of locations outside Earth, especially of natural satellites or planets such as the Moon or Mars, using significant amounts of in-situ resource utilization.\n\nParticipation and representation of humanity in space is an issue ever since the first phase of space exploration.[89] Some rights of non-spacefaring countries have been mostly secured through international space law, declaring space the \"province of all mankind\", understanding spaceflight as its resource, though sharing of space for all humanity is still criticized as imperialist and lacking.[89] Additionally to international inclusion, the inclusion of women and people of colour has also been lacking. To reach a more inclusive spaceflight, some organizations like the Justspace Alliance[89] and IAU featured Inclusive Astronomy[90] have been formed in recent years.\n\nThe first woman to go to space was Valentina Tereshkova. She flew in 1963 but it was not until the 1980s that another woman entered space again. All astronauts were required to be military test pilots at the time and women were not able to join this career. This is one reason for the delay in allowing women to join space crews.[citation needed] After the rule changed, Svetlana Savitskaya became the second woman to go to space, she was also from the Soviet Union. Sally Ride became the next woman in space and the first woman to fly to space through the United States program.\n\nSince then, eleven other countries have allowed women astronauts. The first all-female space walk occurred in 2018, including Christina Koch and Jessica Meir. They had both previously participated in space walks with NASA. The first woman to go to the Moon is planned for 2026.\n\nDespite these developments, women are underrepresented among astronauts and especially cosmonauts. Issues that block potential applicants from the programs, and limit the space missions they are able to go on, include:\n\nArtistry in and from space ranges from signals, capturing and arranging material like Yuri Gagarin's selfie in space or the image The Blue Marble, over drawings like the first one in space by cosmonaut and artist Alexei Leonov, music videos like Chris Hadfield's cover of Space Oddity on board the ISS, to permanent installations on celestial bodies like on the Moon.\n\nSolar System → Local Interstellar Cloud → Local Bubble → Gould Belt → Orion Arm → Milky Way → Milky Way subgroup → Local Group → Local Sheet → Virgo Supercluster → Laniakea Supercluster → Local Hole → Observable universe → UniverseEach arrow (→) may be read as \"within\" or \"part of\".\n"
    },
    {
        "title": "Computer network",
        "url": "https://en.wikipedia.org/wiki/Computer_network",
        "content": "\n\nA computer network is a set of computers sharing resources located on or provided by network nodes. Computers use common communication protocols over digital interconnections to communicate with each other. These interconnections are made up of telecommunication network technologies based on physically wired, optical, and wireless radio-frequency methods that may be arranged in a variety of network topologies.\n\nThe nodes of a computer network can include personal computers, servers, networking hardware, or other specialized or general-purpose hosts. They are identified by network addresses and may have hostnames. Hostnames serve as memorable labels for the nodes and are rarely changed after initial assignment. Network addresses serve for locating and identifying the nodes by communication protocols such as the Internet Protocol.\n\nComputer networks may be classified by many criteria, including the transmission medium used to carry signals, bandwidth, communications protocols to organize network traffic, the network size, the topology, traffic control mechanisms, and organizational intent.[citation needed]\n\nComputer networks support many applications and services, such as access to the World Wide Web, digital video and audio, shared use of application and storage servers, printers and fax machines, and use of email and instant messaging applications.\n\nComputer networking may be considered a branch of computer science, computer engineering, and telecommunications, since it relies on the theoretical and practical application of the related disciplines. Computer networking was influenced by a wide array of technological developments and historical milestones.\n\nComputer networks enhance how users communicate with each other by using various electronic methods like email, instant messaging, online chat, voice and video calls, and video conferencing. Networks also enable the sharing of computing resources. For example, a user can print a document on a shared printer or use shared storage devices. Additionally, networks allow for the sharing of files and information, giving authorized users access to data stored on other computers. Distributed computing leverages resources from multiple computers across a network to perform tasks collaboratively.\n\nMost modern computer networks use protocols based on packet-mode transmission. A network packet is a formatted unit of data carried by a packet-switched network.\n\nPackets consist of two types of data: control information and user data (payload). The control information provides data the network needs to deliver the user data, for example, source and destination network addresses, error detection codes, and sequencing information. Typically, control information is found in packet headers and trailers, with payload data in between.\n\nWith packets, the bandwidth of the transmission medium can be better shared among users than if the network were circuit switched. When one user is not sending packets, the link can be filled with packets from other users, and so the cost can be shared, with relatively little interference, provided the link is not overused. Often the route a packet needs to take through a network is not immediately available. In that case, the packet is queued and waits until a link is free.\n\nThe physical link technologies of packet networks typically limit the size of packets to a certain maximum transmission unit (MTU). A longer message may be fragmented before it is transferred and once the packets arrive, they are reassembled to construct the original message.\n\nThe physical or geographic locations of network nodes and links generally have relatively little effect on a network, but the topology of interconnections of a network can significantly affect its throughput and reliability. With many technologies, such as bus or star networks, a single failure can cause the network to fail entirely. In general, the more interconnections there are, the more robust the network is; but the more expensive it is to install. Therefore, most network diagrams are arranged by their network topology which is the map of logical interconnections of network hosts.\n\nCommon topologies are:\n\nThe physical layout of the nodes in a network may not necessarily reflect the network topology. As an example, with FDDI, the network topology is a ring, but the physical topology is often a star, because all neighboring connections can be routed via a central physical location. Physical layout is not completely irrelevant, however, as common ducting and equipment locations can represent single points of failure due to issues like fires, power failures and flooding.\n\nAn overlay network is a virtual network that is built on top of another network. Nodes in the overlay network are connected by virtual or logical links. Each link corresponds to a path, perhaps through many physical links, in the underlying network. The topology of the overlay network may (and often does) differ from that of the underlying one. For example, many peer-to-peer networks are overlay networks. They are organized as nodes of a virtual system of links that run on top of the Internet.[52]\n\nOverlay networks have been used since the early days of networking, back when computers were connected via telephone lines using modems, even before data networks were developed.\n\nThe most striking example of an overlay network is the Internet itself. The Internet itself was initially built as an overlay on the telephone network.[52] Even today, each Internet node can communicate with virtually any other through an underlying mesh of sub-networks of wildly different topologies and technologies. Address resolution and routing are the means that allow mapping of a fully connected IP overlay network to its underlying network.\n\nAnother example of an overlay network is a distributed hash table, which maps keys to nodes in the network. In this case, the underlying network is an IP network, and the overlay network is a table (actually a map) indexed by keys.\n\nOverlay networks have also been proposed as a way to improve Internet routing, such as through quality of service guarantees achieve higher-quality streaming media. Previous proposals such as IntServ, DiffServ, and IP multicast have not seen wide acceptance largely because they require modification of all routers in the network.[citation needed] On the other hand, an overlay network can be incrementally deployed on end-hosts running the overlay protocol software, without cooperation from Internet service providers. The overlay network has no control over how packets are routed in the underlying network between two overlay nodes, but it can control, for example, the sequence of overlay nodes that a message traverses before it reaches its destination[citation needed].\n\nFor example, Akamai Technologies manages an overlay network that provides reliable, efficient content delivery (a kind of multicast). Academic research includes end system multicast,[53] resilient routing and quality of service studies, among others.\n\nThe transmission media (often referred to in the literature as the physical medium) used to link devices to form a computer network include electrical cable, optical fiber, and free space. In the OSI model, the software to handle the media is defined at layers 1 and 2 — the physical layer and the data link layer.\n\nA widely adopted family that uses copper and fiber media in local area network (LAN) technology are collectively known as Ethernet. The media and protocol standards that enable communication between networked devices over Ethernet are defined by IEEE 802.3. Wireless LAN standards use radio waves, others use infrared signals as a transmission medium. Power line communication uses a building's power cabling to transmit data.\n\nThe following classes of wired technologies are used in computer networking.\n\nNetwork connections can be established wirelessly using radio or other electromagnetic means of communication.\n\nThe last two cases have a large round-trip delay time, which gives slow two-way communication but does not prevent sending large amounts of information (they can have high throughput).\n\nApart from any physical transmission media, networks are built from additional basic system building blocks, such as network interface controllers, repeaters, hubs, bridges, switches, routers, modems, and firewalls. Any particular piece of equipment will frequently contain multiple building blocks and so may perform multiple functions.\n\nA network interface controller (NIC) is computer hardware that connects the computer to the network media and has the ability to process low-level network information. For example, the NIC may have a connector for plugging in a cable, or an aerial for wireless transmission and reception, and the associated circuitry.\n\nIn Ethernet networks, each NIC has a unique Media Access Control (MAC) address—usually stored in the controller's permanent memory. To avoid address conflicts between network devices, the Institute of Electrical and Electronics Engineers (IEEE) maintains and administers MAC address uniqueness. The size of an Ethernet MAC address is six octets. The three most significant octets are reserved to identify NIC manufacturers. These manufacturers, using only their assigned prefixes, uniquely assign the three least-significant octets of every Ethernet interface they produce.\n\nA repeater is an electronic device that receives a network signal, cleans it of unnecessary noise and regenerates it. The signal is retransmitted at a higher power level, or to the other side of obstruction so that the signal can cover longer distances without degradation. In most twisted-pair Ethernet configurations, repeaters are required for cable that runs longer than 100 meters. With fiber optics, repeaters can be tens or even hundreds of kilometers apart.\n\nRepeaters work on the physical layer of the OSI model but still require a small amount of time to regenerate the signal. This can cause a propagation delay that affects network performance and may affect proper function. As a result, many network architectures limit the number of repeaters used in a network, e.g., the Ethernet 5-4-3 rule.\n\nAn Ethernet repeater with multiple ports is known as an Ethernet hub. In addition to reconditioning and distributing network signals, a repeater hub assists with collision detection and fault isolation for the network. Hubs and repeaters in LANs have been largely obsoleted by modern network switches.\n\nNetwork bridges and network switches are distinct from a hub in that they only forward frames to the ports involved in the communication whereas a hub forwards to all ports.[57] Bridges only have two ports but a switch can be thought of as a multi-port bridge. Switches normally have numerous ports, facilitating a star topology for devices, and for cascading additional switches.\n\nBridges and switches operate at the data link layer (layer 2) of the OSI model and bridge traffic between two or more network segments to form a single local network. Both are devices that forward frames of data between ports based on the destination MAC address in each frame.[58]\nThey learn the association of physical ports to MAC addresses by examining the source addresses of received frames and only forward the frame when necessary. If an unknown destination MAC is targeted, the device broadcasts the request to all ports except the source, and discovers the location from the reply.\n\nBridges and switches divide the network's collision domain but maintain a single broadcast domain. Network segmentation through bridging and switching helps break down a large, congested network into an aggregation of smaller, more efficient networks.\n\nA router is an internetworking device that forwards packets between networks by processing the addressing or routing information included in the packet. The routing information is often processed in conjunction with the routing table. A router uses its routing table to determine where to forward packets and does not require broadcasting packets which is inefficient for very big networks.\n\nModems (modulator-demodulator) are used to connect network nodes via wire not originally designed for digital network traffic, or for wireless. To do this one or more carrier signals are modulated by the digital signal to produce an analog signal that can be tailored to give the required properties for transmission. Early modems modulated audio signals sent over a standard voice telephone line. Modems are still commonly used for telephone lines, using a digital subscriber line technology and cable television systems using DOCSIS technology.\n\nA firewall is a network device or software for controlling network security and access rules. Firewalls are inserted in connections between secure internal networks and potentially insecure external networks such as the Internet. Firewalls are typically configured to reject access requests from unrecognized sources while allowing actions from recognized ones. The vital role firewalls play in network security grows in parallel with the constant increase in cyber attacks.\n\nA communication protocol is a set of rules for exchanging information over a network. Communication protocols have various characteristics. They may be connection-oriented or connectionless, they may use circuit mode or packet switching, and they may use hierarchical addressing or flat addressing.\n\nIn a protocol stack, often constructed per the OSI model, communications functions are divided up into protocol layers, where each layer leverages the services of the layer below it until the lowest layer controls the hardware that sends information across the media. The use of protocol layering is ubiquitous across the field of computer networking. An important example of a protocol stack is HTTP (the World Wide Web protocol) running over TCP over IP (the Internet protocols) over IEEE 802.11 (the Wi-Fi protocol). This stack is used between the wireless router and the home user's personal computer when the user is surfing the web.\n\nThere are many communication protocols, a few of which are described below.\n\nThe Internet protocol suite, also called TCP/IP, is the foundation of all modern networking. It offers connection-less and connection-oriented services over an inherently unreliable network traversed by datagram transmission using Internet protocol (IP). At its core, the protocol suite defines the addressing, identification, and routing specifications for Internet Protocol Version 4 (IPv4) and for IPv6, the next generation of the protocol with a much enlarged addressing capability. The Internet protocol suite is the defining set of protocols for the Internet.[59]\n\nIEEE 802 is a family of IEEE standards dealing with local area networks and metropolitan area networks. The complete IEEE 802 protocol suite provides a diverse set of networking capabilities. The protocols have a flat addressing scheme. They operate mostly at layers 1 and 2 of the OSI model.\n\nFor example, MAC bridging (IEEE 802.1D) deals with the routing of Ethernet packets using a Spanning Tree Protocol. IEEE 802.1Q describes VLANs, and IEEE 802.1X defines a port-based network access control protocol, which forms the basis for the authentication mechanisms used in VLANs[60] (but it is also found in WLANs[61]) – it is what the home user sees when the user has to enter a \"wireless access key\".\n\nEthernet is a family of technologies used in wired LANs. It is described by a set of standards together called IEEE 802.3 published by the Institute of Electrical and Electronics Engineers.\n\nWireless LAN based on the IEEE 802.11 standards, also widely known as WLAN or WiFi, is probably the most well-known member of the IEEE 802 protocol family for home users today. IEEE 802.11 shares many properties with wired Ethernet.\n\nSynchronous optical networking (SONET) and Synchronous Digital Hierarchy (SDH) are standardized multiplexing protocols that transfer multiple digital bit streams over optical fiber using lasers. They were originally designed to transport circuit mode communications from a variety of different sources, primarily to support circuit-switched digital telephony. However, due to its protocol neutrality and transport-oriented features, SONET/SDH also was the obvious choice for transporting Asynchronous Transfer Mode (ATM) frames.\n\nAsynchronous Transfer Mode (ATM) is a switching technique for telecommunication networks. It uses asynchronous time-division multiplexing and encodes data into small, fixed-sized cells. This differs from other protocols such as the Internet protocol suite or Ethernet that use variable-sized packets or frames. ATM has similarities with both circuit and packet switched networking. This makes it a good choice for a network that must handle both traditional high-throughput data traffic, and real-time, low-latency content such as voice and video. ATM uses a connection-oriented model in which a virtual circuit must be established between two endpoints before the actual data exchange begins.\n\nATM still plays a role in the last mile, which is the connection between an Internet service provider and the home user.[62][needs update]\n\nThere are a number of different digital cellular standards, including: Global System for Mobile Communications (GSM), General Packet Radio Service (GPRS), cdmaOne, CDMA2000, Evolution-Data Optimized (EV-DO), Enhanced Data Rates for GSM Evolution (EDGE), Universal Mobile Telecommunications System (UMTS), Digital Enhanced Cordless Telecommunications (DECT), Digital AMPS (IS-136/TDMA), and Integrated Digital Enhanced Network (iDEN).[63]\n\nRouting is the process of selecting network paths to carry network traffic. Routing is performed for many kinds of networks, including circuit switching networks and packet switched networks.\n\nIn packet-switched networks, routing protocols direct packet forwarding through intermediate nodes. Intermediate nodes are typically network hardware devices such as routers, bridges, gateways, firewalls, or switches. General-purpose computers can also forward packets and perform routing, though because they lack specialized hardware, may offer limited performance. The routing process directs forwarding on the basis of routing tables, which maintain a record of the routes to various network destinations. Most routing algorithms use only one network path at a time. Multipath routing techniques enable the use of multiple alternative paths.\n\nRouting can be contrasted with bridging in its assumption that network addresses are structured and that similar addresses imply proximity within the network. Structured addresses allow a single routing table entry to represent the route to a group of devices. In large networks, the structured addressing used by routers outperforms unstructured addressing used by bridging. Structured IP addresses are used on the Internet. Unstructured MAC addresses are used for bridging on Ethernet and similar local area networks.\n\nNetworks may be characterized by many properties or features, such as physical capacity, organizational purpose, user authorization, access rights, and others. Another distinct classification method is that of the physical extent or geographic scale.\n\nA nanoscale network has key components implemented at the nanoscale, including message carriers, and leverages physical principles that differ from macroscale communication mechanisms. Nanoscale communication extends communication to very small sensors and actuators such as those found in biological systems and also tends to operate in environments that would be too harsh for other communication techniques.[64]\n\nA personal area network (PAN) is a computer network used for communication among computers and different information technological devices close to one person. Some examples of devices that are used in a PAN are personal computers, printers, fax machines, telephones, PDAs, scanners, and video game consoles. A PAN may include wired and wireless devices. The reach of a PAN typically extends to 10 meters.[65] A wired PAN is usually constructed with USB and FireWire connections while technologies such as Bluetooth and infrared communication typically form a wireless PAN.\n\nA local area network (LAN) is a network that connects computers and devices in a limited geographical area such as a home, school, office building, or closely positioned group of buildings. Wired LANs are most commonly based on Ethernet technology. Other networking technologies such as ITU-T G.hn also provide a way to create a wired LAN using existing wiring, such as coaxial cables, telephone lines, and power lines.[66]\n\nA LAN can be connected to a wide area network (WAN) using a router. The defining characteristics of a LAN, in contrast to a WAN, include higher data transfer rates, limited geographic range, and lack of reliance on leased lines to provide connectivity.[citation needed] Current Ethernet or other IEEE 802.3 LAN technologies operate at data transfer rates up to and in excess of 100 Gbit/s,[67] standardized by IEEE in 2010.\n\nA home area network (HAN) is a residential LAN used for communication between digital devices typically deployed in the home, usually a small number of personal computers and accessories, such as printers and mobile computing devices. An important function is the sharing of Internet access, often a broadband service through a cable Internet access or digital subscriber line (DSL) provider.\n\nA storage area network (SAN) is a dedicated network that provides access to consolidated, block-level data storage. SANs are primarily used to make storage devices, such as disk arrays, tape libraries, and optical jukeboxes, accessible to servers so that the storage appears as locally attached devices to the operating system. A SAN typically has its own network of storage devices that are generally not accessible through the local area network by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium-sized business environments.[citation needed]\n\nA campus area network (CAN) is made up of an interconnection of LANs within a limited geographical area. The networking equipment (switches, routers) and transmission media (optical fiber, Cat5 cabling, etc.) are almost entirely owned by the campus tenant or owner (an enterprise, university, government, etc.).\n\nFor example, a university campus network is likely to link a variety of campus buildings to connect academic colleges or departments, the library, and student residence halls.\n\nA backbone network is part of a computer network infrastructure that provides a path for the exchange of information between different LANs or subnetworks. A backbone can tie together diverse networks within the same building, across different buildings, or over a wide area. When designing a network backbone, network performance and network congestion are critical factors to take into account. Normally, the backbone network's capacity is greater than that of the individual networks connected to it.\n\nFor example, a large company might implement a backbone network to connect departments that are located around the world. The equipment that ties together the departmental networks constitutes the network backbone. Another example of a backbone network is the Internet backbone, which is a massive, global system of fiber-optic cable and optical networking that carry the bulk of data between wide area networks (WANs), metro, regional, national and transoceanic networks.\n\nA metropolitan area network (MAN) is a large computer network that interconnects users with computer resources in a geographic region of the size of a metropolitan area.\n\nA wide area network (WAN) is a computer network that covers a large geographic area such as a city, country, or spans even intercontinental distances. A WAN uses a communications channel that combines many types of media such as telephone lines, cables, and airwaves. A WAN often makes use of transmission facilities provided by common carriers, such as telephone companies. WAN technologies generally function at the lower three layers of the OSI model: the physical layer, the data link layer, and the network layer.\n\nAn enterprise private network is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources.\n\nA virtual private network (VPN) is an overlay network in which some of the links between nodes are carried by open connections or virtual circuits in some larger network (e.g., the Internet) instead of by physical wires. The data link layer protocols of the virtual network are said to be tunneled through the larger network. One common application is secure communications through the public Internet, but a VPN need not have explicit security features, such as authentication or content encryption. VPNs, for example, can be used to separate the traffic of different user communities over an underlying network with strong security features.\n\nVPN may have best-effort performance or may have a defined service level agreement (SLA) between the VPN customer and the VPN service provider.\n\nA global area network (GAN) is a network used for supporting mobile users across an arbitrary number of wireless LANs, satellite coverage areas, etc. The key challenge in mobile communications is handing off communications from one local coverage area to the next. In IEEE Project 802, this involves a succession of terrestrial wireless LANs.[68]\n\nNetworks are typically managed by the organizations that own them. Private enterprise networks may use a combination of intranets and extranets. They may also provide network access to the Internet, which has no single owner and permits virtually unlimited global connectivity.\n\nAn intranet is a set of networks that are under the control of a single administrative entity. An intranet typically uses the Internet Protocol and IP-based tools such as web browsers and file transfer applications. The administrative entity limits the use of the intranet to its authorized users. Most commonly, an intranet is the internal LAN of an organization. A large intranet typically has at least one web server to provide users with organizational information.\n\nAn extranet is a network that is under the administrative control of a single organization but supports a limited connection to a specific external network. For example, an organization may provide access to some aspects of its intranet to share data with its business partners or customers. These other entities are not necessarily trusted from a security standpoint. The network connection to an extranet is often, but not always, implemented via WAN technology.\n\nAn internetwork is the connection of multiple different types of computer networks to form a single computer network using higher-layer network protocols and connecting them together using routers.\n\nThe Internet is the largest example of internetwork. It is a global system of interconnected governmental, academic, corporate, public, and private computer networks. It is based on the networking technologies of the Internet protocol suite. It is the successor of the Advanced Research Projects Agency Network (ARPANET) developed by DARPA of the United States Department of Defense. The Internet utilizes copper communications and an optical networking backbone to enable the World Wide Web (WWW), the Internet of things, video transfer, and a broad range of information services.\n\nParticipants on the Internet use a diverse array of methods of several hundred documented, and often standardized, protocols compatible with the Internet protocol suite and the IP addressing system administered by the Internet Assigned Numbers Authority and address registries. Service providers and large enterprises exchange information about the reachability of their address spaces through the Border Gateway Protocol (BGP), forming a redundant worldwide mesh of transmission paths.\n\nA darknet is an overlay network, typically running on the Internet, that is only accessible through specialized software. It is an anonymizing network where connections are made only between trusted peers — sometimes called friends (F2F)[70] — using non-standard protocols and ports.\n\nDarknets are distinct from other distributed peer-to-peer networks as sharing is anonymous (that is, IP addresses are not publicly shared), and therefore users can communicate with little fear of governmental or corporate interference.[71]\n\nNetwork services are applications hosted by servers on a computer network, to provide some functionality for members or users of the network, or to help the network itself to operate.\n\nThe World Wide Web, E-mail,[72] printing and network file sharing are examples of well-known network services. Network services such as Domain Name System (DNS) give names for IP and MAC addresses (people remember names like nm.lan better than numbers like 210.121.67.18),[73] and Dynamic Host Configuration Protocol (DHCP) to ensure that the equipment on the network has a valid IP address.[74]\n\nServices are usually based on a service protocol that defines the format and sequencing of messages between clients and servers of that network service.\n\nBandwidth in bit/s may refer to consumed bandwidth, corresponding to achieved throughput or goodput, i.e., the average rate of successful data transfer through a communication path. The throughput is affected by processes such as bandwidth shaping, bandwidth management, bandwidth throttling, bandwidth cap and bandwidth allocation (using, for example, bandwidth allocation protocol and dynamic bandwidth allocation).\n\nNetwork delay is a design and performance characteristic of a telecommunications network. It specifies the latency for a bit of data to travel across the network from one communication endpoint to another. Delay may differ slightly, depending on the location of the specific pair of communicating endpoints. Engineers usually report both the maximum and average delay, and they divide the delay into several components, the sum of which is the total delay:\n\nA certain minimum level of delay is experienced by signals due to the time it takes to transmit a packet serially through a link. This delay is extended by more variable levels of delay due to network congestion. IP network delays can range from less than a microsecond to several hundred milliseconds.\n\nThe parameters that affect performance typically can include throughput, jitter, bit error rate and latency.\n\nIn circuit-switched networks, network performance is synonymous with the grade of service. The number of rejected calls is a measure of how well the network is performing under heavy traffic loads.[75] Other types of performance measures can include the level of noise and echo.\n\nIn an Asynchronous Transfer Mode (ATM) network, performance can be measured by line rate, quality of service (QoS), data throughput, connect time, stability, technology, modulation technique, and modem enhancements.[76][verification needed][full citation needed]\n\nThere are many ways to measure the performance of a network, as each network is different in nature and design. Performance can also be modeled instead of measured. For example, state transition diagrams are often used to model queuing performance in a circuit-switched network. The network planner uses these diagrams to analyze how the network performs in each state, ensuring that the network is optimally designed.[77]\n\nNetwork congestion occurs when a link or node is subjected to a greater data load than it is rated for, resulting in a deterioration of its quality of service. When networks are congested and queues become too full, packets have to be discarded, and participants must rely on retransmission to maintain reliable communications. Typical effects of congestion include queueing delay, packet loss or the blocking of new connections. A consequence of these latter two is that incremental increases in offered load lead either to only a small increase in the network throughput or to a potential reduction in network throughput.\n\nNetwork protocols that use aggressive retransmissions to compensate for packet loss tend to keep systems in a state of network congestion even after the initial load is reduced to a level that would not normally induce network congestion. Thus, networks using these protocols can exhibit two stable states under the same level of load. The stable state with low throughput is known as congestive collapse.\n\nModern networks use congestion control, congestion avoidance and traffic control techniques where endpoints typically slow down or sometimes even stop transmission entirely when the network is congested to try to avoid congestive collapse. Specific techniques include: exponential backoff in protocols such as 802.11's CSMA/CA and the original Ethernet, window reduction in TCP, and fair queueing in devices such as routers.\n\nAnother method to avoid the negative effects of network congestion is implementing quality of service priority schemes allowing selected traffic to bypass congestion. Priority schemes do not solve network congestion by themselves, but they help to alleviate the effects of congestion for critical services. A third method to avoid network congestion is the explicit allocation of network resources to specific flows. One example of this is the use of Contention-Free Transmission Opportunities (CFTXOPs) in the ITU-T G.hn home networking standard.\n\nFor the Internet, RFC 2914 addresses the subject of congestion control in detail.\n\nNetwork resilience is \"the ability to provide and maintain an acceptable level of service in the face of faults and challenges to normal operation.\"[78]\n\nComputer networks are also used by security hackers to deploy computer viruses or computer worms on devices connected to the network, or to prevent these devices from accessing the network via a denial-of-service attack.\n\nNetwork Security consists of provisions and policies adopted by the network administrator to prevent and monitor unauthorized access, misuse, modification, or denial of the computer network and its network-accessible resources.[79] Network security is used on a variety of computer networks, both public and private, to secure daily transactions and communications among businesses, government agencies, and individuals.\n\nNetwork surveillance is the monitoring of data being transferred over computer networks such as the Internet. The monitoring is often done surreptitiously and may be done by or at the behest of governments, by corporations, criminal organizations, or individuals. It may or may not be legal and may or may not require authorization from a court or other independent agency.\n\nComputer and network surveillance programs are widespread today, and almost all Internet traffic is or could potentially be monitored for clues to illegal activity.\n\nSurveillance is very useful to governments and law enforcement to maintain social control, recognize and monitor threats, and prevent or investigate criminal activity. With the advent of programs such as the Total Information Awareness program, technologies such as high-speed surveillance computers and biometrics software, and laws such as the Communications Assistance For Law Enforcement Act, governments now possess an unprecedented ability to monitor the activities of citizens.[80]\n\nHowever, many civil rights and privacy groups—such as Reporters Without Borders, the Electronic Frontier Foundation, and the American Civil Liberties Union—have expressed concern that increasing surveillance of citizens may lead to a mass surveillance society, with limited political and personal freedoms. Fears such as this have led to lawsuits such as Hepting v. AT&T.[80][81] The hacktivist group Anonymous has hacked into government websites in protest of what it considers \"draconian surveillance\".[82][83]\n\nEnd-to-end encryption (E2EE) is a digital communications paradigm of uninterrupted protection of data traveling between two communicating parties. It involves the originating party encrypting data so only the intended recipient can decrypt it, with no dependency on third parties. End-to-end encryption prevents intermediaries, such as Internet service providers or application service providers, from reading or tampering with communications. End-to-end encryption generally protects both confidentiality and integrity.\n\nExamples of end-to-end encryption include HTTPS for web traffic, PGP for email, OTR for instant messaging, ZRTP for telephony, and TETRA for radio.\n\nTypical server-based communications systems do not include end-to-end encryption. These systems can only guarantee the protection of communications between clients and servers, not between the communicating parties themselves. Examples of non-E2EE systems are Google Talk, Yahoo Messenger, Facebook, and Dropbox.\n\nThe end-to-end encryption paradigm does not directly address risks at the endpoints of the communication themselves, such as the technical exploitation of clients, poor quality random number generators, or key escrow. E2EE also does not address traffic analysis, which relates to things such as the identities of the endpoints and the times and quantities of messages that are sent.\n\nThe introduction and rapid growth of e-commerce on the World Wide Web in the mid-1990s made it obvious that some form of authentication and encryption was needed. Netscape took the first shot at a new standard. At the time, the dominant web browser was Netscape Navigator. Netscape created a standard called secure socket layer (SSL). SSL requires a server with a certificate. When a client requests access to an SSL-secured server, the server sends a copy of the certificate to the client. The SSL client checks this certificate (all web browsers come with an exhaustive list of root certificates preloaded), and if the certificate checks out, the server is authenticated and the client negotiates a symmetric-key cipher for use in the session. The session is now in a very secure encrypted tunnel between the SSL server and the SSL client.[54]\n\nUsers and network administrators typically have different views of their networks. Users can share printers and some servers from a workgroup, which usually means they are in the same geographic location and are on the same LAN, whereas a network administrator is responsible for keeping that network up and running. A community of interest has less of a connection of being in a local area and should be thought of as a set of arbitrarily located users who share a set of servers, and possibly also communicate via peer-to-peer technologies.\n\nNetwork administrators can see networks from both physical and logical perspectives. The physical perspective involves geographic locations, physical cabling, and the network elements (e.g., routers, bridges and application-layer gateways) that interconnect via the transmission media. Logical networks, called, in the TCP/IP architecture, subnets, map onto one or more transmission media. For example, a common practice in a campus of buildings is to make a set of LAN cables in each building appear to be a common subnet, using VLANs.\n\nUsers and administrators are aware, to varying extents, of a network's trust and scope characteristics. Again using TCP/IP architectural terminology, an intranet is a community of interest under private administration usually by an enterprise, and is only accessible by authorized users (e.g. employees).[84] Intranets do not have to be connected to the Internet, but generally have a limited connection. An extranet is an extension of an intranet that allows secure communications to users outside of the intranet (e.g. business partners, customers).[84]\n\nUnofficially, the Internet is the set of users, enterprises, and content providers that are interconnected by Internet Service Providers (ISP). From an engineering viewpoint, the Internet is the set of subnets, and aggregates of subnets, that share the registered IP address space and exchange information about the reachability of those IP addresses using the Border Gateway Protocol. Typically, the human-readable names of servers are translated to IP addresses, transparently to users, via the directory function of the Domain Name System (DNS).\n\nOver the Internet, there can be business-to-business, business-to-consumer and consumer-to-consumer communications. When money or sensitive information is exchanged, the communications are apt to be protected by some form of communications security mechanism. Intranets and extranets can be securely superimposed onto the Internet, without any access by general Internet users and administrators, using secure VPN technology.\n\n This article incorporates public domain material from Federal Standard 1037C. General Services Administration. Archived from the original on 2022-01-22.\n"
    },
    {
        "title": "French Revolution",
        "url": "https://en.wikipedia.org/wiki/French_Revolution",
        "content": "\n\nThe French Revolution (French: Révolution française [ʁevɔlysjɔ̃ fʁɑ̃sɛːz]) was a period of political and societal change in France that began with the Estates General of 1789, and ended with the coup of 18 Brumaire in November 1799 and the formation of the French Consulate. Many of its ideas are considered fundamental principles of liberal democracy,[1] while its values and institutions remain central to modern French political discourse.[2]\n\nThe causes of the revolution were a combination of social, political, and economic factors which the ancien régime (\"old regime\") proved unable to manage. A financial crisis and widespread social distress led to the convocation of the Estates General in May 1789, its first meeting since 1614. The representatives of the Third Estate broke away, and re-constituted themselves as a National Assembly in June. The Storming of the Bastille in Paris on 14 July was followed by a series of radical measures by the Assembly, among them the abolition of feudalism, state control over the Catholic Church, and a declaration of rights. The next three years were dominated by the struggle for political control, and military defeats following the outbreak of the French Revolutionary Wars in April 1792 led to an insurrection on 10 August. The monarchy was replaced by the French First Republic in September, and Louis XVI was executed in January 1793. \n\nAfter another revolt in June 1793, the constitution was suspended, and adequate political power passed from the National Convention to the Committee of Public Safety, led by the Jacobins. About 16,000 people were executed in what was later referred to as Reign of Terror, which ended in July 1794. Weakened by external threats and internal opposition, the Republic was replaced in 1795 by the Directory, and four years later, in 1799, the Consulate seized power in a military coup led by Napoleon Bonaparte on 9 November. This event is generally seen as marking the end of the Revolutionary period.\n\nThe Revolution resulted from multiple long-term and short-term factors, culminating in a social, economic, financial and political crisis in the late 1780s.[3][4][5] Combined with resistance to reform by the ruling elite, and indecisive policy by Louis XVI and his ministers, the result was a crisis the state was unable to manage.[6][7]\n\nBetween 1715 and 1789, the French population grew from 21 to 28 million, 20% of whom lived in towns or cities, Paris alone having over 600,000 inhabitants.[8] This was accompanied by a tripling in the size of the middle class, which comprised almost 10% of the population by 1789.[9] Despite increases in overall prosperity, its benefits were largely restricted to the rentier and mercantile classes, while the living standards fell for wage labourers and peasant farmers who rented their land.[10][11] Economic recession from 1785, combined with bad harvests in 1787 and 1788, led to high unemployment and food prices, causing a financial and political crisis.[3][12][13][14]\n\nWhile the state also experienced a debt crisis, the level of debt itself was not high compared with Britain's.[15] A significant problem was that tax rates varied widely from one region to another, were often different from the official amounts, and collected inconsistently. Its complexity meant uncertainty over the amount contributed by any authorised tax caused resentment among all taxpayers.[16][a] Attempts to simplify the system were blocked by the regional Parlements which approved financial policy. The resulting impasse led to the calling of the Estates General of 1789, which became radicalised by the struggle for control of public finances.[18]\n\nLouis XVI was willing to consider reforms, but often backed down when faced with opposition from conservative elements within the nobility. Enlightenment critiques of social institutions were widely discussed among the educated French elite. At the same time, the American Revolution and the European revolts of the 1780s inspired public debate on issues such as patriotism, liberty, equality, and democracy. These shaped the response of the educated public to the crisis,[19] while scandals such as the Affair of the Diamond Necklace fuelled widespread anger at the court, nobility, and church officials.[20]\n\nFrance faced a series of budgetary crises during the 18th century, as revenues failed to keep pace with expenditure.[21][22] Although the economy grew solidly, the increase was not reflected in a proportional growth in taxes,[21] their collection being contracted to tax farmers who kept much of it as personal profit. As the nobility and Church benefited from many exemptions, the tax burden fell mainly on peasants.[23] Reform was difficult because new tax laws had to be registered with regional judicial bodies or parlements that were able to block them. The king could impose laws by decree, but this risked open conflict with the parlements, the nobility, and those subject to new taxes.[24]\n\nFrance primarily funded the Anglo-French War of 1778–1783 through loans. Following the peace, the monarchy borrowed heavily, culminating in a debt crisis. By 1788, half of state revenue was required to service its debt.[25] In 1786, the French finance minister, Calonne, proposed a package of reforms including a universal land tax, the abolition of grain controls and internal tariffs, and new provincial assemblies appointed by the king. The new taxes, however, were rejected, first by a hand-picked Assembly of Notables dominated by the nobility, then by the parlements when submitted by Calonne's successor Brienne. The notables and parlements argued that the proposed taxes could only be approved by an Estates-General, a representative body that had last met in 1614.[26]\n\nThe conflict between the Crown and the parlements became a national political crisis. Both sides issued a series of public statements, the government arguing that it was combating privilege and the parlement defending the ancient rights of the nation. Public opinion was firmly on the side of the parlements, and riots broke out in several towns. Brienne's attempts to raise new loans failed, and on 8 August 1788, he announced that the king would summon an Estates-General to convene the following May. Brienne resigned and was replaced by Necker.[27]\n\nIn September 1788, the Parlement of Paris ruled that the Estates-General should convene in the same form as in 1614, meaning that the three estates (the clergy, nobility, and Third Estate or \"commons\") would meet and vote separately, with votes counted by estate rather than by head. As a result, the clergy and nobility could combine to outvote the Third Estate despite representing less than 5% of the population.[28][29]\n\nFollowing the relaxation of censorship and laws against political clubs, a group of liberal nobles and middle class activists, known as the Society of Thirty, launched a campaign for the doubling of Third Estate representation and individual voting. The public debate saw an average of 25 new political pamphlets published a week from 25 September 1788.[30] The Abbé Sieyès issued influential pamphlets denouncing the privilege of the clergy and nobility, and arguing the Third Estate represented the nation and should sit alone as a National Assembly. Activists such as Mounier, Barnave and Robespierre organised regional meetings, petitions and literature in support of these demands.[31] In December, the king agreed to double the representation of the Third Estate, but left the question of counting votes for the Estates-General to decide.[32]\n\nThe Estates-General contained three separate bodies, the First Estate representing 100,000 clergy, the Second the nobility, and the Third the \"commons\".[33] Since each met separately, and any proposals had to be approved by at least two, the First and Second Estates could outvote the Third despite representing less than 5% of the population.[28]\n\nAlthough the Catholic Church in France owned nearly 10% of all land, as well as receiving annual tithes paid by peasants,[34] three-quarters of the 303 clergy elected were parish priests, many of whom earned less than unskilled labourers and had more in common with their poor parishioners than with the bishops of the first estate.[35][36]\n\nThe Second Estate elected 322 deputies, representing about 400,000 men and women, who owned about 25% of the land and collected seigneurial dues and rents from their tenants. Most delegates were town-dwelling members of the noblesse d'épée, or traditional aristocracy. Courtiers and representatives of the noblesse de robe (those who derived rank from judicial or administrative posts) were underrepresented.[37]\n\nOf the 610 deputies of the Third Estate, about two-thirds held legal qualifications and almost half were venal office holders. Less than 100 were in trade or industry and none were peasants or artisans.[38] To assist delegates, each region completed a list of grievances, known as Cahiers de doléances.[39] Tax inequality and seigneurial dues (feudal payments owed to landowners) headed the grievances in the cahiers de doleances for the estate.[40]\n\nOn 5 May 1789, the Estates-General convened at Versailles. Necker outlined the state budget and reiterated the king's decision that each estate should decide on which matters it would agree to meet and vote in common with the other estates. On the following day, each estate was to separately verify the credentials of their representatives. The Third Estate, however, voted to invite the other estates to join them in verifying all the representatives of the Estates-General in common and to agree that votes should be counted by head. Fruitless negotiations lasted to 12 June when the Third Estate began verifying its own members. On 17 June, the Third Estate declared itself to be the National Assembly of France and that all existing taxes were illegal.[41] Within two days, more than 100 members of the clergy had joined them.[42]\n\nShaken by this challenge to his authority, the king agreed to a reform package that he would announce at a Royal Session of the Estates-General. The Salle des États was closed to prepare for the joint session, but the members of the Estates-General were not informed in advance. On 20 June, when the members of the Third Estate found their meeting place closed, they moved to a nearby tennis court and swore not to disperse until a new constitution had been agreed.[43]\n\nAt the Royal Session the king announced a series of tax and other reforms and stated that no new taxes or loans would be implemented without the consent of the Estates-General. However, he stated that the three estates were sacrosanct and it was up to each estate to agree to end their privileges and decide on which matters they would vote in common with the other estates. At the end of the session the Third Estate refused to leave the hall and reiterated their oath not to disperse until a constitution had been agreed. Over the next days more members of the clergy joined the National Assembly. On 27 June, faced with popular demonstrations and mutinies in his French Guards, Louis XVI capitulated. He commanded the members of the first and second estates to join the third in the National Assembly.[44]\n\nEven the limited reforms the king had announced went too far for Marie Antoinette and Louis' younger brother the Comte d'Artois. On their advice, Louis dismissed Necker again as chief minister on 11 July.[45] On 12 July, the Assembly went into a non-stop session after rumours circulated he was planning to use the Swiss Guards to force it to close. The news brought crowds of protestors into the streets, and soldiers of the elite Gardes Françaises regiment refused to disperse them.[46]\n\nOn the 14th, many of these soldiers joined the mob in attacking the Bastille, a royal fortress with large stores of arms and ammunition. Its governor, Bernard-René de Launay, surrendered after several hours of fighting that cost the lives of 83 attackers. Taken to the Hôtel de Ville, he was executed, his head placed on a pike and paraded around the city; the fortress was then torn down in a remarkably short time. Although rumoured to hold many prisoners, the Bastille held only seven: four forgers, a lunatic, a failed assassin, and a deviant nobleman. Nevertheless, as a potent symbol of the Ancien Régime, its destruction was viewed as a triumph and Bastille Day is still celebrated every year.[47] In French culture, some see its fall as the start of the Revolution.[48]\n\nAlarmed by the prospect of losing control of the capital, Louis appointed the Marquis de Lafayette commander of the National Guard, with Jean-Sylvain Bailly as head of a new administrative structure known as the Commune. On 17 July, Louis visited Paris accompanied by 100 deputies, where he was greeted by Bailly and accepted a tricolore cockade to loud cheers. However, it was clear power had shifted from his court; he was welcomed as 'Louis XVI, father of the French and king of a free people.'[49]\n\nThe short-lived unity enforced on the Assembly by a common threat quickly dissipated. Deputies argued over constitutional forms, while civil authority rapidly deteriorated. On 22 July, former Finance Minister Joseph Foullon and his son were lynched by a Parisian mob, and neither Bailly nor Lafayette could prevent it. In rural areas, wild rumours and paranoia resulted in the formation of militia and an agrarian insurrection known as la Grande Peur.[50] The breakdown of law and order and frequent attacks on aristocratic property led much of the nobility to flee abroad. These émigrés funded reactionary forces within France and urged foreign monarchs to back a counter-revolution.[51]\n\nIn response, the Assembly published the August Decrees which abolished feudalism. Over 25% of French farmland was subject to feudal dues, providing the nobility with most of their income; these were now cancelled, along with church tithes. While their former tenants were supposed to pay them compensation, collecting it proved impossible, and the obligation was annulled in 1793.[52] Other decrees included equality before the law, opening public office to all, freedom of worship, and cancellation of special privileges held by provinces and towns.[53]\n\nWith the suspension of the 13 regional parlements in November, the key institutional pillars of the old regime had all been abolished in less than four months. From its early stages, the Revolution therefore displayed signs of its radical nature; what remained unclear was the constitutional mechanism for turning intentions into practical applications.[54]\n\nOn 9 July, the National Assembly appointed a committee to draft a constitution and statement of rights.[55] Twenty drafts were submitted, which were used by a sub-committee to create a Declaration of the Rights of Man and of the Citizen, with Mirabeau being the most prominent member.[56] The Declaration was approved by the Assembly and published on 26 August as a statement of principle.[57]\n\nThe Assembly now concentrated on the constitution itself. Mounier and his monarchist supporters advocated a bicameral system, with an upper house appointed by the king, who would also have the right to appoint ministers and veto legislation. On 10 September, the majority of the Assembly, led by Sieyès and Talleyrand, voted in favour of a single body, and the following day approved a \"suspensive veto\" for the king, meaning Louis could delay implementation of a law, but not block it indefinitely. In October, the Assembly voted to restrict political rights, including voting rights, to \"active citizens\", defined as French males over the age of 25 who paid direct taxes equal to three days' labour. The remainder were designated \"passive citizens\", restricted to \"civil rights\", a distinction opposed by a significant minority, including the Jacobin clubs.[58][59] By mid-1790, the main elements of a constitutional monarchy were in place, although the constitution was not accepted by Louis until 1791.[60]\n\nFood shortages and the worsening economy caused frustration at the lack of progress, and led to popular unrest in Paris. This came to a head in late September 1789, when the Flanders Regiment arrived in Versailles to reinforce the royal bodyguard, and were welcomed with a formal banquet as was common practice. The radical press described this as a 'gluttonous orgy', and claimed the tricolour cockade had been abused, while the Assembly viewed their arrival as an attempt to intimidate them.[61]\n\nOn 5 October, crowds of women assembled outside the Hôtel de Ville, agitating against high food prices and shortages.[62] These protests quickly turned political, and after seizing weapons stored at the Hôtel de Ville, some 7,000 of them marched on Versailles, where they entered the Assembly to present their demands. They were followed to Versailles by 15,000 members of the National Guard under Lafayette, who was virtually \"a prisoner of his own troops\".[63]\n\nWhen the National Guard arrived later that evening, Lafayette persuaded Louis the safety of his family required their relocation to Paris. Next morning, some of the protestors broke into the royal apartments, searching for Marie Antoinette, who escaped. They ransacked the palace, killing several guards. Order was eventually restored, and the royal family and Assembly left for Paris, escorted by the National Guard.[64] Louis had announced his acceptance of the August Decrees and the Declaration, and his official title changed from 'King of France' to 'King of the French'.[65]\n\nHistorian John McManners argues \"in eighteenth-century France, throne and altar were commonly spoken of as in close alliance; their simultaneous collapse ... would one day provide the final proof of their interdependence.\" One suggestion is that after a century of persecution, some French Protestants actively supported an anti-Catholic regime, a resentment fuelled by Enlightenment thinkers such as Voltaire.[66] Jean-Jacques Rousseau, considered a philosophical founder of the revolution,[67][68][69] wrote it was \"manifestly contrary to the law of nature... that a handful of people should gorge themselves with superfluities, while the hungry multitude goes in want of necessities.\"[70]\n\nThe Revolution caused a massive shift of power from the Catholic Church to the state; although the extent of religious belief has been questioned, elimination of tolerance for religious minorities meant by 1789 being French also meant being Catholic.[71] The church was the largest individual landowner in France, controlling nearly 10% of all estates and levied tithes, effectively a 10% tax on income, collected from peasant farmers in the form of crops. In return, it provided a minimal level of social support.[72]\n\nThe August decrees abolished tithes, and on 2 November the Assembly confiscated all church property, the value of which was used to back a new paper currency known as assignats. In return, the state assumed responsibilities such as paying the clergy and caring for the poor, the sick and the orphaned.[73] On 13 February 1790, religious orders and monasteries were dissolved, while monks and nuns were encouraged to return to private life.[74]\n\nThe Civil Constitution of the Clergy of 12 July 1790 made them employees of the state, as well as establishing rates of pay and a system for electing priests and bishops. Pope Pius VI and many French Catholics objected to this since it denied the authority of the Pope over the French Church. In October, thirty bishops wrote a declaration denouncing the law, further fuelling opposition.[75]\n\nWhen clergy were required to swear loyalty to the Civil Constitution in November 1790, it split the church between the 24% who complied, and the majority who refused.[76] This stiffened popular resistance against state interference, especially in traditionally Catholic areas such as Normandy, Brittany and the Vendée, where only a few priests took the oath and the civilian population turned against the revolution.[75] The result was state-led persecution of \"Refractory clergy\", many of whom were forced into exile, deported, or executed.[77]\n\nThe period from October 1789 to spring 1791 is usually seen as one of relative tranquility, when some of the most important legislative reforms were enacted. However, conflict over the source of legitimate authority was more apparent in the provinces, where officers of the Ancien Régime had been swept away, but not yet replaced by new structures. This was less obvious in Paris, since the National Guard made it the best policed city in Europe, but disorder in the provinces inevitably affected members of the Assembly.[78]\n\nCentrists led by Sieyès, Lafayette, Mirabeau and Bailly created a majority by forging consensus with monarchiens like Mounier, and independents including Adrien Duport, Barnave and Alexandre Lameth. At one end of the political spectrum, reactionaries like Cazalès and Maury denounced the Revolution in all its forms, with radicals like Maximilien Robespierre at the other. He and Jean-Paul Marat opposed the criteria for \"active citizens\", gaining them substantial support among the Parisian proletariat, many of whom had been disenfranchised by the measure.[79]\n\nOn 14 July 1790, celebrations were held throughout France commemorating the fall of the Bastille, with participants swearing an oath of fidelity to \"the nation, the law and the king.\" The Fête de la Fédération in Paris was attended by the royal family, with Talleyrand performing a mass. Despite this show of unity, the Assembly was increasingly divided, while external players like the Paris Commune and National Guard competed for power. One of the most significant was the Jacobin club; originally a forum for general debate, by August 1790 it had over 150 members, split into different factions.[80]\n\nThe Assembly continued to develop new institutions; in September 1790, the regional Parlements were abolished and their legal functions replaced by a new independent judiciary, with jury trials for criminal cases. However, moderate deputies were uneasy at popular demands for universal suffrage, labour unions and cheap bread, and over the winter of 1790 and 1791, they passed a series of measures intended to disarm popular radicalism. These included exclusion of poorer citizens from the National Guard, limits on use of petitions and posters, and the June 1791 Le Chapelier Law suppressing trade guilds and any form of worker organisation.[81]\n\nThe traditional force for preserving law and order was the army, which was increasingly divided between officers, who largely came from the nobility, and ordinary soldiers. In August 1790, the loyalist General Bouillé suppressed a serious mutiny at Nancy; although congratulated by the Assembly, he was criticised by Jacobin radicals for the severity of his actions. Growing disorder meant many professional officers either left or became émigrés, further destabilising the institution.[82]\n\nHeld in the Tuileries Palace under virtual house arrest, Louis XVI was urged by his brother and wife to re-assert his independence by taking refuge with Bouillé, who was based at Montmédy with 10,000 soldiers considered loyal to the Crown.[83] The royal family left the palace in disguise on the night of 20 June 1791; late the next day, Louis was recognised as he passed through Varennes, arrested and taken back to Paris. The attempted escape had a profound impact on public opinion; since it was clear Louis had been seeking refuge in Austria, the Assembly now demanded oaths of loyalty to the regime, and began preparing for war, while fear of 'spies and traitors' became pervasive.[84]\n\nDespite calls to replace the monarchy with a republic, Louis retained his position but was generally regarded with acute suspicion and forced to swear allegiance to the constitution. A new decree stated retracting this oath, making war upon the nation, or permitting anyone to do so in his name would be considered abdication. However, radicals led by Jacques Pierre Brissot prepared a petition demanding his deposition, and on 17 July, an immense crowd gathered in the Champ de Mars to sign. Led by Lafayette, the National Guard was ordered to \"preserve public order\" and responded to a barrage of stones by firing into the crowd, killing between 13 and 50 people.[85]\n\nThe massacre badly damaged Lafayette's reputation: the authorities responded by closing radical clubs and newspapers, while their leaders went into exile or hiding, including Marat.[86] On 27 August, Emperor Leopold II and King Frederick William II of Prussia issued the Declaration of Pillnitz declaring their support for Louis and hinting at an invasion of France on his behalf. In reality, the meeting between Leopold and Frederick was primarily to discuss the Partitions of Poland; the Declaration was intended to satisfy Comte d'Artois and other French émigrés but the threat rallied popular support behind the regime.[87]\n\nBased on a motion proposed by Robespierre, existing deputies were barred from elections held in early September for the French Legislative Assembly. Although Robespierre himself was one of those excluded, his support in the clubs gave him a political power base not available to Lafayette and Bailly, who resigned respectively as head of the National Guard and the Paris Commune. The new laws were gathered together in the 1791 Constitution, and submitted to Louis XVI, who pledged to defend it \"from enemies at home and abroad\". On 30 September, the Constituent Assembly was dissolved, and the Legislative Assembly convened the next day.[88]\n\nThe Legislative Assembly is often dismissed by historians as an ineffective body, compromised by divisions over the role of the monarchy, an issue exacerbated when Louis attempted to prevent or reverse limitations on his powers.[89] At the same time, restricting the vote to those who paid a minimal amount of tax disenfranchised a significant proportion of the 6 million Frenchmen over 25, while only 10% of those able to vote actually did so. Finally, poor harvests and rising food prices led to unrest among the urban class known as Sans-culottes, who saw the new regime as failing to meet their demands for bread and work.[90]\n\nThis meant the new constitution was opposed by significant elements inside and outside the Assembly, itself split into three main groups. 264 members were affiliated with Barnave's Feuillants, constitutional monarchists who considered the Revolution had gone far enough, while another 136 were Jacobin leftists who supported a republic, led by Brissot and usually referred to as Brissotins.[91] The remaining 345 belonged to La Plaine, a centrist faction who switched votes depending on the issue, but many of whom shared doubts as to whether Louis was committed to the Revolution.[91] After he officially accepted the new Constitution, one recorded response was \"Vive le roi, s'il est de bon foi!\", or \"Long live the king – if he keeps his word\".[92]\n\nAlthough a minority in the Assembly, control of key committees allowed the Brissotins to provoke Louis into using his veto. They first managed to pass decrees confiscating émigré property and threatening them with the death penalty.[93] This was followed by measures against non-juring priests, whose opposition to the Civil Constitution led to a state of near civil war in southern France, which Barnave tried to defuse by relaxing the more punitive provisions. On 29 November, the Assembly approved a decree giving refractory clergy eight days to comply, or face charges of 'conspiracy against the nation', an act opposed even by Robespierre.[94] When Louis vetoed both, his opponents were able to portray him as opposed to reform in general.[95]\n\nBrissot accompanied this with a campaign for war against Austria and Prussia, often interpreted as a mixture of calculation and idealism. While exploiting popular anti-Austrianism, it reflected a genuine belief in exporting the values of political liberty and popular sovereignty.[96] Simultaneously, conservatives headed by Marie Antoinette also favoured war, seeing it as a way to regain control of the military, and restore royal authority. In December 1791, Louis made a speech in the Assembly giving foreign powers a month to disband the émigrés or face war, an act greeted with enthusiasm by supporters, but suspicion from opponents.[97]\n\nBarnave's inability to build a consensus in the Assembly resulted in the appointment of a new government, chiefly composed of Brissotins. On 20 April 1792, the French Revolutionary Wars began when French armies attacked Austrian and Prussian forces along their borders, before suffering a series of disastrous defeats. In an effort to mobilise popular support, the government ordered non-juring priests to swear the oath or be deported, dissolved the Constitutional Guard and replaced it with 20,000 fédérés; Louis agreed to disband the Guard, but vetoed the other two proposals, while Lafayette called on the Assembly to suppress the clubs.[98]\n\nPopular anger increased when details of the Brunswick Manifesto reached Paris on 1 August, threatening 'unforgettable vengeance' should any oppose the Allies in seeking to restore the power of the monarchy. On the morning of 10 August, a combined force of the Paris National Guard and provincial fédérés attacked the Tuileries Palace, killing many of the Swiss Guards protecting it.[99] Louis and his family took refuge with the Assembly and shortly after 11:00 am, the deputies present voted to 'temporarily relieve the king', effectively suspending the monarchy.[100]\n\nIn late August, elections were held for the National Convention. New restrictions on the franchise meant the number of votes cast fell to 3.3 million, versus 4 million in 1791, while intimidation was widespread.[101] The Brissotins now split between moderate Girondins led by Brissot, and radical Montagnards, headed by Robespierre, Georges Danton and Jean-Paul Marat. While loyalties constantly shifted, voting patterns suggest roughly 160 of the 749 deputies can generally be categorised as Girondists, with another 200 Montagnards. The remainder were part of a centrist faction known as La Plaine, headed by Bertrand Barère, Pierre Joseph Cambon and Lazare Carnot.[102]\n\nIn the September Massacres, between 1,100 and 1,600 prisoners held in Parisian jails were summarily executed, the vast majority being common criminals.[103] A response to the capture of Longwy and Verdun by Prussia, the perpetrators were largely National Guard members and fédérés on their way to the front. While responsibility is still disputed, even moderates expressed sympathy for the action, which soon spread to the provinces. One suggestion is that the killings stemmed from concern over growing lawlessness, rather than political ideology.[104]\n\nOn 20 September, the French defeated the Prussians at the Battle of Valmy, in what was the first major victory by the army of France during the Revolutionary Wars. Emboldened by this, on 22 September the Convention replaced the monarchy with the French First Republic (1792–1804) and introduced a new calendar, with 1792 becoming \"Year One\".[105] The next few months were taken up with the trial of Citoyen Louis Capet, formerly Louis XVI. While evenly divided on the question of his guilt, members of the convention were increasingly influenced by radicals based within the Jacobin clubs and Paris Commune. The Brunswick Manifesto made it easy to portray Louis as a threat to the Revolution, especially when extracts from his personal correspondence showed him conspiring with Royalist exiles.[106]\n\nOn 17 January 1793, Louis was sentenced to death for \"conspiracy against public liberty and general safety\". 361 deputies were in favour, 288 against, while another 72 voted to execute him, subject to delaying conditions. The sentence was carried out on 21 January on the Place de la Révolution, now the Place de la Concorde.[107] Conservatives across Europe now called for the destruction of revolutionary France, and in February the Convention responded by declaring war on Britain and the Dutch Republic. Together with Austria and Prussia, these two countries were later joined by Spain, Portugal, Naples, and Tuscany in the War of the First Coalition (1792–1797).[108]\n\nThe Girondins hoped war would unite the people behind the government and provide an excuse for rising prices and food shortages, but found themselves the target of popular anger. Many left for the provinces. The first conscription measure or levée en masse on 24 February sparked riots in Paris and other regional centres. Already unsettled by changes imposed on the church, in March the traditionally conservative and royalist Vendée rose in revolt. On 18th, Dumouriez was defeated at Neerwinden and defected to the Austrians. Uprisings followed in Bordeaux, Lyon, Toulon, Marseille and Caen. The Republic seemed on the verge of collapse.[109]\n\nThe crisis led to the creation on 6 April 1793 of the Committee of Public Safety, an executive committee accountable to the convention.[110] The Girondins made a fatal political error by indicting Marat before the Revolutionary Tribunal for allegedly directing the September massacres; he was quickly acquitted, further isolating the Girondins from the sans-culottes. When Jacques Hébert called for a popular revolt against the \"henchmen of Louis Capet\" on 24 May, he was arrested by the Commission of Twelve, a Girondin-dominated tribunal set up to expose 'plots'. In response to protests by the Commune, the Commission warned \"if by your incessant rebellions something befalls the representatives of the nation, Paris will be obliterated\".[109]\n\nGrowing discontent allowed the clubs to mobilise against the Girondins. Backed by the Commune and elements of the National Guard, on 31 May they attempted to seize power in a coup. Although the coup failed, on 2 June the convention was surrounded by a crowd of up to 80,000, demanding cheap bread, unemployment pay and political reforms, including restriction of the vote to the sans-culottes, and the right to remove deputies at will.[111] Ten members of the commission and another twenty-nine members of the Girondin faction were arrested, and on 10 June, the Montagnards took over the Committee of Public Safety.[112]\n\nMeanwhile, a committee led by Robespierre's close ally Saint-Just was tasked with preparing a new Constitution. Completed in only eight days, it was ratified by the convention on 24 June, and contained radical reforms, including universal male suffrage. However, normal legal processes were suspended following the assassination of Marat on 13 July by the Girondist Charlotte Corday, which the Committee of Public Safety used as an excuse to take control. The 1793 Constitution was suspended indefinitely in October.[113]\n\nKey areas of focus for the new government included creating a new state ideology, economic regulation and winning the war.[114] They were helped by divisions among their internal opponents; while areas like the Vendée and Brittany wanted to restore the monarchy, most supported the Republic but opposed the regime in Paris. On 17 August, the Convention voted a second levée en masse; despite initial problems in equipping and supplying such large numbers, by mid-October Republican forces had re-taken Lyon, Marseille and Bordeaux, while defeating Coalition armies at Hondschoote and Wattignies.[115] The new class of military leaders included a young colonel named Napoleon Bonaparte, who was appointed commander of artillery at the siege of Toulon thanks to his friendship with Augustin Robespierre. His success in that role resulted in promotion to the Army of Italy in April 1794, and the beginning of his rise to military and political power.[116]\n\nAlthough intended to bolster revolutionary fervour, the Reign of Terror rapidly degenerated into the settlement of personal grievances. At the end of July, the Convention set price controls on a wide range of goods, with the death penalty for hoarders. On 9 September, 'revolutionary groups' were established to enforce these controls, while the Law of Suspects on 17th approved the arrest of suspected \"enemies of freedom\". This initiated what has become known as the \"Terror\". From September 1793 to July 1794, around 300,000 were arrested,[117] with some 16,600 people executed on charges of counter-revolutionary activity, while another 40,000 may have been summarily executed, or died awaiting trial.[118]\n\nPrice controls made farmers reluctant to sell their produce in Parisian markets, and by early September, the city was suffering acute food shortages. At the same time, the war increased public debt, which the Assembly tried to finance by selling confiscated property. However, few would buy assets that might be repossessed by their former owners, a concern that could only be achieved by military victory. This meant the financial position worsened as threats to the Republic increased, while printing assignats to deal with the deficit further increased inflation.[119]\n\nOn 10 October, the Convention recognised the Committee of Public Safety as the supreme Revolutionary Government, and suspended the Constitution until peace was achieved.[113] In mid-October, Marie Antoinette was convicted of a long list of crimes, and guillotined; two weeks later, the Girondist leaders arrested in June were also executed, along with Philippe Égalité. The \"Terror\" was not confined to Paris, with over 2,000 killed in Lyons after its recapture.[120]\n\nAt Cholet on 17 October, the Republican army won a decisive victory over the Vendée rebels, and the survivors escaped into Brittany. Another defeat at Le Mans on 23 December ended the rebellion as a major threat, although the insurgency continued until 1796. The extent of the repression that followed has been debated by French historians since the mid-19th century.[121] Between November 1793 to February 1794, over 4,000 were drowned in the Loire at Nantes under the supervision of Jean-Baptiste Carrier. Historian Reynald Secher claims that as many as 117,000 died between 1793 and 1796. Although those numbers have been challenged, François Furet concluded it \"not only revealed massacre and destruction on an unprecedented scale, but a zeal so violent that it has bestowed as its legacy much of the region's identity.\"[122] [b]\n\nAt the height of the Terror, not even its supporters were immune from suspicion, leading to divisions within the Montagnard faction between radical Hébertists and moderates led by Danton.[c] Robespierre saw their dispute as de-stabilising the regime, and, as a deist, objected to the anti-religious policies advocated by the atheist Hébert, who was arrested and executed on 24 March with 19 of his colleagues, including Carrier.[126] To retain the loyalty of the remaining Hébertists, Danton was arrested and executed on 5 April with Camille Desmoulins, after a show trial that arguably did more damage to Robespierre than any other act in this period.[127]\n\nThe Law of 22 Prairial (10 June) denied \"enemies of the people\" the right to defend themselves. Those arrested in the provinces were now sent to Paris for judgement; from March to July, executions in Paris increased from five to twenty-six a day.[128] Many Jacobins ridiculed the festival of the Cult of the Supreme Being on 8 June, a lavish and expensive ceremony led by Robespierre, who was also accused of circulating claims he was a second Messiah. Relaxation of price controls and rampant inflation caused increasing unrest among the sans-culottes, but the improved military situation reduced fears the Republic was in danger. Fearing their own survival depended on Robespierre's removal, on 29 June three members of the Committee of Public Safety openly accused him of being a dictator.[129]\n\nRobespierre responded by refusing to attend Committee meetings, allowing his opponents to build a coalition against him. In a speech made to the convention on 26 July, he claimed certain members were conspiring against the Republic, an almost certain death sentence if confirmed. When he refused to provide names, the session broke up in confusion. That evening he repeated these claims at the Jacobins club, where it was greeted with demands for execution of the 'traitors'. Fearing the consequences if they did not act first, his opponents attacked Robespierre and his allies in the Convention next day. When Robespierre attempted to speak, his voice failed, one deputy crying \"The blood of Danton chokes him!\"[130]\n\nAfter the Convention authorised his arrest, he and his supporters took refuge in the Hotel de Ville, which was defended by elements of the National Guard. Other units loyal to the Convention stormed the building that evening and detained Robespierre, who severely injured himself attempting suicide. He was executed on 28 July with 19 colleagues, including Saint-Just and Georges Couthon, followed by 83 members of the Commune.[131] The Law of 22 Prairial was repealed, any surviving Girondists reinstated as deputies, and the Jacobin Club was closed and banned.[132]\n\nThere are various interpretations of the Terror and the violence with which it was conducted. François Furet argues that the intense ideological commitment of the revolutionaries and their utopian goals required the extermination of any opposition.[133] A middle position suggests violence was not inevitable but the product of a series of complex internal events, exacerbated by war.[134]\n\nThe bloodshed did not end with the death of Robespierre; Southern France saw a wave of revenge killings, directed against alleged Jacobins, Republican officials and Protestants. Although the victors of Thermidor asserted control over the Commune by executing their leaders, some of those closely involved in the \"Terror\" retained their positions. They included Paul Barras, later chief executive of the French Directory, and Joseph Fouché, director of the killings in Lyon who served as Minister of Police under the Directory, the Consulate and Empire.[135] Despite his links to Augustin Robespierre, military success in Italy meant Napoleon Bonaparte escaped censure.[136]\n\nThe December 1794 Treaty of La Jaunaye ended the Chouannerie in western France by allowing freedom of worship and the return of non-juring priests.[137] This was accompanied by military success; in January 1795, French forces helped the Dutch Patriots set up the Batavian Republic, securing their northern border.[138] The war with Prussia was concluded in favour of France by the Peace of Basel in April 1795, while Spain made peace shortly thereafter.[139]\n\nHowever, the Republic still faced a crisis at home. Food shortages arising from a poor 1794 harvest were exacerbated in Northern France by the need to supply the army in Flanders, while the winter was the worst since 1709.[140] By April 1795, people were starving and the assignat was worth only 8% of its face value; in desperation, the Parisian poor rose again.[141] They were quickly dispersed and the main impact was another round of arrests, while Jacobin prisoners in Lyon were summarily executed.[142]\n\nA committee drafted a new constitution, approved by plebiscite on 23 September 1795 and put into place on 27th.[143] Largely designed by Pierre Daunou and Boissy d'Anglas, it established a bicameral legislature, intended to slow down the legislative process, ending the wild swings of policy under the previous unicameral systems. The Council of 500 was responsible for drafting legislation, which was reviewed and approved by the Council of Ancients, an upper house containing 250 men over the age of 40. Executive power was in the hands of five Directors, selected by the Council of Ancients from a list provided by the lower house, with a five-year mandate.[144]\n\nDeputies were chosen by indirect election, a total franchise of around 5 million voting in primaries for 30,000 electors, or 0.6% of the population. Since they were also subject to stringent property qualification, it guaranteed the return of conservative or moderate deputies. In addition, rather than dissolving the previous legislature as in 1791 and 1792, the so-called 'law of two-thirds' ruled only 150 new deputies would be elected each year. The remaining 600 Conventionnels kept their seats, a move intended to ensure stability.[145]\n\nJacobin sympathisers viewed the Directory as a betrayal of the Revolution, while Bonapartists later justified Napoleon's coup by emphasising its corruption.[146] The regime also faced internal unrest, a weak economy, and an expensive war, while the Council of 500 could block legislation at will. Since the Directors had no power to call new elections, the only way to break a deadlock was rule by decree or use force. As a result, the Directory was characterised by \"chronic violence, ambivalent forms of justice, and repeated recourse to heavy-handed repression.\"[147]\n\nRetention of the Conventionnels ensured the Thermidorians held a majority in the legislature and three of the five Directors, but they were increasingly challenged by the right. On 5 October, Convention troops led by Napoleon put down a royalist rising in Paris; when the first elections were held two weeks later, over 100 of the 150 new deputies were royalists of some sort.[148] The power of the Parisian sans-culottes had been broken by the suppression of the May 1795 revolt; relieved of pressure from below, the Jacobin clubs became supporters of the Directory, largely to prevent restoration of the monarchy.[149]\n\nRemoval of price controls and a collapse in the value of the assignat led to inflation and soaring food prices. By April 1796, over 500,000 Parisians were unemployed, resulting in the May insurrection known as the Conspiracy of the Equals. Led by the revolutionary François-Noël Babeuf, their demands included immediate implementation of the 1793 Constitution, and a more equitable distribution of wealth. Despite support from sections of the military, the revolt was easily crushed, while Babeuf and other leaders were executed.[150] Nevertheless, by 1799 the economy had been stabilised, and important reforms made allowing steady expansion of French industry. Many of these remained in place for much of the 19th century.[151]\n\nPrior to 1797, three of the five Directors were firmly Republican; Barras, Révellière-Lépeaux and Jean-François Rewbell, as were around 40% of the legislature. The same percentage were broadly centrist or unaffiliated, along with two Directors, Étienne-François Letourneur and Lazare Carnot. Although only 20% were committed Royalists, many centrists supported the restoration of the exiled Louis XVIII of France in the belief this would bring peace.[152] The elections of May 1797 resulted in significant gains for the right, with Royalists Jean-Charles Pichegru elected President of the Council of 500, and Barthélemy appointed a Director.[153]\n\nWith Royalists apparently on the verge of power, Republicans attempted a pre-emptive coup on 4 September. Using troops from Napoleon's Army of Italy under Pierre Augereau, the Council of 500 was forced to approve the arrest of Barthélemy, Pichegru and Carnot. The elections were annulled, sixty-three leading Royalists deported to French Guiana, and new laws passed against émigrés, Royalists and ultra-Jacobins. The removal of his conservative opponents opened the way for direct conflict between Barras, and those on the left.[154]\n\nFighting continued despite general war weariness, and the 1798 elections saw a resurgence in Jacobin strength. Napoleon's invasion of Egypt in July 1798 confirmed European fears of French expansionism, and the War of the Second Coalition began in November. Without a majority in the legislature, the Directors relied on the army to enforce decrees, and extract revenue from conquered territories. Generals like Napoleon and Joubert were now central to the political process, while both the army and Directory became notorious for their corruption.[155]\n\nIt has been suggested the Directory collapsed because by 1799, many 'preferred the uncertainties of authoritarian rule to the continuing ambiguities of parliamentary politics'.[156] The architect of its end was Sieyès, who when asked what he had done during the Terror allegedly answered \"I survived\". Nominated to the Directory, his first action was to remove Barras, with the help of allies including Talleyrand, and Napoleon's brother Lucien, President of the Council of 500.[157] On 9 November 1799, the Coup of 18 Brumaire replaced the five Directors with the French Consulate, which consisted of three members, Napoleon, Sieyès, and Roger Ducos. Most historians consider this the end point of the French Revolution.[158]\n\nThe role of ideology in the Revolution is controversial with Jonathan Israel stating that the \"radical Enlightenment\" was the primary driving force of the Revolution.[159] Cobban, however, argues \"[t]he actions of the revolutionaries were most often prescribed by the need to find practical solutions to immediate problems, using the resources at hand, not by pre-conceived theories.\"[160]\n\nThe identification of ideologies is complicated by the profusion of revolutionary clubs, factions and publications, absence of formal political parties, and individual flexibility in the face of changing circumstances.[161] In addition, although the Declaration of the Rights of Man was a fundamental document for all revolutionary factions, its interpretation varied widely.[162]\n\nWhile all revolutionaries professed their devotion to liberty in principle, \"it appeared to mean whatever those in power wanted.\"[163] For example, the liberties specified in the Rights of Man were limited by law when they might \"cause harm to others, or be abused\". Prior to 1792, Jacobins and others frequently opposed press restrictions on the grounds these violated a basic right.[164] However, the radical National Convention passed laws in September 1793 and July 1794 imposing the death penalty for offences such as \"disparaging the National Convention\", and \"misleading public opinion.\"[165]\n\nWhile revolutionaries also endorsed the principle of equality, few advocated equality of wealth since property was also viewed as a right.[166] The National Assembly opposed equal political rights for women,[167] while the abolition of slavery in the colonies was delayed until February 1794 because it conflicted with the property rights of slave owners, and many feared it would disrupt trade.[168] Political equality for male citizens was another divisive issue, with the 1791 constitution limiting the right to vote and stand for office to males over 25 who met a property qualification, so-called \"active citizens\". This restriction was opposed by many activists, including Robespierre, the Jacobins, and Cordeliers.[169]\n\nThe principle that sovereignty resided in the nation was a key concept of the Revolution.[170] However, Israel argues this obscures ideological differences over whether the will of the nation was best expressed through representative assemblies and constitutions, or direct action by revolutionary crowds, and popular assemblies such as the sections of the Paris commune.[171] Many considered constitutional monarchy as incompatible with the principle of popular sovereignty,[172] but prior to 1792, there was a strong bloc with an ideological commitment to such a system, based on the writings of Hobbes, Locke, Montesquieu and Voltaire.[173]\n\nIsrael argues the nationalisation of church property and the establishment of the Constitutional Church reflected an ideological commitment to secularism, and a determination to undermine a bastion of old regime privilege.[174] While Cobban agrees the Constitutional Church was motivated by ideology, he sees its origins in the anti-clericalism of Voltaire and other Enlightenment figures.[175]\n\nJacobins were hostile to formal political parties and factions which they saw as a threat to national unity and the general will, with \"political virtue\" and \"love of country\" key elements of their ideology.[176][177] They viewed the ideal revolutionary as selfless, sincere, free of political ambition, and devoted to the nation.[178] The disputes leading to the departure first of the Feuillants, then later the Girondists, were conducted in terms of the relative political virtue and patriotism of the disputants. In December 1793, all members of the Jacobin clubs were subject to a \"purifying scrutiny\", to determine whether they were \"men of virtue\".[179]\n\nThe Revolution initiated a series of conflicts that began in 1792 and ended only with Napoleon's defeat at Waterloo in 1815. In its early stages, this seemed unlikely; the 1791 Constitution specifically disavowed \"war for the purpose of conquest\", and although traditional tensions between France and Austria re-emerged in the 1780s, Emperor Joseph II cautiously welcomed the reforms. Austria was at war with the Ottomans, as were the Russians, while both were negotiating with Prussia over partitioning Poland. Most importantly, Britain preferred peace, and as Emperor Leopold II stated after the Declaration of Pillnitz, \"without England, there is no case\".[180]\n\nIn late 1791, factions within the Assembly came to see war as a way to unite the country and secure the Revolution by eliminating hostile forces on its borders and establishing its \"natural frontiers\".[181] France declared war on Austria in April 1792 and issued the first conscription orders, with recruits serving for twelve months. By the time peace finally came in 1815, the conflict had involved every major European power as well as the United States, redrawn the map of Europe and expanded into the Americas, the Middle East, and the Indian Ocean.[182]\n\nFrom 1701 to 1801, the population of Europe grew from 118 to 187 million; combined with new mass production techniques, this allowed belligerents to support large armies, requiring the mobilisation of national resources. It was a different kind of war, fought by nations rather than kings, intended to destroy their opponents' ability to resist, but also to implement deep-ranging social change. While all wars are political to some degree, this period was remarkable for the emphasis placed on reshaping boundaries and the creation of entirely new European states.[183]\n\nIn April 1792, French armies invaded the Austrian Netherlands but suffered a series of setbacks before victory over an Austrian-Prussian army at Valmy in September. After defeating a second Austrian army at Jemappes on 6 November, they occupied the Netherlands, areas of the Rhineland, Nice and Savoy. Emboldened by this success, in February 1793 France declared war on the Dutch Republic, Spain and Britain, beginning the War of the First Coalition.[184] However, the expiration of the 12-month term for the 1792 recruits forced the French to relinquish their conquests. In August, new conscription measures were passed and by May 1794 the French army had between 750,000 and 800,000 men.[185] Despite high rates of desertion, this was large enough to manage multiple internal and external threats; for comparison, the combined Prussian-Austrian army was less than 90,000.[186]\n\nBy February 1795, France had annexed the Austrian Netherlands, established their frontier on the left bank of the Rhine and replaced the Dutch Republic with the Batavian Republic, a satellite state. These victories led to the collapse of the anti-French coalition; Prussia made peace in April 1795, followed soon after by Spain, leaving Britain and Austria as the only major powers still in the war.[187] In October 1797, a series of defeats by Bonaparte in Italy led Austria to agree to the Treaty of Campo Formio, in which they formally ceded the Netherlands and recognised the Cisalpine Republic.[188]\n\nFighting continued for two reasons; first, French state finances had come to rely on indemnities levied on their defeated opponents. Second, armies were primarily loyal to their generals, for whom the wealth achieved by victory and the status it conferred became objectives in themselves. Leading soldiers like Hoche, Pichegru and Carnot wielded significant political influence and often set policy; Campo Formio was approved by Bonaparte, not the Directory, which strongly objected to terms it considered too lenient.[188]\n\nDespite these concerns, the Directory never developed a realistic peace programme, fearing the destabilising effects of peace and the consequent demobilisation of hundreds of thousands of young men. As long as the generals and their armies stayed away from Paris, they were happy to allow them to continue fighting, a key factor behind sanctioning Bonaparte's invasion of Egypt. This resulted in aggressive and opportunistic policies, leading to the War of the Second Coalition in November 1798.[189]\n\nIn 1789, the most populous French colonies were Saint-Domingue (today Haiti), Martinique, Guadeloupe, the Île Bourbon (Réunion) and the Île de la France. These colonies produced commodities such as sugar, coffee and cotton for exclusive export to France. There were about 700,000 slaves in the colonies, of which about 500,000 were in Saint-Domingue. Colonial products accounted for about a third of France's exports.[190]\n\nIn February 1788, the Société des Amis des Noirs (Society of the Friends of Blacks) was formed in France with the aim of abolishing slavery in the empire. In August 1789, colonial slave owners and merchants formed the rival Club de Massiac to represent their interests. When the Constituent Assembly adopted the Declaration of the Rights of Man and of the Citizen in August 1789, delegates representing the colonial landowners successfully argued that the principles should not apply in the colonies as they would bring economic ruin and disrupt trade. Colonial landowners also gained control of the Colonial Committee of the Assembly from where they exerted a powerful influence against abolition.[191][192]\n\nPeople of colour also faced social and legal discrimination in mainland France and its colonies, including a bar on their access to professions such as law, medicine and pharmacy.[193] In 1789–90, a delegation of free coloureds, led by Vincent Ogé and Julien Raimond, unsuccessfully lobbied the Assembly to end discrimination against free coloureds. Ogé left for Saint-Domingue where an uprising against white landowners broke out in October 1790. The revolt failed and Ogé was killed.[194][192]\n\nIn May 1791, the National Assembly granted full political rights to coloureds born of two free parents, but left the rights of freed slaves to be determined by the colonial assemblies. The assemblies refused to implement the decree and fighting broke out between the coloured population of Saint-Domingue and white colonists, each side recruiting slaves to their forces. A major slave revolt followed in August.[195]\n\nIn March 1792, the Legislative Assembly responded to the revolt by granting citizenship to all free coloureds and sending two commissioners, Sonthonax and Polvérel, and 6,000 troops to Saint-Domingue to enforce the decree. On arrival in September, the commissioners announced that slavery would remain in force. Over 72,000 slaves were still in revolt, mostly in the north.[196]\n\nBrissot and his supporters envisaged an eventual abolition of slavery but their immediate concern was securing trade and the support of merchants for the revolutionary wars. After Brissot's fall, the new constitution of June 1793 included a new Declaration of the Rights of Man and the Citizen but excluded the colonies from its provisions. In any event, the new constitution was suspended until France was at peace.[197]\n\nIn early 1793, royalist planters from Guadeloupe and Saint-Domingue formed an alliance with Britain. The Spanish supported insurgent slaves, led by Jean-François Papillon and Georges Biassou, in the north of Saint-Domingue. White planters loyal to the republic sent representatives to Paris to convince the Jacobin controlled Convention that those calling for the abolition of slavery were British agents and supporters of Brissot, hoping to disrupt trade.[198]\n\nIn June, the commissioners in Saint-Domingue freed 10,000 slaves fighting for the republic. As the royalists and their British and Spanish supporters were also offering freedom for slaves willing to fight for their cause, the commissioners outbid them by abolishing slavery in the north in August, and throughout the colony in October. Representatives were sent to Paris to gain the approval of the convention for the decision.[198][199]\n\nThe Convention voted for the abolition of slavery in the colonies on 4 February 1794 and decreed that all residents of the colonies had the full rights of French citizens irrespective of colour.[200] An army of 1,000 sans-culottes led by Victor Hugues was sent to Guadeloupe to expel the British and enforce the decree. The army recruited former slaves and eventually numbered 11,000, capturing Guadeloupe and other smaller islands. Abolition was also proclaimed on Guyane. Martinique remained under British occupation, while colonial landowners in Réunion and the Îles Mascareignes repulsed the republicans.[201] Black armies drove the Spanish out of Saint-Domingue in 1795, and the last of the British withdrew in 1798.[202]\n\nIn republican controlled areas from 1793 to 1799, freed slaves were required to work on their former plantations or for their former masters if they were in domestic service. They were paid a wage and gained property rights. Black and coloured generals were effectively in control of large areas of Guadeloupe and Saint-Domingue, including Toussaint Louverture in the north of Saint-Domingue, and André Rigaud in the south. Historian Fréderic Régent states that the restrictions on the freedom of employment and movement of former slaves meant that, \"only whites, persons of color already freed before the decree, and former slaves in the army or on warships really benefited from general emancipation.\"[201]\n\nNewspapers and pamphlets played a central role in stimulating and defining the Revolution. Prior to 1789, there have been a small number of heavily censored newspapers that needed a royal licence to operate, but the Estates-General created an enormous demand for news, and over 130 newspapers appeared by the end of the year. Among the most significant were Marat's L'Ami du peuple and Elysée Loustallot's Revolutions de Paris [fr].[203] Over the next decade, more than 2,000 newspapers were founded, 500 in Paris alone. Most lasted only a matter of weeks but they became the main communication medium, combined with the very large pamphlet literature.[204]\n\nNewspapers were read aloud in taverns and clubs, and circulated hand to hand. There was a widespread assumption that writing was a vocation, not a business, and the role of the press was the advancement of civic republicanism.[205] By 1793 the radicals were most active but initially the royalists flooded the country with their publication the \"L'Ami du Roi [fr]\" (Friends of the King) until they were suppressed.[206]\n\nTo illustrate the differences between the new Republic and the old regime, the leaders needed to implement a new set of symbols to be celebrated instead of the old religious and monarchical symbols. To this end, symbols were borrowed from historic cultures and redefined, while those of the old regime were either destroyed or reattributed acceptable characteristics. These revised symbols were used to instil in the public a new sense of tradition and reverence for the Enlightenment and the Republic.[207]\n\n\"La Marseillaise\" (French pronunciation: [la maʁsɛjɛːz]) became the national anthem of France. The song was written and composed in 1792 by Claude Joseph Rouget de Lisle, and was originally titled \"Chant de guerre pour l'Armée du Rhin\". The French National Convention adopted it as the First Republic's anthem in 1795. It acquired its nickname after being sung in Paris by volunteers from Marseille marching on the capital.\n\nThe song is the first example of the \"European march\" anthemic style, while the evocative melody and lyrics led to its widespread use as a song of revolution and incorporation into many pieces of classical and popular music. De Lisle was instructed to 'produce a hymn which conveys to the soul of the people the enthusiasm which it (the music) suggests.'[209]\n\nThe guillotine remains \"the principal symbol of the Terror in the French Revolution.\"[210] Invented by a physician during the Revolution as a quicker, more efficient and more distinctive form of execution, the guillotine became a part of popular culture and historic memory. It was celebrated on the left as the people's avenger, for example in the revolutionary song La guillotine permanente,[211] and cursed as the symbol of the Terror by the right.[212]\n\nIts operation became a popular entertainment that attracted great crowds of spectators. Vendors sold programmes listing the names of those scheduled to die. Many people came day after day and vied for the best locations from which to observe the proceedings; knitting women (tricoteuses) formed a cadre of hardcore regulars, inciting the crowd. Parents often brought their children. By the end of the Terror, the crowds had thinned drastically. Repetition had staled even this most grisly of entertainments, and audiences grew bored.[213]\n\nCockades were widely worn by revolutionaries beginning in 1789. They now pinned the blue-and-red cockade of Paris onto the white cockade of the Ancien Régime. Camille Desmoulins asked his followers to wear green cockades on 12 July 1789. The Paris militia, formed on 13 July, adopted a blue and red cockade. Blue and red are the traditional colours of Paris, and they are used on the city's coat of arms. Cockades with various colour schemes were used during the storming of the Bastille on 14 July.[214]\n\nThe Liberty cap, also known as the Phrygian cap, or pileus, is a brimless, felt cap that is conical in shape with the tip pulled forward. It reflects Roman republicanism and liberty, alluding to the Roman ritual of manumission, in which a freed slave receives the bonnet as a symbol of his newfound liberty.[215]\n\nDeprived of political rights by the Ancien Régime, the Revolution initially allowed women to participate, although only to a limited degree. Activists included Girondists like Olympe de Gouges, author of the Declaration of the Rights of Woman and of the Female Citizen, and Charlotte Corday, killer of Marat. Others like Théroigne de Méricourt, Pauline Léon and the Society of Revolutionary Republican Women supported the Jacobins, staged demonstrations in the National Assembly and took part in the October 1789 March to Versailles. Despite this, the 1791 and 1793 constitutions denied them political rights and democratic citizenship.[216]\n\nIn 1793, the Society of Revolutionary Republican Women campaigned for strict price controls on bread, and a law that would compel all women to wear the tricolour cockade. Although both demands were successful, in October the male-dominated Jacobins who then controlled the government denounced the Society as dangerous rabble-rousers and made all women's clubs and associations illegal. Organised women were permanently shut out of the French Revolution after 30 October 1793.[217]\n\nAt the same time, especially in the provinces, women played a prominent role in resisting social changes introduced by the Revolution. This was particularly so in terms of the reduced role of the Catholic Church; for those living in rural areas, closing of the churches meant a loss of normality.[218] This sparked a counter-revolutionary movement led by women; while supporting other political and social changes, they opposed the dissolution of the Catholic Church and revolutionary cults like the Cult of the Supreme Being.[219] Olwen Hufton argues some wanted to protect the Church from heretical changes enforced by revolutionaries, viewing themselves as \"defenders of faith\".[220]\n\nOlympe de Gouges was an author whose publications emphasised that while women and men were different, this should not prevent equality under the law. In her Declaration of the Rights of Woman and of the Female Citizen she insisted women deserved rights, especially in areas concerning them directly, such as divorce and recognition of illegitimate children.[221][full citation needed] Along with other Girondists, she was executed in November 1793 during the Terror.\n\nMadame Roland, also known as Manon or Marie Roland, was another important female activist whose political focus was not specifically women but other aspects of the government. A Girondist, her personal letters to leaders of the Revolution influenced policy; in addition, she often hosted political gatherings of the Brissotins, a political group which allowed women to join. She too was executed in November 1793.[222]\n\nThe Revolution abolished many economic constraints imposed by the Ancien Régime, including church tithes and feudal dues although tenants often paid higher rents and taxes.[223] All church lands were nationalised, along with those owned by Royalist exiles, which were used to back paper currency known as assignats, and the feudal guild system eliminated.[224] It also abolished the highly inefficient system of tax farming, whereby private individuals would collect taxes for a hefty fee. The government seized the foundations that had been set up (starting in the 13th century) to provide an annual stream of revenue for hospitals, poor relief, and education. The state sold the lands but typically local authorities did not replace the funding and so most of the nation's charitable and school systems were massively disrupted.[225]\n\nBetween 1790 and 1796, industrial and agricultural output dropped, foreign trade plunged, and prices soared, forcing the government to finance expenditure by issuing ever increasing quantities assignats. When this resulted in escalating inflation, the response was to impose price controls and persecute private speculators and traders, creating a black market. Between 1789 and 1793, the annual deficit increased from 10% to 64% of gross national product, while annual inflation reached 3,500% after a poor harvest in 1794 and the removal of price controls. The assignats were withdrawn in 1796 but inflation continued until the introduction of the gold-based Franc germinal in 1803.[226]\n\nThe French Revolution had a major impact on western history, by ending feudalism in France and creating a path for advances in individual freedoms throughout Europe.[227][2] The revolution represented the most significant challenge to political absolutism up to that point in history and spread democratic ideals throughout Europe and ultimately the world.[228] Its impact on French nationalism was profound, while also stimulating nationalist movements throughout Europe.[229] Some modern historians argue the concept of the nation state was a direct consequence of the revolution.[230] As such, the revolution is often seen as marking the start of modernity and the modern period.[231]\n\n\nThe long-term impact on France was profound, shaping politics, society, religion and ideas, and polarising politics for more than a century. Historian François Aulard wrote:\n\"From the social point of view, the Revolution consisted in the suppression of what was called the feudal system, in the emancipation of the individual, in greater division of landed property, the abolition of the privileges of noble birth, the establishment of equality, the simplification of life.... The French Revolution differed from other revolutions in being not merely national, for it aimed at benefiting all humanity.\"[232][title missing]\nThe revolution permanently crippled the power of the aristocracy and drained the wealth of the Church, although the two institutions survived. Hanson suggests the French underwent a fundamental transformation in self-identity, evidenced by the elimination of privileges and their replacement by intrinsic human rights.[233] After the collapse of the First French Empire in 1815, the French public lost many of the rights and privileges earned since the revolution, but remembered the participatory politics that characterised the period. According to Paul Hanson, \"Revolution became a tradition, and republicanism an enduring option.\"[234]\n\nThe Revolution meant an end to arbitrary royal rule and held out the promise of rule by law under a constitutional order. Napoleon as emperor set up a constitutional system and the restored Bourbons were forced to retain one. After the abdication of Napoleon III in 1871, the French Third Republic was launched with a deep commitment to upholding the ideals of the Revolution.[235][236] The Vichy regime (1940–1944), tried to undo the revolutionary heritage, but retained the republic. However, there were no efforts by the Bourbons, Vichy or any other government to restore the privileges that had been stripped away from the nobility in 1789. France permanently became a society of equals under the law.[234]\n\nAgriculture was transformed by the Revolution. With the breakup of large estates controlled by the Church and the nobility and worked by hired hands, rural France became more a land of small independent farms. Harvest taxes were ended, such as the tithe and seigneurial dues. Primogeniture was ended both for nobles and peasants, thereby weakening the family patriarch, and led to a fall in the birth rate since all children had a share in the family property.[237] Cobban argues the Revolution bequeathed to the nation \"a ruling class of landowners.\"[238]\n\nEconomic historians are divided on the economic impact of the Revolution. One suggestion is the resulting fragmentation of agricultural holdings had a significant negative impact in the early years of 19th century, then became positive in the second half of the century because it facilitated the rise in human capital investments.[239] Others argue the redistribution of land had an immediate positive impact on agricultural productivity, before the scale of these gains gradually declined over the course of the 19th century.[240]\n\nIn the cities, entrepreneurship on a small scale flourished, as restrictive monopolies, privileges, barriers, rules, taxes and guilds gave way. However, the British blockade virtually ended overseas and colonial trade, hurting the cities and their supply chains. Overall, the Revolution did not greatly change the French business system, and probably helped freeze in place the horizons of the small business owner. The typical businessman owned a small store, mill or shop, with family help and a few paid employees; large-scale industry was less common than in other industrialising nations.[241]\n\nHistorians often see the impact of the Revolution as through the institutions and ideas exported by Napoleon. Economic historians Dan Bogart, Mauricio Drelichman, Oscar Gelderblom, and Jean-Laurent Rosenthal describe Napoleon's codified law as the French Revolution's \"most significant export.\"[242] According to Daron Acemoglu, Davide Cantoni, Simon Johnson, and James A. Robinson the French Revolution had long-term effects in Europe. They suggest that \"areas that were occupied by the French and that underwent radical institutional reform experienced more rapid urbanization and economic growth, especially after 1850. There is no evidence of a negative effect of French invasion.\"[243]\n\nThe Revolution sparked intense debate in Britain. The Revolution Controversy was a \"pamphlet war\" set off by the publication of A Discourse on the Love of Our Country, a speech given by Richard Price to the Revolution Society on 4 November 1789, supporting the French Revolution. Edmund Burke responded in November 1790 with his own pamphlet, Reflections on the Revolution in France, attacking the French Revolution as a threat to the aristocracy of all countries.[244][245] William Coxe opposed Price's premise that one's country is principles and people, not the State itself.[246]\n\nConversely, two seminal political pieces of political history were written in Price's favour, supporting the general right of the French people to replace their State. One of the first of these \"pamphlets\" into print was A Vindication of the Rights of Men by Mary Wollstonecraft . Wollstonecraft's title was echoed by Thomas Paine's Rights of Man, published a few months later. In 1792 Christopher Wyvill published Defence of Dr. Price and the Reformers of England, a plea for reform and moderation.[247] This exchange of ideas has been described as \"one of the great political debates in British history\".[248]\n\nIn Ireland, the effect was to transform what had been an attempt by Protestant settlers to gain some autonomy into a mass movement led by the Society of United Irishmen involving Catholics and Protestants. It stimulated the demand for further reform throughout Ireland, especially in Ulster. The upshot was a revolt in 1798, led by Wolfe Tone, that was crushed by Britain.[249]\n\nGerman reaction to the Revolution swung from favourable to antagonistic. At first it brought liberal and democratic ideas, the end of guilds, serfdom and the Jewish ghetto. It brought economic freedoms and agrarian and legal reform. Above all the antagonism helped stimulate and shape German nationalism.[250]\n\nFrance invaded Switzerland and turned it into the \"Helvetic Republic\" (1798–1803), a French puppet state. French interference with localism and traditions was deeply resented in Switzerland, although some reforms took hold and survived in the later period of restoration.[251][252]\n\nDuring the Revolutionary Wars, the French invaded and occupied the region now known as Belgium between 1794 and 1814. The new government enforced reforms, incorporating the region into France. Resistance was strong in every sector, as Belgian nationalism emerged to oppose French rule. The French legal system, however, was adopted, with its equal legal rights, and abolition of class distinctions.[253]\n\nThe Kingdom of Denmark adopted liberalising reforms in line with those of the French Revolution. Reform was gradual and the regime itself carried out agrarian reforms that had the effect of weakening absolutism by creating a class of independent peasant freeholders. Much of the initiative came from well-organised liberals who directed political change in the first half of the 19th century.[254]\n\nThe Constitution of Norway of 1814 was inspired by the French Revolution,[255] and was considered to be one of the most liberal and democratic constitutions at the time.[256]\n\nInitially, most people in the Province of Quebec were favourable toward the revolutionaries' aims. The Revolution took place against the background of an ongoing campaign for constitutional reform in the colony by Loyalist emigrants from the United States.[257] Public opinion began to shift against the Revolution after the Flight to Varennes and further soured after the September Massacres and the subsequent execution of Louis XVI.[258] French migration to the Canadas experienced a substantial decline during and after the Revolution. Only a limited number of artisans, professionals, and religious emigres were allowed to settle in the region during this period.[259] Most emigres settled in Montreal or Quebec City.[259] The influx of religious emigres also revitalised the local Catholic Church, with exiled priests establishing a number of parishes across the Canadas.[259]\n\nIn the United States, the French Revolution deeply polarised American politics, and this polarisation led to the creation of the First Party System. In 1793, as war broke out in Europe, the Democratic-Republican Party led by former American minister to France Thomas Jefferson favored revolutionary France and pointed to the 1778 treaty that was still in effect. George Washington and his unanimous cabinet, including Jefferson, decided that the treaty did not bind the United States to enter the war. Washington proclaimed neutrality instead.[260]\n\nThe first writings on the French revolution were near contemporaneous with events and mainly divided along ideological lines. These included Edmund Burke's conservative critique Reflections on the Revolution in France (1790) and Thomas Paine's response Rights of Man (1791).[261] From 1815, narrative histories dominated, often based on first-hand experience of the revolutionary years. By the mid-nineteenth century, more scholarly histories appeared, written by specialists and based on original documents and a more critical assessment of contemporary accounts.[262]\n\nDupuy identifies three main strands in nineteenth century historiography of the Revolution. The first is represented by reactionary writers who rejected the revolutionary ideals of popular sovereignty, civil equality, and the promotion of rationality, progress and personal happiness over religious faith. The second stream is those writers who celebrated its democratic, and republican values. The third were liberals like Germaine de Staël and Guizot, who accepted the necessity of reforms establishing a constitution and the rights of man, but rejected state interference with private property and individual rights, even when supported by a democratic majority.[263]\n\nJules Michelet was a leading 19th-century historian of the democratic republican strand, and Thiers, Mignet and Tocqueville were prominent in the liberal strand.[264] Hippolyte Taine's Origins of Contemporary France (1875–1894) was modern in its use of departmental archives, but Dupuy sees him as reactionary, given his contempt for the crowd, and Revolutionary values.[265]\n\nThe broad distinction between conservative, democratic-republican and liberal interpretations of the Revolution persisted in the 20th-century, although historiography became more nuanced, with greater attention to critical analysis of documentary evidence.[265][266] Alphonse Aulard (1849–1928) was the first professional historian of the Revolution; he promoted graduate studies, scholarly editions, and learned journals.[267][268] His major work, The French Revolution, a Political History, 1789–1804 (1905), was a democratic and republican interpretation of the Revolution.[269]\n\nSocio-economic analysis and a focus on the experiences of ordinary people dominated French studies of the Revolution from the 1930s.[270] Georges Lefebvre elaborated a Marxist socio-economic analysis of the revolution with detailed studies of peasants, the rural panic of 1789, and the behaviour of revolutionary crowds.[271][272] Albert Soboul, also writing in the Marxist-Republican tradition, published a major study of the sans-culottes in 1958.[273]\n\nAlfred Cobban challenged Jacobin-Marxist social and economic explanations of the revolution in two important works, The Myth of the French Revolution (1955) and Social Interpretation of the French Revolution (1964). He argued the Revolution was primarily a political conflict, which ended in a victory for conservative property owners, a result which retarded economic development.[274][275]\n\nIn their 1965 work, La Revolution française, François Furet and Denis Richet also argued for the primacy of political decisions, contrasting the reformist period of 1789 to 1790 with the following interventions of the urban masses which led to radicalisation and an ungovernable situation.[276]\n\nFrom the 1990s, Western scholars largely abandoned Marxist interpretations of the revolution in terms of bourgeoisie-proletarian class struggle as anachronistic. However, no new explanatory model has gained widespread support.[231][277] The historiography of the Revolution has expanded into areas such as cultural and regional histories, visual representations, transnational interpretations, and decolonisation.[276]\n"
    },
    {
        "title": "World War II",
        "url": "https://en.wikipedia.org/wiki/World_War_II",
        "content": "\n\nAsia-Pacific\n\nMediterranean and Middle East\n\nOther campaigns\n\nCoups\n\nWorld War II[b] or the Second World War (1 September 1939 – 2 September 1945) was a global conflict between two coalitions: the Allies and the Axis powers. Nearly all the world's countries—including all the great powers—participated, with many investing all available economic, industrial, and scientific capabilities in pursuit of total war, blurring the distinction between military and civilian resources. Tanks and aircraft played major roles, with the latter enabling the strategic bombing of population centres and delivery of the only two nuclear weapons ever used in war.  World War II was the deadliest conflict in history, resulting in 70 to 85 million deaths, more than half being civilians. Millions died in genocides, including the Holocaust of European Jews, as well as from massacres, starvation, and disease. Following the Allied powers' victory, Germany, Austria, Japan, and Korea were occupied, and war crimes tribunals were conducted against German and Japanese leaders.\n\nThe causes of World War II included unresolved tensions in the aftermath of World War I and the rise of fascism in Europe and militarism in Japan. Key events leading up to the war included Japan's invasion of Manchuria, the Spanish Civil War, the outbreak of the Second Sino-Japanese War, and Germany's annexations of Austria and the Sudetenland. World War II is generally considered to have begun on 1 September 1939, when Nazi Germany, under Adolf Hitler, invaded Poland, prompting the United Kingdom and France to declare war on Germany.  Poland was divided between Germany and the Soviet Union under the Molotov–Ribbentrop Pact, in which they had agreed on \"spheres of influence\" in Eastern Europe. In 1940, the Soviets annexed the Baltic states and parts of Finland and Romania. After the fall of France in June 1940, the war continued mainly between Germany and the British Empire, with fighting in the Balkans, Mediterranean, and Middle East, the aerial Battle of Britain and the Blitz, and naval Battle of the Atlantic. Through a series of campaigns and treaties, Germany took control of much of continental Europe and formed the Axis alliance with Italy, Japan, and other countries. In June 1941, Germany led the European Axis in an invasion of the Soviet Union, opening the Eastern Front and initially making large territorial gains.\n\nJapan aimed to dominate East Asia and the Asia-Pacific, and by 1937 was at war with the Republic of China. In December 1941, Japan attacked American and British territories in Southeast Asia and the Central Pacific, including Pearl Harbor in Hawaii, which resulted in the US and the UK declaring war against Japan, and the European Axis declaring war on the US. Japan conquered much of coastal China and Southeast Asia, but its advances in the Pacific were halted in mid-1942 after its defeat in the naval Battle of Midway; Germany and Italy were defeated in North Africa and at Stalingrad in the Soviet Union.  Key setbacks in 1943—including German defeats on the Eastern Front, the Allied invasions of Sicily and the Italian mainland, and Allied offensives in the Pacific—cost the Axis powers their initiative and forced them into strategic retreat on all fronts. In 1944, the Western Allies invaded German-occupied France at Normandy, while the Soviet Union regained its territorial losses and pushed Germany and its allies westward. At the same time, Japan suffered reversals in mainland Asia, while the Allies crippled the Japanese Navy and captured key islands. \n\nThe war in Europe concluded with the liberation of German-occupied territories; the invasion of Germany by the Western Allies and the Soviet Union, culminating in the fall of Berlin to Soviet troops; Hitler's suicide; and the German unconditional surrender on 8 May 1945. Following the refusal of Japan to surrender on the terms of the Potsdam Declaration, the US dropped the first atomic bombs on Hiroshima and Nagasaki on 6 and 9 August. Faced with an imminent invasion of the Japanese archipelago, the possibility of further atomic bombings, and the Soviet declaration of war against Japan and its invasion of Manchuria, Japan announced its unconditional surrender on 15 August and signed a surrender document on 2 September 1945, marking the end of the war.\n\nWorld War II changed the political alignment and social structure of the world, and it set the foundation of international relations for the rest of the 20th century and into the 21st century. The United Nations was established to foster international cooperation and prevent conflicts, with the victorious great powers—China, France, the Soviet Union, the UK, and the US—becoming the permanent members of its security council. The Soviet Union and the United States emerged as rival superpowers, setting the stage for the Cold War. In the wake of European devastation, the influence of its great powers waned, triggering the decolonisation of Africa and Asia. Most countries whose industries had been damaged moved towards economic recovery and expansion.\n\nWorld War II began in Europe on 1 September 1939[1][2] with the German invasion of Poland and the United Kingdom and France's declaration of war on Germany two days later on 3 September 1939. Dates for the beginning of the Pacific War include the start of the Second Sino-Japanese War on 7 July 1937,[3][4] or the earlier Japanese invasion of Manchuria, on 19 September 1931.[5][6] Others follow the British historian A. J. P. Taylor, who stated that the Sino-Japanese War and war in Europe and its colonies occurred simultaneously, and the two wars became World War II in 1941.[7] Other proposed starting dates for World War II include the Italian invasion of Abyssinia on 3 October 1935.[8] The British historian Antony Beevor views the beginning of World War II as the Battles of Khalkhin Gol fought between Japan and the forces of Mongolia and the Soviet Union from May to September 1939.[9] Others view the Spanish Civil War as the start or prelude to World War II.[10][11]\n\nThe exact date of the war's end also is not universally agreed upon. It was generally accepted at the time that the war ended with the armistice of 15 August 1945 (V-J Day), rather than with the formal surrender of Japan on 2 September 1945, which officially ended the war in Asia. A peace treaty between Japan and the Allies was signed in 1951.[12] A 1990 treaty regarding Germany's future allowed the reunification of East and West Germany to take place and resolved most post–World War II issues.[13] No formal peace treaty between Japan and the Soviet Union was ever signed,[14] although the state of war between the two countries was terminated by the Soviet–Japanese Joint Declaration of 1956, which also restored full diplomatic relations between them.[15]\n\nWorld War I had radically altered the political European map with the defeat of the Central Powers—including Austria-Hungary, Germany, Bulgaria, and the Ottoman Empire—and the 1917 Bolshevik seizure of power in Russia, which led to the founding of the Soviet Union. Meanwhile, the victorious Allies of World War I, such as France, Belgium, Italy, Romania, and Greece, gained territory, and new nation-states were created out of the dissolution of the Austro-Hungarian, Ottoman, and Russian Empires.[16]\n\nTo prevent a future world war, the League of Nations was established in 1920 by the Paris Peace Conference. The organisation's primary goals were to prevent armed conflict through collective security, military, and naval disarmament, as well as settling international disputes through peaceful negotiations and arbitration.[17]\n\nDespite strong pacifist sentiment after World War I,[18] irredentist and revanchist nationalism had emerged in several European states. These sentiments were especially marked in Germany because of the significant territorial, colonial, and financial losses imposed by the Treaty of Versailles. Under the treaty, Germany lost around 13 percent of its home territory and all its overseas possessions, while German annexation of other states was prohibited, reparations were imposed, and limits were placed on the size and capability of the country's armed forces.[19]\n\nThe German Empire was dissolved in the German revolution of 1918–1919, and a democratic government, later known as the Weimar Republic, was created. The interwar period saw strife between supporters of the new republic and hardline opponents on both the political right and left. Italy, as an Entente ally, had made some post-war territorial gains; however, Italian nationalists were angered that the promises made by the United Kingdom and France to secure Italian entrance into the war were not fulfilled in the peace settlement. From 1922 to 1925, the Fascist movement led by Benito Mussolini seized power in Italy with a nationalist, totalitarian, and class collaborationist agenda that abolished representative democracy, repressed socialist, left-wing, and liberal forces, and pursued an aggressive expansionist foreign policy aimed at making Italy a world power, promising the creation of a \"New Roman Empire\".[20]\n\nAdolf Hitler, after an unsuccessful attempt to overthrow the German government in 1923, eventually became the chancellor of Germany in 1933 when President Paul von Hindenburg and the Reichstag appointed him. Following Hindenburg's death in 1934, Hitler proclaimed himself Führer of Germany and abolished democracy, espousing a radical, racially motivated revision of the world order, and soon began a massive rearmament campaign.[21] France, seeking to secure its alliance with Italy, allowed Italy a free hand in Ethiopia, which Italy desired as a colonial possession. The situation was aggravated in early 1935 when the Territory of the Saar Basin was legally reunited with Germany, and Hitler repudiated the Treaty of Versailles, accelerated his rearmament programme, and introduced conscription.[22]\n\nThe United Kingdom, France and Italy formed the Stresa Front in April 1935 in order to contain Germany, a key step towards military globalisation; however, that June, the United Kingdom made an independent naval agreement with Germany, easing prior restrictions. The Soviet Union, concerned by Germany's goals of capturing vast areas of Eastern Europe, drafted a treaty of mutual assistance with France. Before taking effect, though, the Franco-Soviet pact was required to go through the bureaucracy of the League of Nations, which rendered it essentially toothless.[23] The United States, concerned with events in Europe and Asia, passed the Neutrality Act in August of the same year.[24]\n\nHitler defied the Versailles and Locarno Treaties by remilitarising the Rhineland in March 1936, encountering little opposition due to the policy of appeasement.[25] In October 1936, Germany and Italy formed the Rome–Berlin Axis. A month later, Germany and Japan signed the Anti-Comintern Pact, which Italy joined the following year.[26]\n\nThe Kuomintang (KMT) party in China launched a unification campaign against regional warlords and nominally unified China in the mid-1920s, but was soon embroiled in a civil war against its former Chinese Communist Party (CCP) allies[27] and new regional warlords. In 1931, an increasingly militaristic Empire of Japan, which had long sought influence in China[28] as the first step of what its government saw as the country's right to rule Asia, staged the Mukden incident as a pretext to invade Manchuria and establish the puppet state of Manchukuo.[29]\n\nChina appealed to the League of Nations to stop the Japanese invasion of Manchuria. Japan withdrew from the League of Nations after being condemned for its incursion into Manchuria. The two nations then fought several battles, in Shanghai, Rehe and Hebei, until the Tanggu Truce was signed in 1933. Thereafter, Chinese volunteer forces continued the resistance to Japanese aggression in Manchuria, and Chahar and Suiyuan.[30] After the 1936 Xi'an Incident, the Kuomintang and CCP forces agreed on a ceasefire to present a united front to oppose Japan.[31]\n\nThe Second Italo-Ethiopian War was a brief colonial war that began in October 1935 and ended in May 1936. The war began with the invasion of the Ethiopian Empire (also known as Abyssinia) by the armed forces of the Kingdom of Italy (Regno d'Italia), which was launched from Italian Somaliland and Eritrea.[32] The war resulted in the military occupation of Ethiopia and its annexation into the newly created colony of Italian East Africa (Africa Orientale Italiana, or AOI); in addition it exposed the weakness of the League of Nations as a force to preserve peace. Both Italy and Ethiopia were member nations, but the League did little when the former clearly violated Article X of the League's Covenant.[33] The United Kingdom and France supported imposing sanctions on Italy for the invasion, but the sanctions were not fully enforced and failed to end the Italian invasion.[34] Italy subsequently dropped its objections to Germany's goal of absorbing Austria.[35]\n\nWhen civil war broke out in Spain, Hitler and Mussolini lent military support to the Nationalist rebels, led by General Francisco Franco. Italy supported the Nationalists to a greater extent than the Nazis: Mussolini sent more than 70,000 ground troops, 6,000 aviation personnel, and 720 aircraft to Spain.[36] The Soviet Union supported the existing government of the Spanish Republic. More than 30,000 foreign volunteers, known as the International Brigades, also fought against the Nationalists. Both Germany and the Soviet Union used this proxy war as an opportunity to test in combat their most advanced weapons and tactics. The Nationalists won the civil war in April 1939; Franco, now dictator, remained officially neutral during World War II but generally favoured the Axis.[37] His greatest collaboration with Germany was the sending of volunteers to fight on the Eastern Front.[38]\n\nIn July 1937, Japan captured the former Chinese imperial capital of Peking after instigating the Marco Polo Bridge incident, which culminated in the Japanese campaign to invade all of China.[39] The Soviets quickly signed a non-aggression pact with China to lend materiel support, effectively ending China's prior cooperation with Germany. From September to November, the Japanese attacked Taiyuan, engaged the Kuomintang Army around Xinkou,[40] and fought Communist forces in Pingxingguan.[41][42] Generalissimo Chiang Kai-shek deployed his best army to defend Shanghai, but after three months of fighting, Shanghai fell. The Japanese continued to push Chinese forces back, capturing the capital Nanking in December 1937. After the fall of Nanking, tens or hundreds of thousands of Chinese civilians and disarmed combatants were murdered by the Japanese.[43][44]\n\nIn March 1938, Nationalist Chinese forces won their first major victory at Taierzhuang, but then the city of Xuzhou was taken by the Japanese in May.[45] In June 1938, Chinese forces stalled the Japanese advance by flooding the Yellow River; this manoeuvre bought time for the Chinese to prepare their defences at Wuhan, but the city was taken by October.[46] Japanese military victories did not bring about the collapse of Chinese resistance that Japan had hoped to achieve; instead, the Chinese government relocated inland to Chongqing and continued the war.[47][48]\n\nIn the mid-to-late 1930s, Japanese forces in Manchukuo had sporadic border clashes with the Soviet Union and Mongolia. The Japanese doctrine of Hokushin-ron, which emphasised Japan's expansion northward, was favoured by the Imperial Army during this time. This policy would prove difficult to maintain in light of the Japanese defeat at Khalkin Gol in 1939, the ongoing Second Sino-Japanese War[49] and ally Nazi Germany pursuing neutrality with the Soviets. Japan and the Soviet Union eventually signed a Neutrality Pact in April 1941, and Japan adopted the doctrine of Nanshin-ron, promoted by the Navy, which took its focus southward and eventually led to war with the United States and the Western Allies.[50][51]\n\nIn Europe, Germany and Italy were becoming more aggressive. In March 1938, Germany annexed Austria, again provoking little response from other European powers.[52] Encouraged, Hitler began pressing German claims on the Sudetenland, an area of Czechoslovakia with a predominantly ethnic German population. Soon the United Kingdom and France followed the appeasement policy of British Prime Minister Neville Chamberlain and conceded this territory to Germany in the Munich Agreement, which was made against the wishes of the Czechoslovak government, in exchange for a promise of no further territorial demands.[53] Soon afterwards, Germany and Italy forced Czechoslovakia to cede additional territory to Hungary, and Poland annexed the Trans-Olza region of Czechoslovakia.[54]\n\nAlthough all of Germany's stated demands had been satisfied by the agreement, privately Hitler was furious that British interference had prevented him from seizing all of Czechoslovakia in one operation. In subsequent speeches Hitler attacked British and Jewish \"war-mongers\" and in January 1939 secretly ordered a major build-up of the German navy to challenge British naval supremacy. In March 1939, Germany invaded the remainder of Czechoslovakia and subsequently split it into the German Protectorate of Bohemia and Moravia and a pro-German client state, the Slovak Republic.[55] Hitler also delivered an ultimatum to Lithuania on 20 March 1939, forcing the concession of the Klaipėda Region, formerly the German Memelland.[56]\n\nGreatly alarmed and with Hitler making further demands on the Free City of Danzig, the United Kingdom and France guaranteed their support for Polish independence; when Italy conquered Albania in April 1939, the same guarantee was extended to the Kingdoms of Romania and Greece.[57] Shortly after the Franco-British pledge to Poland, Germany and Italy formalised their own alliance with the Pact of Steel.[58] Hitler accused the United Kingdom and Poland of trying to \"encircle\" Germany and renounced the Anglo-German Naval Agreement and the German–Polish declaration of non-aggression.[59]\n\nThe situation became a crisis in late August as German troops continued to mobilise against the Polish border. On 23 August the Soviet Union signed a non-aggression pact with Germany,[60] after tripartite negotiations for a military alliance between France, the United Kingdom, and Soviet Union had stalled.[61] This pact had a secret protocol that defined German and Soviet \"spheres of influence\" (western Poland and Lithuania for Germany; eastern Poland, Finland, Estonia, Latvia and Bessarabia for the Soviet Union), and raised the question of continuing Polish independence.[62] The pact neutralised the possibility of Soviet opposition to a campaign against Poland and assured that Germany would not have to face the prospect of a two-front war, as it had in World War I. Immediately afterwards, Hitler ordered the attack to proceed on 26 August, but upon hearing that the United Kingdom had concluded a formal mutual assistance pact with Poland and that Italy would maintain neutrality, he decided to delay it.[63]\n\nIn response to British requests for direct negotiations to avoid war, Germany made demands on Poland, which served as a pretext to worsen relations.[64] On 29 August, Hitler demanded that a Polish plenipotentiary immediately travel to Berlin to negotiate the handover of Danzig, and to allow a plebiscite in the Polish Corridor in which the German minority would vote on secession.[64] The Poles refused to comply with the German demands, and on the night of 30–31 August in a confrontational meeting with the British ambassador Nevile Henderson, Ribbentrop declared that Germany considered its claims rejected.[65]\n\nOn 1 September 1939, Germany invaded Poland after having staged several false flag border incidents as a pretext to initiate the invasion.[66] The first German attack of the war came against the Polish defences at Westerplatte.[67] The United Kingdom responded with an ultimatum for Germany to cease military operations, and on 3 September, after the ultimatum was ignored, Britain and France declared war on Germany.[68] During the Phoney War period, the alliance provided no direct military support to Poland, outside of a cautious French probe into the Saarland.[69] The Western Allies also began a naval blockade of Germany, which aimed to damage the country's economy and war effort.[70] Germany responded by ordering U-boat warfare against Allied merchant and warships, which would later escalate into the Battle of the Atlantic.[71]\n\nOn 8 September, German troops reached the suburbs of Warsaw. The Polish counter-offensive to the west halted the German advance for several days, but it was outflanked and encircled by the Wehrmacht. Remnants of the Polish army broke through to besieged Warsaw. On 17 September 1939, two days after signing a cease-fire with Japan, the Soviet Union invaded Poland[72] under the supposed pretext that the Polish state had ceased to exist.[73] On 27 September, the Warsaw garrison surrendered to the Germans, and the last large operational unit of the Polish Army surrendered on 6 October. Despite the military defeat, Poland never surrendered; instead, it formed the Polish government-in-exile and a clandestine state apparatus remained in occupied Poland.[74] A significant part of Polish military personnel evacuated to Romania and Latvia; many of them later fought against the Axis in other theatres of the war.[75]\n\nGermany annexed western Poland and occupied central Poland; the Soviet Union annexed eastern Poland; small shares of Polish territory were transferred to Lithuania and Slovakia. On 6 October, Hitler made a public peace overture to the United Kingdom and France but said that the future of Poland was to be determined exclusively by Germany and the Soviet Union. The proposal was rejected[65] and Hitler ordered an immediate offensive against France,[76] which was postponed until the spring of 1940 due to bad weather.[77][78][79]\n\nAfter the outbreak of war in Poland, Stalin threatened Estonia, Latvia, and Lithuania with military invasion, forcing the three Baltic countries to sign pacts allowing the creation of Soviet military bases in these countries; in October 1939, significant Soviet military contingents were moved there.[80][81][82] Finland refused to sign a similar pact and rejected ceding part of its territory to the Soviet Union. The Soviet Union invaded Finland in November 1939,[83] and was subsequently expelled from the League of Nations for this crime of aggression.[84] Despite overwhelming numerical superiority, Soviet military success during the Winter War was modest,[85] and the Finno-Soviet war ended in March 1940 with some Finnish concessions of territory.[86]\n\nIn June 1940, the Soviet Union occupied the entire territories of Estonia, Latvia and Lithuania,[81] as well as the Romanian regions of Bessarabia, Northern Bukovina, and the Hertsa region. In August 1940, Hitler imposed the Second Vienna Award on Romania which led to the transfer of Northern Transylvania to Hungary.[87] In September 1940, Bulgaria demanded Southern Dobruja from Romania with German and Italian support, leading to the Treaty of Craiova.[88] The loss of one-third of Romania's 1939 territory caused a coup against King Carol II, turning Romania into a fascist dictatorship under Marshal Ion Antonescu, with a course set towards the Axis in the hopes of a German guarantee.[89] Meanwhile, German-Soviet political relations and economic co-operation[90][91] gradually stalled,[92][93] and both states began preparations for war.[94]\n\nIn April 1940, Germany invaded Denmark and Norway to protect shipments of iron ore from Sweden, which the Allies were attempting to cut off.[95] Denmark capitulated after six hours, and despite Allied support, Norway was conquered within two months.[96] British discontent over the Norwegian campaign led to the resignation of Prime Minister Neville Chamberlain, who was replaced by Winston Churchill on 10 May 1940.[97]\n\nOn the same day, Germany launched an offensive against France. To circumvent the strong Maginot Line fortifications on the Franco-German border, Germany directed its attack at the neutral nations of Belgium, the Netherlands, and Luxembourg.[98] The Germans carried out a flanking manoeuvre through the Ardennes region,[99] which was mistakenly perceived by the Allies as an impenetrable natural barrier against armoured vehicles.[100][101] By successfully implementing new Blitzkrieg tactics, the Wehrmacht rapidly advanced to the Channel and cut off the Allied forces in Belgium, trapping the bulk of the Allied armies in a cauldron on the Franco-Belgian border near Lille. The United Kingdom was able to evacuate a significant number of Allied troops from the continent by early June, although they had to abandon almost all their equipment.[102]\n\nOn 10 June, Italy invaded France, declaring war on both France and the United Kingdom.[103] The Germans turned south against the weakened French army, and Paris fell to them on 14 June. Eight days later France signed an armistice with Germany; it was divided into German and Italian occupation zones,[104] and an unoccupied rump state under the Vichy Regime, which, though officially neutral, was generally aligned with Germany. France kept its fleet, which the United Kingdom attacked on 3 July in an attempt to prevent its seizure by Germany.[105]\n\nThe air Battle of Britain[106] began in early July with Luftwaffe attacks on shipping and harbours.[107] The German campaign for air superiority started in August but its failure to defeat RAF Fighter Command forced the indefinite postponement of the proposed German invasion of Britain. The German strategic bombing offensive intensified with night attacks on London and other cities in the Blitz, but largely ended in May 1941[108] after failing to significantly disrupt the British war effort.[107]\n\nUsing newly captured French ports, the German Navy enjoyed success against an over-extended Royal Navy, using U-boats against British shipping in the Atlantic.[109] The British Home Fleet scored a significant victory on 27 May 1941 by sinking the German battleship Bismarck.[110]\n\nIn November 1939, the United States was assisting China and the Western Allies, and had amended the Neutrality Act to allow \"cash and carry\" purchases by the Allies.[111] In 1940, following the German capture of Paris, the size of the United States Navy was significantly increased. In September the United States further agreed to a trade of American destroyers for British bases.[112] Still, a large majority of the American public continued to oppose any direct military intervention in the conflict well into 1941.[113] In December 1940, Roosevelt accused Hitler of planning world conquest and ruled out any negotiations as useless, calling for the United States to become an \"arsenal of democracy\" and promoting Lend-Lease programmes of military and humanitarian aid to support the British war effort; Lend-Lease was later extended to the other Allies, including the Soviet Union after it was invaded by Germany.[114] The United States started strategic planning to prepare for a full-scale offensive against Germany.[115]\n\nAt the end of September 1940, the Tripartite Pact formally united Japan, Italy, and Germany as the Axis powers. The Tripartite Pact stipulated that any country—with the exception of the Soviet Union—that attacked any Axis Power would be forced to go to war against all three.[116] The Axis expanded in November 1940 when Hungary, Slovakia, and Romania joined.[117] Romania and Hungary later made major contributions to the Axis war against the Soviet Union, in Romania's case partially to recapture territory ceded to the Soviet Union.[118]\n\nIn early June 1940, the Italian Regia Aeronautica attacked and besieged Malta, a British possession. From late summer to early autumn, Italy conquered British Somaliland and made an incursion into British-held Egypt. In October, Italy attacked Greece, but the attack was repulsed with heavy Italian casualties; the campaign ended within months with minor territorial changes.[119] To assist Italy and prevent Britain from gaining a foothold, Germany prepared to invade the Balkans, which would threaten Romanian oil fields and strike against British dominance of the Mediterranean.[120]\n\nIn December 1940, British Empire forces began counter-offensives against Italian forces in Egypt and Italian East Africa.[121] The offensives were successful; by early February 1941, Italy had lost control of eastern Libya, and large numbers of Italian troops had been taken prisoner. The Italian Navy also suffered significant defeats, with the Royal Navy putting three Italian battleships out of commission after a carrier attack at Taranto, and neutralising several more warships at the Battle of Cape Matapan.[122]\n\nItalian defeats prompted Germany to deploy an expeditionary force to North Africa; at the end of March 1941, Rommel's Afrika Korps launched an offensive which drove back Commonwealth forces.[123] In less than a month, Axis forces advanced to western Egypt and besieged the port of Tobruk.[124]\n\nBy late March 1941, Bulgaria and Yugoslavia signed the Tripartite Pact; however, the Yugoslav government was overthrown two days later by pro-British nationalists. Germany and Italy responded with simultaneous invasions of both Yugoslavia and Greece, commencing on 6 April 1941; both nations were forced to surrender within the month.[125] The airborne invasion of the Greek island of Crete at the end of May completed the German conquest of the Balkans.[126] Partisan warfare subsequently broke out against the Axis occupation of Yugoslavia, which continued until the end of the war.[127]\n\nIn the Middle East in May, Commonwealth forces quashed an uprising in Iraq which had been supported by German aircraft from bases within Vichy-controlled Syria.[128] Between June and July, British-led forces invaded and occupied the French possessions of Syria and Lebanon, assisted by the Free French.[129]\n\nWith the situation in Europe and Asia relatively stable, Germany, Japan, and the Soviet Union made preparations for war. With the Soviets wary of mounting tensions with Germany, and the Japanese planning to take advantage of the European War by seizing resource-rich European possessions in Southeast Asia, the two powers signed the Soviet–Japanese Neutrality Pact in April 1941.[130] By contrast, the Germans were steadily making preparations for an attack on the Soviet Union, massing forces on the Soviet border.[131]\n\nHitler believed that the United Kingdom's refusal to end the war was based on the hope that the United States and the Soviet Union would enter the war against Germany sooner or later.[132] On 31 July 1940, Hitler decided that the Soviet Union should be eliminated and aimed for the conquest of Ukraine, the Baltic states and Byelorussia.[133] However, other senior German officials like Ribbentrop saw an opportunity to create a Euro-Asian bloc against the British Empire by inviting the Soviet Union into the Tripartite Pact.[134] In November 1940, negotiations took place to determine if the Soviet Union would join the pact. The Soviets showed some interest but asked for concessions from Finland, Bulgaria, Turkey, and Japan that Germany considered unacceptable. On 18 December 1940, Hitler issued the directive to prepare for an invasion of the Soviet Union.[135]\n\nOn 22 June 1941, Germany, supported by Italy and Romania, invaded the Soviet Union in Operation Barbarossa, with Germany accusing the Soviets of plotting against them; they were joined shortly by Finland and Hungary.[136] The primary targets of this surprise offensive[137] were the Baltic region, Moscow and Ukraine, with the ultimate goal of ending the 1941 campaign near the Arkhangelsk-Astrakhan line—from the Caspian to the White Seas. Hitler's objectives were to eliminate the Soviet Union as a military power, exterminate Communism, generate Lebensraum (\"living space\")[138] by dispossessing the native population,[139] and guarantee access to the strategic resources needed to defeat Germany's remaining rivals.[140]\n\nAlthough the Red Army was preparing for strategic counter-offensives before the war,[141] Operation Barbarossa forced the Soviet supreme command to adopt strategic defence. During the summer, the Axis made significant gains into Soviet territory, inflicting immense losses in both personnel and materiel. By mid-August, however, the German Army High Command decided to suspend the offensive of a considerably depleted Army Group Centre, and to divert the 2nd Panzer Group to reinforce troops advancing towards central Ukraine and Leningrad.[142] The Kiev offensive was overwhelmingly successful, resulting in encirclement and elimination of four Soviet armies, and made possible further advance into Crimea and industrially-developed Eastern Ukraine (the First Battle of Kharkov).[143]\n\nThe diversion of three-quarters of the Axis troops and the majority of their air forces from France and the central Mediterranean to the Eastern Front[144] prompted the United Kingdom to reconsider its grand strategy.[145] In July, the UK and the Soviet Union formed a military alliance against Germany[146] and in August, the United Kingdom and the United States jointly issued the Atlantic Charter, which outlined British and American goals for the post-war world.[147] In late August the British and Soviets invaded neutral Iran to secure the Persian Corridor, Iran's oil fields, and preempt any Axis advances through Iran toward the Baku oil fields or India.[148]\n\nBy October, Axis powers had achieved operational objectives in Ukraine and the Baltic region, with only the sieges of Leningrad[149] and Sevastopol continuing.[150] A major offensive against Moscow was renewed; after two months of fierce battles in increasingly harsh weather, the German army almost reached the outer suburbs of Moscow, where the exhausted troops[151] were forced to suspend the offensive.[152] Large territorial gains were made by Axis forces, but their campaign had failed to achieve its main objectives: two key cities remained in Soviet hands, the Soviet capability to resist was not broken, and the Soviet Union retained a considerable part of its military potential. The blitzkrieg phase of the war in Europe had ended.[153]\n\nBy early December, freshly mobilised reserves[154] allowed the Soviets to achieve numerical parity with Axis troops.[155] This, as well as intelligence data which established that a minimal number of Soviet troops in the East would be sufficient to deter any attack by the Japanese Kwantung Army,[156] allowed the Soviets to begin a massive counter-offensive that started on 5 December all along the front and pushed German troops 100–250 kilometres (62–155 mi) west.[157]\n\nFollowing the Japanese false flag Mukden incident in 1931, the Japanese shelling of the American gunboat USS Panay in 1937, and the 1937–1938 Nanjing Massacre, Japanese-American relations deteriorated. In 1939, the United States notified Japan that it would not be extending its trade treaty and American public opinion opposing Japanese expansionism led to a series of economic sanctions—the Export Control Acts—which banned U.S. exports of chemicals, minerals and military parts to Japan, and increased economic pressure on the Japanese regime.[114][158][159] During 1939 Japan launched its first attack against Changsha, but was repulsed by late September.[160] Despite several offensives by both sides, by 1940 the war between China and Japan was at a stalemate. To increase pressure on China by blocking supply routes, and to better position Japanese forces in the event of a war with the Western powers, Japan invaded and occupied northern Indochina in September 1940.[161]\n\nChinese nationalist forces launched a large-scale counter-offensive in early 1940. In August, Chinese communists launched an offensive in Central China; in retaliation, Japan instituted harsh measures in occupied areas to reduce human and material resources for the communists.[162] Continued antipathy between Chinese communist and nationalist forces culminated in armed clashes in January 1941, effectively ending their co-operation.[163] In March, the Japanese 11th army attacked the headquarters of the Chinese 19th army but was repulsed during Battle of Shanggao.[164] In September, Japan attempted to take the city of Changsha again and clashed with Chinese nationalist forces.[165]\n\nGerman successes in Europe prompted Japan to increase pressure on European governments in Southeast Asia. The Dutch government agreed to provide Japan with oil supplies from the Dutch East Indies, but negotiations for additional access to their resources ended in failure in June 1941.[166] In July 1941 Japan sent troops to southern Indochina, thus threatening British and Dutch possessions in the Far East. The United States, the United Kingdom, and other Western governments reacted to this move with a freeze on Japanese assets and a total oil embargo.[167][168] At the same time, Japan was planning an invasion of the Soviet Far East, intending to take advantage of the German invasion in the west, but abandoned the operation after the sanctions.[169]\n\nSince early 1941, the United States and Japan had been engaged in negotiations in an attempt to improve their strained relations and end the war in China. During these negotiations, Japan advanced a number of proposals which were dismissed by the Americans as inadequate.[170] At the same time the United States, the United Kingdom, and the Netherlands engaged in secret discussions for the joint defence of their territories, in the event of a Japanese attack against any of them.[171] Roosevelt reinforced the Philippines (an American protectorate scheduled for independence in 1946) and warned Japan that the United States would react to Japanese attacks against any \"neighboring countries\".[171]\n\nFrustrated at the lack of progress and feeling the pinch of the American–British–Dutch sanctions, Japan prepared for war. Emperor Hirohito, after initial hesitation about Japan's chances of victory,[172] began to favour Japan's entry into the war.[173] As a result, Prime Minister Fumimaro Konoe resigned.[174][175] Hirohito refused the recommendation to appoint Prince Naruhiko Higashikuni in his place, choosing War Minister Hideki Tojo instead.[176] On 3 November, Nagano explained in detail the plan of the attack on Pearl Harbor to the Emperor.[177] On 5 November, Hirohito approved in imperial conference the operations plan for the war.[178] On 20 November, the new government presented an interim proposal as its final offer. It called for the end of American aid to China and for lifting the embargo on the supply of oil and other resources to Japan. In exchange, Japan promised not to launch any attacks in Southeast Asia and to withdraw its forces from southern Indochina.[170] The American counter-proposal of 26 November required that Japan evacuate all of China without conditions and conclude non-aggression pacts with all Pacific powers.[179] That meant Japan was essentially forced to choose between abandoning its ambitions in China, or seizing the natural resources it needed in the Dutch East Indies by force;[180][181] the Japanese military did not consider the former an option, and many officers considered the oil embargo an unspoken declaration of war.[182]\n\nJapan planned to seize European colonies in Asia to create a large defensive perimeter stretching into the Central Pacific. The Japanese would then be free to exploit the resources of Southeast Asia while exhausting the over-stretched Allies by fighting a defensive war.[183][184] To prevent American intervention while securing the perimeter, it was further planned to neutralise the United States Pacific Fleet and the American military presence in the Philippines from the outset.[185] On 7 December 1941 (8 December in Asian time zones), Japan attacked British and American holdings with near-simultaneous offensives against Southeast Asia and the Central Pacific.[186] These included an attack on the American fleets at Pearl Harbor and the Philippines, as well as invasions of Guam, Wake Island, Malaya,[186] Thailand, and Hong Kong.[187]\n\nThese attacks led the United States, United Kingdom, China, Australia, and several other states to formally declare war on Japan, whereas the Soviet Union, being heavily involved in large-scale hostilities with European Axis countries, maintained its neutrality agreement with Japan.[188] Germany, followed by the other Axis states, declared war on the United States[189] in solidarity with Japan, citing as justification the American attacks on German war vessels that had been ordered by Roosevelt.[136][190]\n\nOn 1 January 1942, the Allied Big Four[191]—the Soviet Union, China, the United Kingdom, and the United States—and 22 smaller or exiled governments issued the Declaration by United Nations, thereby affirming the Atlantic Charter[192] and agreeing not to sign a separate peace with the Axis powers.[193]\n\nDuring 1942, Allied officials debated on the appropriate grand strategy to pursue. All agreed that defeating Germany was the primary objective. The Americans favoured a straightforward, large-scale attack on Germany through France. The Soviets demanded a second front. The British argued that military operations should target peripheral areas to wear out German strength, leading to increasing demoralisation, and bolstering resistance forces; Germany itself would be subject to a heavy bombing campaign. An offensive against Germany would then be launched primarily by Allied armour, without using large-scale armies.[194] Eventually, the British persuaded the Americans that a landing in France was infeasible in 1942 and they should instead focus on driving the Axis out of North Africa.[195]\n\nAt the Casablanca Conference in early 1943, the Allies reiterated the statements issued in the 1942 Declaration and demanded the unconditional surrender of their enemies. The British and Americans agreed to continue to press the initiative in the Mediterranean by invading Sicily to fully secure the Mediterranean supply routes.[196] Although the British argued for further operations in the Balkans to bring Turkey into the war, in May 1943, the Americans extracted a British commitment to limit Allied operations in the Mediterranean to an invasion of the Italian mainland, and to invade France in 1944.[197]\n\nBy the end of April 1942, Japan and its ally Thailand had almost conquered Burma, Malaya, the Dutch East Indies, Singapore, and Rabaul, inflicting severe losses on Allied troops and taking a large number of prisoners.[198] Despite stubborn resistance by Filipino and U.S. forces, the Philippine Commonwealth was eventually captured in May 1942, forcing its government into exile.[199] On 16 April, in Burma, 7,000 British soldiers were encircled by the Japanese 33rd Division during the Battle of Yenangyaung and rescued by the Chinese 38th Division.[200] Japanese forces also achieved naval victories in the South China Sea, Java Sea, and Indian Ocean,[201] and bombed the Allied naval base at Darwin, Australia. In January 1942, the only Allied success against Japan was a Chinese victory at Changsha.[202] These easy victories over the unprepared U.S. and European opponents left Japan overconfident, and overextended.[203]\n\nIn early May 1942, Japan initiated operations to capture Port Moresby by amphibious assault and thus sever communications and supply lines between the United States and Australia. The planned invasion was thwarted when an Allied task force, centred on two American fleet carriers, fought Japanese naval forces to a draw in the Battle of the Coral Sea.[204] Japan's next plan, motivated by the earlier Doolittle Raid, was to seize Midway Atoll and lure American carriers into battle to be eliminated; as a diversion, Japan would also send forces to occupy the Aleutian Islands in Alaska.[205] In mid-May, Japan started the Zhejiang-Jiangxi campaign in China, with the goal of inflicting retribution on the Chinese who aided the surviving American airmen in the Doolittle Raid by destroying Chinese air bases and fighting against the Chinese 23rd and 32nd Army Groups.[206][207] In early June, Japan put its operations into action, but the Americans had broken Japanese naval codes in late May and were fully aware of the plans and order of battle, and used this knowledge to achieve a decisive victory at Midway over the Imperial Japanese Navy.[208]\n\nWith its capacity for aggressive action greatly diminished as a result of the Midway battle, Japan attempted to capture Port Moresby by an overland campaign in the Territory of Papua.[209] The Americans planned a counterattack against Japanese positions in the southern Solomon Islands, primarily Guadalcanal, as a first step towards capturing Rabaul, the main Japanese base in Southeast Asia.[210]\n\nBoth plans started in July, but by mid-September, the Battle for Guadalcanal took priority for the Japanese, and troops in New Guinea were ordered to withdraw from the Port Moresby area to the northern part of the island, where they faced Australian and United States troops in the Battle of Buna–Gona.[211] Guadalcanal soon became a focal point for both sides with heavy commitments of troops and ships in the battle for Guadalcanal. By the start of 1943, the Japanese were defeated on the island and withdrew their troops.[212] In Burma, Commonwealth forces mounted two operations. The first was a disastrous offensive into the Arakan region in late 1942 that forced a retreat back to India by May 1943.[213] The second was the insertion of irregular forces behind Japanese frontlines in February which, by the end of April, had achieved mixed results.[214]\n\nDespite considerable losses, in early 1942 Germany and its allies stopped a major Soviet offensive in central and southern Russia, keeping most territorial gains they had achieved during the previous year.[215] In May, the Germans defeated Soviet offensives in the Kerch Peninsula and at Kharkov,[216] and then in June 1942 launched their main summer offensive against southern Russia, to seize the oil fields of the Caucasus and occupy the Kuban steppe, while maintaining positions on the northern and central areas of the front. The Germans split Army Group South into two groups: Army Group A advanced to the lower Don River and struck south-east to the Caucasus, while Army Group B headed towards the Volga River. The Soviets decided to make their stand at Stalingrad on the Volga.[217]\n\nBy mid-November, the Germans had nearly taken Stalingrad in bitter street fighting. The Soviets began their second winter counter-offensive, starting with an encirclement of German forces at Stalingrad,[218] and an assault on the Rzhev salient near Moscow, though the latter failed disastrously.[219] By early February 1943, the German Army had taken tremendous losses; German troops at Stalingrad had been defeated,[220] and the front-line had been pushed back beyond its position before the summer offensive. In mid-February, after the Soviet push had tapered off, the Germans launched another attack on Kharkov, creating a salient in their front line around the Soviet city of Kursk.[221]\n\nExploiting poor American naval command decisions, the German navy ravaged Allied shipping off the American Atlantic coast.[222] By November 1941, Commonwealth forces had launched a counter-offensive in North Africa, Operation Crusader, and reclaimed all the gains the Germans and Italians had made.[223] The Germans also launched a North African offensive in January, pushing the British back to positions at the Gazala line by early February,[224] followed by a temporary lull in combat which Germany used to prepare for their upcoming offensives.[225] Concerns that the Japanese might use bases in Vichy-held Madagascar caused the British to invade the island in early May 1942.[226] An Axis offensive in Libya forced an Allied retreat deep inside Egypt until Axis forces were stopped at El Alamein.[227] On the Continent, raids of Allied commandos on strategic targets, culminating in the failed Dieppe Raid,[228] demonstrated the Western Allies' inability to launch an invasion of continental Europe without much better preparation, equipment, and operational security.[229]\n\nIn August 1942, the Allies succeeded in repelling a second attack against El Alamein[230] and, at a high cost, managed to deliver desperately needed supplies to the besieged Malta.[231] A few months later, the Allies commenced an attack of their own in Egypt, dislodging the Axis forces and beginning a drive west across Libya.[232] This attack was followed up shortly after by Anglo-American landings in French North Africa, which resulted in the region joining the Allies.[233] Hitler responded to the French colony's defection by ordering the occupation of Vichy France;[233] although Vichy forces did not resist this violation of the armistice, they managed to scuttle their fleet to prevent its capture by German forces.[233][234] Axis forces in Africa withdrew into Tunisia, which was conquered by the Allies in May 1943.[233][235]\n\nIn June 1943, the British and Americans began a strategic bombing campaign against Germany with a goal to disrupt the war economy, reduce morale, and \"de-house\" the civilian population.[236] The firebombing of Hamburg was among the first attacks in this campaign, inflicting significant casualties and considerable losses on infrastructure of this important industrial centre.[237]\n\nAfter the Guadalcanal campaign, the Allies initiated several operations against Japan in the Pacific. In May 1943, Canadian and U.S. forces were sent to eliminate Japanese forces from the Aleutians.[238] Soon after, the United States, with support from Australia, New Zealand and Pacific Islander forces, began major ground, sea and air operations to isolate Rabaul by capturing surrounding islands, and breach the Japanese Central Pacific perimeter at the Gilbert and Marshall Islands.[239] By the end of March 1944, the Allies had completed both of these objectives and had also neutralised the major Japanese base at Truk in the Caroline Islands. In April, the Allies launched an operation to retake Western New Guinea.[240]\n\nIn the Soviet Union, both the Germans and the Soviets spent the spring and early summer of 1943 preparing for large offensives in central Russia. On 5 July 1943, Germany attacked Soviet forces around the Kursk Bulge. Within a week, German forces had exhausted themselves against the Soviets' well-constructed defences,[241] and for the first time in the war, Hitler cancelled an operation before it had achieved tactical or operational success.[242] This decision was partially affected by the Western Allies' invasion of Sicily launched on 9 July, which, combined with previous Italian failures, resulted in the ousting and arrest of Mussolini later that month.[243]\n\nOn 12 July 1943, the Soviets launched their own counter-offensives, thereby dispelling any chance of German victory or even stalemate in the east. The Soviet victory at Kursk marked the end of German superiority,[244] giving the Soviet Union the initiative on the Eastern Front.[245][246] The Germans tried to stabilise their eastern front along the hastily fortified Panther–Wotan line, but the Soviets broke through it at Smolensk and the Lower Dnieper Offensive.[247]\n\nOn 3 September 1943, the Western Allies invaded the Italian mainland, following Italy's armistice with the Allies and the ensuing German occupation of Italy.[248] Germany, with the help of fascists, responded to the armistice by disarming Italian forces that were in many places without superior orders, seizing military control of Italian areas,[249] and creating a series of defensive lines.[250] German special forces then rescued Mussolini, who then soon established a new client state in German-occupied Italy named the Italian Social Republic,[251] causing an Italian civil war. The Western Allies fought through several lines until reaching the main German defensive line in mid-November.[252]\n\nGerman operations in the Atlantic also suffered. By May 1943, as Allied counter-measures became increasingly effective, the resulting sizeable German submarine losses forced a temporary halt of the German Atlantic naval campaign.[253] In November 1943, Franklin D. Roosevelt and Winston Churchill met with Chiang Kai-shek in Cairo and then with Joseph Stalin in Tehran.[254] The former conference determined the post-war return of Japanese territory[255] and the military planning for the Burma campaign,[256] while the latter included agreement that the Western Allies would invade Europe in 1944 and that the Soviet Union would declare war on Japan within three months of Germany's defeat.[257]\n\nFrom November 1943, during the seven-week Battle of Changde, the Chinese awaited allied relief as they forced Japan to fight a costly war of attrition.[258][259][260] In January 1944, the Allies launched a series of attacks in Italy against the line at Monte Cassino and tried to outflank it with landings at Anzio.[261]\n\nOn 27 January 1944, Soviet troops launched a major offensive that expelled German forces from the Leningrad region, thereby ending the most lethal siege in history.[262] The following Soviet offensive was halted on the pre-war Estonian border by the German Army Group North aided by Estonians hoping to re-establish national independence. This delay slowed subsequent Soviet operations in the Baltic Sea region.[263] By late May 1944, the Soviets had liberated Crimea, largely expelled Axis forces from Ukraine, and made incursions into Romania, which were repulsed by the Axis troops.[264] The Allied offensives in Italy had succeeded and, at the expense of allowing several German divisions to retreat, Rome was captured on 4 June.[265]\n\nThe Allies had mixed success in mainland Asia. In March 1944, the Japanese launched the first of two invasions, an operation against Allied positions in Assam, India,[266] and soon besieged Commonwealth positions at Imphal and Kohima.[267] In May 1944, British and Indian forces mounted a counter-offensive that drove Japanese troops back to Burma by July,[267] and Chinese forces that had invaded northern Burma in late 1943 besieged Japanese troops in Myitkyina.[268] The second Japanese invasion of China aimed to destroy China's main fighting forces, secure railways between Japanese-held territory and capture Allied airfields.[269] By June, the Japanese had conquered the province of Henan and begun a new attack on Changsha.[270]\n\nOn 6 June 1944 (commonly known as D-Day), after three years of Soviet pressure,[271] the Western Allies invaded northern France. After reassigning several Allied divisions from Italy, they also attacked southern France.[272] These landings were successful and led to the defeat of the German Army units in France. Paris was liberated on 25 August by the local resistance assisted by the Free French Forces, both led by General Charles de Gaulle,[273] and the Western Allies continued to push back German forces in western Europe during the latter part of the year. An attempt to advance into northern Germany spearheaded by a major airborne operation in the Netherlands failed.[274] After that, the Western Allies slowly pushed into Germany, but failed to cross the Ruhr river. In Italy, the Allied advance slowed due to the last major German defensive line.[275]\n\nOn 22 June, the Soviets launched a strategic offensive in Belarus (\"Operation Bagration\") that nearly destroyed the German Army Group Centre.[276] Soon after that, another Soviet strategic offensive forced German troops from Western Ukraine and Eastern Poland. The Soviets formed the Polish Committee of National Liberation to control territory in Poland and combat the Polish Armia Krajowa; the Soviet Red Army remained in the Praga district on the other side of the Vistula and watched passively as the Germans quelled the Warsaw Uprising initiated by the Armia Krajowa.[277] The national uprising in Slovakia was also quelled by the Germans.[278] The Soviet Red Army's strategic offensive in eastern Romania cut off and destroyed the considerable German troops there and triggered a successful coup d'état in Romania and in Bulgaria, followed by those countries' shift to the Allied side.[279]\n\nIn September 1944, Soviet troops advanced into Yugoslavia and forced the rapid withdrawal of German Army Groups E and F in Greece, Albania and Yugoslavia to rescue them from being cut off.[280] By this point, the communist-led Partisans under Marshal Josip Broz Tito, who had led an increasingly successful guerrilla campaign against the occupation since 1941, controlled much of the territory of Yugoslavia and engaged in delaying efforts against German forces further south. In northern Serbia, the Soviet Red Army, with limited support from Bulgarian forces, assisted the Partisans in a joint liberation of the capital city of Belgrade on 20 October. A few days later, the Soviets launched a massive assault against German-occupied Hungary that lasted until the fall of Budapest in February 1945.[281] Unlike impressive Soviet victories in the Balkans, bitter Finnish resistance to the Soviet offensive in the Karelian Isthmus denied the Soviets occupation of Finland and led to a Soviet-Finnish armistice on relatively mild conditions,[282] although Finland was forced to fight their former German allies.[283]\n\nBy the start of July 1944, Commonwealth forces in Southeast Asia had repelled the Japanese sieges in Assam, pushing the Japanese back to the Chindwin River[284] while the Chinese captured Myitkyina. In September 1944, Chinese forces captured Mount Song and reopened the Burma Road.[285] In China, the Japanese had more successes, having finally captured Changsha in mid-June and the city of Hengyang by early August.[286] Soon after, they invaded the province of Guangxi, winning major engagements against Chinese forces at Guilin and Liuzhou by the end of November[287] and successfully linking up their forces in China and Indochina by mid-December.[288]\n\nIn the Pacific, U.S. forces continued to push back the Japanese perimeter. In mid-June 1944, they began their offensive against the Mariana and Palau islands and decisively defeated Japanese forces in the Battle of the Philippine Sea. These defeats led to the resignation of the Japanese Prime Minister, Hideki Tojo, and provided the United States with air bases to launch intensive heavy bomber attacks on the Japanese home islands. In late October, American forces invaded the Filipino island of Leyte; soon after, Allied naval forces scored another large victory in the Battle of Leyte Gulf, one of the largest naval battles in history.[289]\n\nOn 16 December 1944, Germany made a last attempt to split the Allies on the Western Front by using most of its remaining reserves to launch a massive counter-offensive in the Ardennes and along the French-German border, hoping to encircle large portions of Western Allied troops and prompt a political settlement after capturing their primary supply port at Antwerp. By 16 January 1945, this offensive had been repulsed with no strategic objectives fulfilled.[290] In Italy, the Western Allies remained stalemated at the German defensive line. In mid-January 1945, the Red Army attacked in Poland, pushing from the Vistula to the Oder river in Germany, and overran East Prussia.[291] On 4 February Soviet, British, and U.S. leaders met for the Yalta Conference. They agreed on the occupation of post-war Germany, and on when the Soviet Union would join the war against Japan.[292]\n\nIn February, the Soviets entered Silesia and Pomerania, while the Western Allies entered western Germany and closed to the Rhine river. By March, the Western Allies crossed the Rhine north and south of the Ruhr, encircling the German Army Group B.[293] In early March, in an attempt to protect its last oil reserves in Hungary and retake Budapest, Germany launched its last major offensive against Soviet troops near Lake Balaton. Within two weeks, the offensive had been repulsed, the Soviets advanced to Vienna, and captured the city. In early April, Soviet troops captured Königsberg, while the Western Allies finally pushed forward in Italy and swept across western Germany capturing Hamburg and Nuremberg. American and Soviet forces met at the Elbe river on 25 April, leaving unoccupied pockets in southern Germany and around Berlin.\n\nSoviet troops stormed and captured Berlin in late April.[294] In Italy, German forces surrendered on 29 April, while the Italian Social Republic capitulated two days later. On 30 April, the Reichstag was captured, signalling the military defeat of Nazi Germany.[295]\n\nMajor changes in leadership occurred on both sides during this period. On 12 April, President Roosevelt died and was succeeded by his vice president, Harry S. Truman. Benito Mussolini was killed by Italian partisans on 28 April.[296] On 30 April, Hitler committed suicide in his headquarters, and was succeeded by Grand Admiral Karl Dönitz (as President of the Reich) and Joseph Goebbels (as Chancellor of the Reich); Goebbels also committed suicide on the following day and was replaced by Lutz Graf Schwerin von Krosigk, in what would later be known as the Flensburg Government. Total and unconditional surrender in Europe was signed on 7 and 8 May, to be effective by the end of 8 May.[297] German Army Group Centre resisted in Prague until 11 May.[298] On 23 May all remaining members of the German government were arrested by the Allied Forces in Flensburg, while on 5 June all German political and military institutions were transferred under the control of the Allies through the Berlin Declaration.[299]\n\nIn the Pacific theatre, American forces accompanied by the forces of the Philippine Commonwealth advanced in the Philippines, clearing Leyte by the end of April 1945. They landed on Luzon in January 1945 and recaptured Manila in March. Fighting continued on Luzon, Mindanao, and other islands of the Philippines until the end of the war.[300] Meanwhile, the United States Army Air Forces launched a massive firebombing campaign of strategic cities in Japan in an effort to destroy Japanese war industry and civilian morale. A devastating bombing raid on Tokyo of 9–10 March was the deadliest conventional bombing raid in history.[301]\n\nIn May 1945, Australian troops landed in Borneo, overrunning the oilfields there. British, American, and Chinese forces defeated the Japanese in northern Burma in March, and the British pushed on to reach Rangoon by 3 May.[302] Chinese forces started a counterattack in the Battle of West Hunan that occurred between 6 April and 7 June 1945. American naval and amphibious forces also moved towards Japan, taking Iwo Jima by March, and Okinawa by the end of June.[303] At the same time, a naval blockade by submarines was strangling Japan's economy and drastically reducing its ability to supply overseas forces.[304][305]\n\nOn 11 July, Allied leaders met in Potsdam, Germany. They confirmed earlier agreements about Germany,[306] and the American, British and Chinese governments reiterated the demand for unconditional surrender of Japan, specifically stating that \"the alternative for Japan is prompt and utter destruction\".[307] During this conference, the United Kingdom held its general election, and Clement Attlee replaced Churchill as Prime Minister.[308]\n\nThe call for unconditional surrender was rejected by the Japanese government, which believed it would be capable of negotiating for more favourable surrender terms.[309] In early August, the United States dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki. Between the two bombings, the Soviets, pursuant to the Yalta agreement, declared war on Japan, invaded Japanese-held Manchuria and quickly defeated the Kwantung Army, which was the largest Japanese fighting force.[310] These two events persuaded previously adamant Imperial Army leaders to accept surrender terms.[311] The Red Army also captured the southern part of Sakhalin Island and the Kuril Islands. On the night of 9–10 August 1945, Emperor Hirohito announced his decision to accept the terms demanded by the Allies in the Potsdam Declaration.[312] On 15 August, the Emperor communicated this decision to the Japanese people through a speech broadcast on the radio (Gyokuon-hōsō, literally \"broadcast in the Emperor's voice\").[313] On 15 August 1945, Japan surrendered, with the surrender documents finally signed at Tokyo Bay on the deck of the American battleship USS Missouri on 2 September 1945, ending the war.[314]\n\nThe Allies established occupation administrations in Austria and Germany, both initially divided between western and eastern occupation zones controlled by the Western Allies and the Soviet Union, respectively. However, their paths soon diverged. In Germany, the western and eastern occupation zones controlled by the Western Allies and the Soviet Union officially ended in 1949, with the respective zones becoming separate countries, West Germany and East Germany.[315] In Austria, however, occupation continued until 1955, when a joint settlement between the Western Allies and the Soviet Union permitted the reunification of Austria as a democratic state officially non-aligned with any political bloc (although in practice having better relations with the Western Allies). A denazification program in Germany led to the prosecution of Nazi war criminals in the Nuremberg trials and the removal of ex-Nazis from power, although this policy moved towards amnesty and re-integration of ex-Nazis into West German society.[316]\n\nGermany lost a quarter of its pre-war (1937) territory. Among the eastern territories, Silesia, Neumark and most of Pomerania were taken over by Poland,[317] and East Prussia was divided between Poland and the Soviet Union, followed by the expulsion to Germany of the nine million Germans from these provinces,[318][319] as well as three million Germans from the Sudetenland in Czechoslovakia. By the 1950s, one-fifth of West Germans were refugees from the east. The Soviet Union also took over the Polish provinces east of the Curzon Line,[320] from which two million Poles were expelled.[319][321] North-east Romania,[322][323] parts of eastern Finland,[324] and the Baltic states were annexed into the Soviet Union.[325][326] Italy lost its monarchy, colonial empire and some European territories.[327]\n\nIn an effort to maintain world peace,[328] the Allies formed the United Nations,[329] which officially came into existence on 24 October 1945,[330] and adopted the Universal Declaration of Human Rights in 1948 as a common standard for all member nations.[331] The great powers that were the victors of the war—France, China, the United Kingdom, the Soviet Union and the United States—became the permanent members of the UN's Security Council.[332] The five permanent members remain so to the present, although there have been two seat changes, between the Republic of China and the People's Republic of China in 1971, and between the Soviet Union and its successor state, the Russian Federation, following the dissolution of the Soviet Union in 1991. The alliance between the Western Allies and the Soviet Union had begun to deteriorate even before the war was over.[333]\n\nBesides Germany, the rest of Europe was also divided into Western and Soviet spheres of influence.[334] Most eastern and central European countries fell into the Soviet sphere, which led to establishment of Communist-led regimes, with full or partial support of the Soviet occupation authorities. As a result, East Germany,[335] Poland, Hungary, Romania, Bulgaria, Czechoslovakia, and Albania[336] became Soviet satellite states. Communist Yugoslavia conducted a fully independent policy, causing tension with the Soviet Union.[337] A Communist uprising in Greece was put down with Anglo-American support and the country remained aligned with the West.[338]\n\nPost-war division of the world was formalised by two international military alliances, the United States-led NATO and the Soviet-led Warsaw Pact.[339] The long period of political tensions and military competition between them—the Cold War—would be accompanied by an unprecedented arms race and number of proxy wars throughout the world.[340]\n\nIn Asia, the United States led the occupation of Japan and administered Japan's former islands in the Western Pacific, while the Soviets annexed South Sakhalin and the Kuril Islands.[341] Korea, formerly under Japanese colonial rule, was divided and occupied by the Soviet Union in the North and the United States in the South between 1945 and 1948. Separate republics emerged on both sides of the 38th parallel in 1948, each claiming to be the legitimate government for all of Korea, which led ultimately to the Korean War.[342]\n\nIn China, nationalist and communist forces resumed the civil war in June 1946. Communist forces were victorious and established the People's Republic of China on the mainland, while nationalist forces retreated to Taiwan in 1949.[343] In the Middle East, the Arab rejection of the United Nations Partition Plan for Palestine and the creation of Israel marked the escalation of the Arab–Israeli conflict. While European powers attempted to retain some or all of their colonial empires, their losses of prestige and resources during the war rendered this unsuccessful, leading to decolonisation.[344][345]\n\nThe global economy suffered heavily from the war, although participating nations were affected differently. The United States emerged much richer than any other nation, leading to a baby boom, and by 1950 its gross domestic product per person was much higher than that of any of the other powers, and it dominated the world economy.[346] The Allied occupational authorities pursued a policy of industrial disarmament in Western Germany from 1945 to 1948.[347] Due to international trade interdependencies, this policy led to an economic stagnation in Europe and delayed European recovery from the war for several years.[348][349]\n\nAt the Bretton Woods Conference in July 1944, the Allied nations drew up an economic framework for the post-war world. The agreement created the International Monetary Fund (IMF) and the International Bank for Reconstruction and Development (IBRD), which later became part of the World Bank Group. The Bretton Woods system lasted until 1973.[350] Recovery began with the mid-1948 currency reform in Western Germany, and was sped up by the liberalisation of European economic policy that the U.S. Marshall Plan economic aid (1948–1951) both directly and indirectly caused.[351][352] The post-1948 West German recovery has been called the German economic miracle.[353] Italy also experienced an economic boom[354] and the French economy rebounded.[355] By contrast, the United Kingdom was in a state of economic ruin,[356] and although receiving a quarter of the total Marshall Plan assistance, more than any other European country,[357] it continued in relative economic decline for decades.[358] The Soviet Union, despite enormous human and material losses, also experienced rapid increase in production in the immediate post-war era,[359] having seized and transferred most of Germany's industrial plants and exacted war reparations from its satellite states.[c][360] Japan recovered much later.[361] China returned to its pre-war industrial production by 1952.[362]\n\nEstimates for the total number of casualties in the war vary, because many deaths went unrecorded.[363] Most suggest that some 60 million people died in the war, including about 20 million military personnel and 40 million civilians.[364][365][366]\n\nThe Soviet Union alone lost around 27 million people during the war,[367] including 8.7 million military and 19 million civilian deaths.[368] A quarter of the total people in the Soviet Union were wounded or killed.[369] Germany sustained 5.3 million military losses, mostly on the Eastern Front and during the final battles in Germany.[370]\n\nAn estimated 11[371] to 17 million[372] civilians died as a direct or as an indirect result of Hitler's racist policies, including mass killing of around 6 million Jews, along with Roma, homosexuals, at least 1.9 million ethnic Poles[373][374] and millions of other Slavs (including Russians, Ukrainians and Belarusians), and other ethnic and minority groups.[375][372] Between 1941 and 1945, more than 200,000 ethnic Serbs, along with Roma and Jews, were persecuted and murdered by the Axis-aligned Croatian Ustaše in Yugoslavia.[376] Concurrently, Muslims and Croats were persecuted and killed by Serb nationalist Chetniks,[377] with an estimated 50,000–68,000 victims (of which 41,000 were civilians).[378] Also, more than 100,000 Poles were massacred by the Ukrainian Insurgent Army in the Volhynia massacres, between 1943 and 1945.[379] At the same time, about 10,000–15,000 Ukrainians were killed by the Polish Home Army and other Polish units, in reprisal attacks.[380]\n\nIn Asia and the Pacific, the number of people killed by Japanese troops remains contested. According to R.J. Rummel, the Japanese killed between 3 million and more than 10 million people, with the most probable case of almost 6,000,000 people.[381] According to the British historian M. R. D. Foot, civilian deaths are between 10 million and 20 million, whereas Chinese military casualties (killed and wounded) are estimated to be over five million.[382] Other estimates say that up to 30 million people, most of them civilians, were killed.[383][384] The most infamous Japanese atrocity was the Nanjing Massacre, in which fifty to three hundred thousand Chinese civilians were raped and murdered.[385] Mitsuyoshi Himeta reported that 2.7 million casualties occurred during the Three Alls policy. General Yasuji Okamura implemented the policy in Hebei and Shandong.[386]\n\nAxis forces employed biological and chemical weapons. The Imperial Japanese Army used a variety of such weapons during its invasion and occupation of China (see Unit 731)[387][388] and in early conflicts against the Soviets.[389] Both the Germans and the Japanese tested such weapons against civilians,[390] and sometimes on prisoners of war.[391]\n\nThe Soviet Union was responsible for the Katyn massacre of 22,000 Polish officers,[392] and the imprisonment or execution of hundreds of thousands of political prisoners by the NKVD secret police, along with mass civilian deportations to Siberia, in the Baltic states and eastern Poland annexed by the Red Army.[393] Soviet soldiers committed mass rapes in occupied territories, especially in Germany.[394][395] The exact number of German women and girls raped by Soviet troops during the war and occupation is uncertain, but historians estimate their numbers are likely in the hundreds of thousands, and possibly as many as two million,[396] while figures for women raped by German soldiers in the Soviet Union go as far as ten million.[397][398]\n\nThe mass bombing of cities in Europe and Asia has often been called a war crime, although no positive or specific customary international humanitarian law with respect to aerial warfare existed before or during World War II.[399] The USAAF bombed a total of 67 Japanese cities, killing 393,000 civilians, including the atomic bombings of Hiroshima and Nagasaki, and destroying 65% of built-up areas.[400]\n\nNazi Germany, under the dictatorship of Adolf Hitler, was responsible for murdering about 6 million Jews in what is now known as the Holocaust. They also murdered an additional 4 million others who were deemed \"unworthy of life\" (including the disabled and mentally ill, Soviet prisoners of war, Romani, homosexuals, Freemasons, and Jehovah's Witnesses) as part of a program of deliberate extermination, in effect becoming a \"genocidal state\".[401] Soviet POWs were kept in especially unbearable conditions, and 3.6 million Soviet POWs out of 5.7 million died in Nazi camps during the war.[402][403] In addition to concentration camps, death camps were created in Nazi Germany to exterminate people on an industrial scale. Nazi Germany extensively used forced labourers; about 12 million Europeans from German-occupied countries were abducted and used as a slave work force in German industry, agriculture and war economy.[404]\n\nThe Soviet Gulag became a de facto system of deadly camps during 1942–43, when wartime privation and hunger caused numerous deaths of inmates,[406] including foreign citizens of Poland and other countries occupied in 1939–40 by the Soviet Union, as well as Axis POWs.[407] By the end of the war, most Soviet POWs liberated from Nazi camps and many repatriated civilians were detained in special filtration camps where they were subjected to NKVD evaluation, and 226,127 were sent to the Gulag as real or perceived Nazi collaborators.[408]\n\nJapanese prisoner-of-war camps, many of which were used as labour camps, also had high death rates. The International Military Tribunal for the Far East found the death rate of Western prisoners was 27 percent (for American POWs, 37 percent),[409] seven times that of POWs under the Germans and Italians.[410] While 37,583 prisoners from the UK, 28,500 from the Netherlands, and 14,473 from the United States were released after the surrender of Japan, the number of Chinese released was only 56.[411]\n\nAt least five million Chinese civilians from northern China and Manchukuo were enslaved between 1935 and 1941 by the East Asia Development Board, or Kōain, for work in mines and war industries. After 1942, the number reached 10 million.[412] In Java, between 4 and 10 million rōmusha (Japanese: \"manual labourers\"), were forced to work by the Japanese military. About 270,000 of these Javanese labourers were sent to other Japanese-held areas in Southeast Asia, and only 52,000 were repatriated to Java.[413]\n\nIn Europe, occupation came under two forms. In Western, Northern, and Central Europe (France, Norway, Denmark, the Low Countries, and the annexed portions of Czechoslovakia) Germany established economic policies through which it collected roughly 69.5 billion reichsmarks (27.8 billion U.S. dollars) by the end of the war; this figure does not include the plunder of industrial products, military equipment, raw materials and other goods.[414] Thus, the income from occupied nations was over 40 percent of the income Germany collected from taxation, a figure which increased to nearly 40 percent of total German income as the war went on.[415]\n\nIn the East, the intended gains of Lebensraum were never attained as fluctuating front-lines and Soviet scorched earth policies denied resources to the German invaders.[416] Unlike in the West, the Nazi racial policy encouraged extreme brutality against what it considered to be the \"inferior people\" of Slavic descent; most German advances were thus followed by mass atrocities and war crimes.[417] The Nazis killed an estimated 2.77 million ethnic Poles during the war in addition to Polish-Jewish victims of the Holocaust.[418][better source needed] Although resistance groups formed in most occupied territories, they did not significantly hamper German operations in either the East[419] or the West[420] until late 1943.\n\nIn Asia, Japan termed nations under its occupation as being part of the Greater East Asia Co-Prosperity Sphere, essentially a Japanese hegemony which it claimed was for purposes of liberating colonised peoples.[421] Although Japanese forces were sometimes welcomed as liberators from European domination, Japanese war crimes frequently turned local public opinion against them.[422] During Japan's initial conquest, it captured 4,000,000 barrels (640,000 m3) of oil (~550,000 tonnes) left behind by retreating Allied forces; and by 1943, was able to get production in the Dutch East Indies up to 50 million barrels (7,900,000 m3) of oil (~6.8 million tonnes), 76 percent of its 1940 output rate.[422]\n\nIn the 1930s Britain and the United States of America together controlled almost 75% of world mineral output—essential for projecting military power.[423]\n\nIn Europe, before the outbreak of the war, the Allies had significant advantages in both population and economics. In 1938, the Western Allies (United Kingdom, France, Poland and the British Dominions) had a 30 percent larger population and a 30 percent higher gross domestic product than the European Axis powers (Germany and Italy); including colonies, the Allies had more than a 5:1 advantage in population and a nearly 2:1 advantage in GDP.[424] In Asia at the same time, China had roughly six times the population of Japan but only an 89 percent higher GDP; this reduces to three times the population and only a 38 percent higher GDP if Japanese colonies are included.[424]\n\nThe United States produced about two-thirds of all munitions used by the Allies in World War II, including warships, transports, warplanes, artillery, tanks, trucks, and ammunition.[425] Although the Allies' economic and population advantages were largely mitigated during the initial rapid blitzkrieg attacks of Germany and Japan, they became the decisive factor by 1942, after the United States and Soviet Union joined the Allies and the war evolved into one of attrition.[426] While the Allies' ability to out-produce the Axis was partly due to more access to natural resources, other factors, such as Germany and Japan's reluctance to employ women in the labour force,[427] Allied strategic bombing,[428] and Germany's late shift to a war economy[429] contributed significantly. Additionally, neither Germany nor Japan planned to fight a protracted war, and had not equipped themselves to do so.[430] To improve their production, Germany and Japan used millions of slave labourers;[431] Germany enslaved about 12 million people, mostly from Eastern Europe,[404] while Japan used more than 18 million people in Far East Asia.[412][413]\n\nAircraft were used for reconnaissance, as fighters, bombers, and ground-support, and each role developed considerably. Innovations included airlift (the capability to quickly move limited high-priority supplies, equipment, and personnel);[432] and strategic bombing (the bombing of enemy industrial and population centres to destroy the enemy's ability to wage war).[433] Anti-aircraft weaponry also advanced, including defences such as radar and surface-to-air artillery, in particular the introduction of the proximity fuze. The use of the jet aircraft was pioneered and led to jets becoming standard in air forces worldwide.[434]\n\nAdvances were made in nearly every aspect of naval warfare, most notably with aircraft carriers and submarines. Although aeronautical warfare had relatively little success at the start of the war, actions at Taranto, Pearl Harbor, and the Coral Sea established the carrier as the dominant capital ship (in place of the battleship).[435][436][437] In the Atlantic, escort carriers became a vital part of Allied convoys, increasing the effective protection radius and helping to close the Mid-Atlantic gap.[438] Carriers were also more economical than battleships due to the relatively low cost of aircraft[439] and because they are not required to be as heavily armoured.[440] Submarines, which had proved to be an effective weapon during the First World War,[441] were expected by all combatants to be important in the second. The British focused development on anti-submarine weaponry and tactics, such as sonar and convoys, while Germany focused on improving its offensive capability, with designs such as the Type VII submarine and wolfpack tactics.[442][better source needed] Gradually, improving Allied technologies such as the Leigh Light, Hedgehog, Squid, and homing torpedoes proved effective against German submarines.[443]\n\nLand warfare changed from the static frontlines of trench warfare of World War I, which had relied on improved artillery that outmatched the speed of both infantry and cavalry, to increased mobility and combined arms. The tank, which had been used predominantly for infantry support in the First World War, had evolved into the primary weapon.[444] In the late 1930s, tank design was considerably more advanced than it had been during World War I,[445] and advances continued throughout the war with increases in speed, armour and firepower.[446][447] At the start of the war, most commanders thought enemy tanks should be met by tanks with superior specifications.[448] This idea was challenged by the poor performance of the relatively light early tank guns against armour, and German doctrine of avoiding tank-versus-tank combat. This, along with Germany's use of combined arms, were among the key elements of their highly successful blitzkrieg tactics across Poland and France.[444] Many means of destroying tanks, including indirect artillery, anti-tank guns (both towed and self-propelled), mines, short-ranged infantry antitank weapons, and other tanks were used.[448] Even with large-scale mechanisation, infantry remained the backbone of all forces,[449] and throughout the war, most infantry were equipped similarly to World War I.[450] The portable machine gun spread, a notable example being the German MG 34, and various submachine guns which were suited to close combat in urban and jungle settings.[450] The assault rifle, a late war development incorporating many features of the rifle and submachine gun, became the standard post-war infantry weapon for most armed forces.[451]\n\nMost major belligerents attempted to solve the problems of complexity and security involved in using large codebooks for cryptography by designing ciphering machines, the most well-known being the German Enigma machine.[452] Development of SIGINT (signals intelligence) and cryptanalysis enabled the countering process of decryption. Notable examples were the Allied decryption of Japanese naval codes[453] and British Ultra, a pioneering method for decoding Enigma that benefited from information given to the United Kingdom by the Polish Cipher Bureau, which had been decoding early versions of Enigma before the war.[454] Another component of military intelligence was deception, which the Allies used to great effect in operations such as Mincemeat and Bodyguard.[453][455]\n\nOther technological and engineering feats achieved during, or as a result of, the war include the world's first programmable computers (Z3, Colossus, and ENIAC), guided missiles and modern rockets, the Manhattan Project's development of nuclear weapons, operations research, the development of artificial harbours, and oil pipelines under the English Channel.[456] Penicillin was first developed, mass-produced, and used during the war.[457]\n"
    },
    {
        "title": "Ancient Greece",
        "url": "https://en.wikipedia.org/wiki/Ancient_Greece",
        "content": "\n\nAncient Greece (Ancient Greek: Ἑλλάς, romanized: Hellás) was a northeastern Mediterranean civilisation, existing from the Greek Dark Ages of the 12th–9th centuries BC to the end of classical antiquity (c. 600 AD), that comprised a loose collection of culturally and linguistically related city-states and communities. Prior to the Roman period, most of these regions were officially unified only once under the Kingdom of Macedon from 338 to 323 BC.[a] In Western history, the era of classical antiquity was immediately followed by the Early Middle Ages and the Byzantine period.[1]\n\nThree centuries after the decline of Mycenaean Greece during the Bronze Age collapse, Greek urban poleis began to form in the 8th century BC, ushering in the Archaic period and the colonisation of the Mediterranean Basin. This was followed by the age of Classical Greece, from the Greco-Persian Wars to the death of Alexander the Great in 323 BC, and which included the Golden Age of Athens and the Peloponnesian War. The unification of Greece by Macedon under Philip II and subsequent conquest of the Achaemenid Empire by Alexander the Great spread Hellenistic civilisation across the Middle East. The Hellenistic period is considered to have ended in 30 BC, when the last Hellenistic kingdom, Ptolemaic Egypt, was annexed by the Roman Republic.\n\nClassical Greek culture, especially philosophy, had a powerful influence on ancient Rome, which carried a version of it throughout the Mediterranean and much of Europe. For this reason, Classical Greece is generally considered the cradle of Western civilisation, the seminal culture from which the modern West derives many of its founding archetypes and ideas in politics, philosophy, science, and art.[2][3][4]\n\nClassical antiquity in the Mediterranean region is commonly considered to have begun in the 8th century BC[5] (around the time of the earliest recorded poetry of Homer) and ended in the 6th century AD.\n\nClassical antiquity in Greece was preceded by the Greek Dark Ages (c. 1200 – c. 800 BC), archaeologically characterised by the protogeometric and geometric styles of designs on pottery. Following the Dark Ages was the Archaic period, beginning around the 8th century BC, which saw early developments in Greek culture and society leading to the Classical period[6] from the Persian invasion of Greece in 480 BC until the death of Alexander the Great in 323 BC.[7] The Classical period is characterised by a \"classical\" style, i.e. one which was considered exemplary by later observers, most famously in the Parthenon of Athens. Politically, the Classical period was dominated by Athens and the Delian League during the 5th century, but displaced by Spartan hegemony during the early 4th century BC, before power shifted to Thebes and the Boeotian League and finally to the League of Corinth led by Macedon. This period was shaped by the Greco-Persian Wars, the Peloponnesian War, and the Rise of Macedon.\n\nFollowing the Classical period was the Hellenistic period (323–146 BC), during which Greek culture and power expanded into the Near and Middle East from the death of Alexander until the Roman conquest. Roman Greece is usually counted from the Roman victory over the Corinthians at the Battle of Corinth in 146 BC to the establishment of Byzantium by Constantine as the capital of the Roman Empire in 330 AD. Finally, Late Antiquity refers to the period of Christianisation during the later 4th to early 6th centuries AD, consummated by the closure of the Academy of Athens by Justinian I in 529.[8]\n\nThe historical period of ancient Greece is unique in world history as the first period attested directly in comprehensive, narrative historiography, while earlier ancient history or protohistory is known from much more fragmentary documents such as annals, king lists, and pragmatic epigraphy.\n\nHerodotus is widely known as the \"father of history\": his Histories are eponymous of the entire field. Written between the 450s and 420s BC, Herodotus' work reaches about a century into the past, discussing 6th century BC historical figures such as Darius I of Persia, Cambyses II and Psamtik III, and alluding to some 8th century BC persons such as Candaules. The accuracy of Herodotus' works is debated.[9][10][11][12][13]\n\nHerodotus was succeeded by authors such as Thucydides, Xenophon, Demosthenes, Plato and Aristotle. Most were either Athenian or pro-Athenian, which is why far more is known about the history and politics of Athens than of many other cities.\nTheir scope is further limited by a focus on political, military and diplomatic history, ignoring economic and social history.[14]\n\nThe archaic period, lasting from approximately 800 to 500 BC, saw the culmination of political and social developments which had begun in the Greek dark age, with the polis (city-state) becoming the most important unit of political organisation in Greece.[15]  The absence of powerful states in Greece after the collapse of Mycenaean power, and the geography of Greece, where many settlements were separated from their neighbours by mountainous terrain, encouraged the development of small independent city-states.[16]  Several Greek states saw tyrants rise to power in this period, most famously at Corinth from 657 BC.[17]  The period also saw the founding of Greek colonies around the Mediterranean, with Euboean settlements at Al-Mina in the east as early as 800 BC, and Ischia in the west by 775.[18]  Increasing contact with non-Greek peoples in this period, especially in the Near East, inspired developments in art and architecture, the adoption of coinage, and the development of the Greek alphabet.[19]\n\nAthens developed its democratic system over the course of the archaic period.  Already in the seventh century, the right of all citizen men to attend the assembly appears to have been established.[20]  After a failed coup led by Cylon of Athens around 636 BC, Draco was appointed to establish a code of laws in 621.  This failed to reduce the political tension between the poor and the elites, and in 594 Solon was given the authority to enact another set of reforms, which attempted to balance the power of the rich and the poor.[21]  In the middle of the sixth century, Pisistratus established himself as a tyrant, and after his death in 527 his son Hippias inherited his position; by the end of the sixth century he had been overthrown and Cleisthenes carried out further democratising reforms.[22]\n\nIn Sparta, a political system with two kings, a council of elders, and five ephors developed over the course of the eighth and seventh century.  According to Spartan tradition, this constitution was established by the legendary lawgiver Lycurgus.[23]  Over the course of the first and second Messenian wars, Sparta subjugated the neighbouring region of Messenia, enserfing the population.[24]\n\nIn the sixth century, Greek city-states began to develop formal relationships with one another, where previously individual rulers had relied on personal relationships with the elites of other cities.[25]  Towards the end of the archaic period, Sparta began to build a series of alliances, the Peloponnesian League, with cities including Corinth, Elis, and Megara,[26] isolating Messenia and reinforcing Sparta's position against Argos, the other major power in the Peloponnese.[27]  Other alliances in the sixth century included those between Elis and Heraea in the Peloponnese; and between the Greek colony Sybaris in southern Italy, its allies, and the Serdaioi.[28]\n\nIn 499 BC, the Ionian city states under Persian rule rebelled against their Persian-supported tyrant rulers.[29] Supported by troops sent from Athens and Eretria, they advanced as far as Sardis and burnt the city before being driven back by a Persian counterattack.[30] The revolt continued until 494, when the rebelling Ionians were defeated.[30] Darius did not forget that Athens had assisted the Ionian revolt, and in 490 he assembled an armada to retaliate.[31] Though heavily outnumbered, the Athenians—supported by their Plataean allies—defeated the Persian hordes at the Battle of Marathon, and the Persian fleet turned tail.[32]\n\nTen years later, a second invasion was launched by Darius' son Xerxes.[33] The city-states of northern and central Greece submitted to the Persian forces without resistance, but a coalition of 31 Greek city states, including Athens and Sparta, determined to resist the Persian invaders.[33] At the same time, Greek Sicily was invaded by a Carthaginian force.[33] In 480 BC, the first major battle of the invasion was fought at Thermopylae, where a small rearguard of Greeks, led by three hundred Spartans, held a crucial pass guarding the heart of Greece for several days; at the same time Gelon, tyrant of Syracuse, defeated the Carthaginian invasion at the Battle of Himera.[34]\n\nThe Persians were decisively defeated at sea by a primarily Athenian naval force at the Battle of Salamis, and on land in 479 BC at the Battle of Plataea.[35] The alliance against Persia continued, initially led by the Spartan Pausanias but from 477 by Athens,[36] and by 460 Persia had been driven out of the Aegean.[37] During this long campaign, the Delian League gradually transformed from a defensive alliance of Greek states into an Athenian empire, as Athens' growing naval power intimidated the other league states.[38] Athens ended its campaigns against Persia in 450, after a disastrous defeat in Egypt in 454, and the death of Cimon in action against the Persians on Cyprus in 450.[39]\n\nAs the Athenian fight against the Persian empire waned, conflict grew between Athens and Sparta. Suspicious of the increasing Athenian power funded by the Delian League, Sparta offered aid to reluctant members of the League to rebel against Athenian domination. These tensions were exacerbated in 462 BC when Athens sent a force to aid Sparta in overcoming a helot revolt, but this aid was rejected by the Spartans.[40] In the 450s, Athens took control of Boeotia, and won victories over Aegina and Corinth.[39] However, Athens failed to win a decisive victory, and in 447 lost Boeotia again.[39] Athens and Sparta signed the Thirty Years' Peace in the winter of 446/5, ending the conflict.[39]\n\nDespite the treaty, Athenian relations with Sparta declined again in the 430s, and in 431 BC the Peloponnesian War began.[41] The first phase of the war saw a series of fruitless annual invasions of Attica by Sparta, while Athens successfully fought the Corinthian empire in northwest Greece and defended its own empire, despite a plague which killed the leading Athenian statesman Pericles.[42] The war turned after Athenian victories led by Cleon at Pylos and Sphakteria,[42] and Sparta sued for peace, but the Athenians rejected the proposal.[43] The Athenian failure to regain control of Boeotia at Delium and Brasidas' successes in northern Greece in 424 improved Sparta's position after Sphakteria.[43] After the deaths of Cleon and Brasidas, the strongest proponents of war on each side, a peace treaty was negotiated in 421 by the Athenian general Nicias.[44]\n\nThe peace did not last, however. In 418 BC allied forces of Athens and Argos were defeated by Sparta at Mantinea.[45] In 415 Athens launched an ambitious naval expedition to dominate Sicily;[46] the expedition ended in disaster at the harbor of Syracuse, with almost the entire army killed, and the ships destroyed.[47] Soon after the Athenian defeat in Syracuse, Athens' Ionian allies began to rebel against the Delian league, while Persia began to once again involve itself in Greek affairs on the Spartan side.[48] Initially the Athenian position continued relatively strong, with important victories at Cyzicus in 410 and Arginusae in 406.[49] However, in 405 the Spartan Lysander defeated Athens in the Battle of Aegospotami, and began to blockade Athens' harbour;[50] driven by hunger, Athens sued for peace, agreeing to surrender their fleet and join the Spartan-led Peloponnesian League.[51] Following the Athenian surrender, Sparta installed an oligarchic regime, the Thirty Tyrants, in Athens,[50] one of a number of Spartan-backed oligarchies which rose to power after the Peloponnesian war.[52] Spartan predominance did not last: after only a year, the Thirty had been overthrown.[53]\n\nThe first half of the fourth century saw the major Greek states attempt to dominate the mainland; none were successful, and their resulting weakness led to a power vacuum which was eventually filled by Macedon under Philip II and then Alexander the Great.[54] In the immediate aftermath of the Peloponnesian war, Sparta attempted to extend their own power, leading Argos, Athens, Corinth, and Thebes to join against them.[55] Aiming to prevent any single Greek state gaining the dominance that would allow it to challenge Persia, the Persian king initially joined the alliance against Sparta, before imposing the Peace of Antalcidas (\"King's Peace\") which restored Persia's control over the Anatolian Greeks.[56]\n\nBy 371 BC, Thebes was in the ascendancy, defeating Sparta at the Battle of Leuctra, killing the Spartan king Cleombrotus I, and invading Laconia. Further Theban successes against Sparta in 369 led to Messenia gaining independence; Sparta never recovered from the loss of Messenia's fertile land and the helot workforce it provided.[57] The rising power of Thebes led Sparta and Athens to join forces; in 362 they were defeated by Thebes at the Battle of Mantinea. In the aftermath of Mantinea, none of the major Greek states were able to dominate. Though Thebes had won the battle, their general Epaminondas was killed, and they spent the following decades embroiled in wars with their neighbours; Athens, meanwhile, saw its second naval alliance, formed in 377, collapse in the mid-350s.[58]\n\nThe power vacuum in Greece after the Battle of Mantinea was filled by Macedon, under Philip II. In 338 BC, he defeated a Greek alliance at the Battle of Chaeronea, and subsequently formed the League of Corinth. Philip planned to lead the League to invade Persia, but was murdered in 336 BC. His son Alexander the Great was left to fulfil his father's ambitions.[59] After campaigns against Macedon's western and northern enemies, and those Greek states that had broken from the League of Corinth following the death of Philip, Alexander began his campaign against Persia in 334 BC.[60] He conquered Persia, defeating Darius III at the Battle of Issus in 333 BC, and after the Battle of Gaugamela in 331 BC proclaimed himself king of Asia.[61] From 329 BC he led expeditions to Bactria and then India;[62] further plans to invade Arabia and North Africa were halted by his death in 323 BC.[63]\n\nThe period from the death of Alexander the Great in 323 BC until the death of Cleopatra, the last Macedonian ruler of Egypt, is known as the Hellenistic period. In the early part of this period, a new form of kingship developed based on Macedonian and Near Eastern traditions. The first Hellenistic kings were previously Alexander's generals, and took power in the period following his death, though they were not part of existing royal lineages and lacked historic claims to the territories they controlled.[64] The most important of these rulers in the decades after Alexander's death were Antigonus I and his son Demetrius in Macedonia and the rest of Greece, Ptolemy in Egypt, and Seleucus I in Syria and the former Persian empire;[65] smaller Hellenistic kingdoms included the Attalids in Anatolia and the Greco-Bactrian kingdom.[66]\n\nIn the early part of the Hellenistic period, the exact borders of the Hellenistic kingdoms were not settled. Antigonus attempted to expand his territory by attacking the other successor kingdoms until they joined against him, and he was killed at the Battle of Ipsus in 301 BC.[67] His son Demetrius spent many years in Seleucid captivity, and his son, Antigonus II, only reclaimed the Macedonian throne around 276.[67] Meanwhile, the Seleucid kingdom gave up territory in the east to the Indian king Chandragupta Maurya in exchange for war elephants, and later lost large parts of Persia to the Parthian Empire.[67] By the mid-third century, the kingdoms of Alexander's successors was mostly stable, though there continued to be disputes over border areas.[66]\n\nThe great capitals of Hellenistic culture were Alexandria in the Ptolemaic Kingdom[68][69] and Antioch in the Seleucid Empire.[70][71]\n\nThe conquests of Alexander had numerous consequences for the Greek city-states. It greatly widened the horizons of the Greeks and led to a steady emigration of the young and ambitious to the new Greek empires in the east.[72] Many Greeks migrated to Alexandria, Antioch and the many other new Hellenistic cities founded in Alexander's wake, as far away as present-day Afghanistan and Pakistan, where the Greco-Bactrian Kingdom and the Indo-Greek Kingdom survived until the end of the first century BC.\n\nThe city-states within Greece formed themselves into two leagues; the Achaean League (including Corinth and Argos)[73][74] and the Aetolian League (including Sparta and Athens).[citation needed] For much of the period until the Roman conquest, these leagues were at war, often participating in the conflicts between the Diadochi (the successor states to Alexander's empire).[citation needed]\n\nThe Antigonid Kingdom became involved in a war with the Roman Republic in the late 3rd century. Although the First Macedonian War was inconclusive, the Romans, in typical fashion, continued to fight Macedon until it was completely absorbed into the Roman Republic (by 149 BC). In the east, the unwieldy Seleucid Empire gradually disintegrated, although a rump survived until 64 BC, whilst the Ptolemaic Kingdom continued in Egypt until 30 BC when it too was conquered by the Romans. The Aetolian league grew wary of Roman involvement in Greece, and sided with the Seleucids in the Roman–Seleucid War; when the Romans were victorious, the league was effectively absorbed into the Republic. Although the Achaean league outlasted both the Aetolian league and Macedon, it was also soon defeated and absorbed by the Romans in 146 BC, bringing Greek independence to an end.\n\nThe Greek peninsula came under Roman rule during the 146 BC conquest of Greece after the Battle of Corinth. Macedonia became a Roman province while southern Greece came under the surveillance of Macedonia's prefect; however, some Greek poleis managed to maintain a partial independence and avoid taxation. The Aegean Islands were added to this territory in 133 BC. Athens and other Greek cities revolted in 88 BC, and the peninsula was crushed by the Roman general Sulla. The Roman civil wars devastated the land even further, until Augustus organised the peninsula as the province of Achaea in 27 BC.\n\nGreece was a key eastern province of the Roman Empire, as the Roman culture had long been in fact Greco-Roman. The Greek language served as a lingua franca in the East and in Italy, and many Greek intellectuals such as Galen would perform most of their work in Rome.\n\nThe territory of Greece is mountainous, and as a result, ancient Greece consisted of many smaller regions, each with its own dialect, cultural peculiarities, and identity. Regionalism and regional conflicts were prominent features of ancient Greece. Cities tended to be located in valleys between mountains, or on coastal plains, and dominated a certain area around them.\n\nIn the south lay the Peloponnese, consisting of the regions of Laconia (southeast), Messenia (southwest), Elis (west), Achaia (north), Korinthia (northeast), Argolis (east), and Arcadia (center). These names survive to the present day as regional units of modern Greece, though with somewhat different boundaries. Mainland Greece to the north, nowadays known as Central Greece, consisted of Aetolia and Acarnania in the west, Locris, Doris, and Phocis in the center, while in the east lay Boeotia, Attica, and Megaris. Northeast lay Thessaly, while Epirus lay to the northwest. Epirus stretched from the Ambracian Gulf in the south to the Ceraunian Mountains and the Aoos river in the north, and consisted of Chaonia (north), Molossia (center), and Thesprotia (south). In the northeast corner was Macedonia,[75] originally consisting Lower Macedonia and its regions, such as Elimeia, Pieria, and Orestis. Around the time of Alexander I of Macedon, the Argead kings of Macedon started to expand into Upper Macedonia, lands inhabited by independent Macedonian tribes like the Lyncestae, Orestae and the Elimiotae and to the west, beyond the Axius river, into Eordaia, Bottiaea, Mygdonia, and Almopia, regions settled by Thracian tribes.[76] To the north of Macedonia lay various non-Greek peoples such as the Paeonians due north, the Thracians to the northeast, and the Illyrians, with whom the Macedonians were frequently in conflict, to the northwest. Chalcidice was settled early on by southern Greek colonists and was considered part of the Greek world, while from the late 2nd millennium BC substantial Greek settlement also occurred on the eastern shores of the Aegean, in Anatolia.\n\nDuring the Archaic period, the Greek population grew beyond the capacity of the limited arable land of Greece proper, resulting in the large-scale establishment of colonies elsewhere: according to one estimate, the population of the widening area of Greek settlement increased roughly tenfold from 800 BC to 400 BC, from 800,000 to as many as 7+1⁄2-10 million.[77] This was not simply for trade, but also to found settlements.  These Greek colonies were not, as Roman colonies were, dependent on their mother-city, but were independent city-states in their own right.[78]\n\nGreeks settled outside of Greece in two distinct ways.  The first was in permanent settlements founded by Greeks, which formed as independent poleis.  The second form was in what historians refer to as emporia; trading posts which were occupied by both Greeks and non-Greeks and which were primarily concerned with the manufacture and sale of goods.  Examples of this latter type of settlement are found at Al Mina in the east and Pithekoussai in the west.[79] From about 750 BC the Greeks began 250 years of expansion, settling colonies in all directions. To the east, the Aegean coast of Asia Minor was colonised first, followed by Cyprus and the coasts of Thrace, the Sea of Marmara and south coast of the Black Sea.\n\nEventually, Greek colonisation reached as far northeast as present-day Ukraine and Russia (Taganrog). To the west the coasts of Illyria, Southern Italy (called \"Magna Graecia\") were settled, followed by Southern France, Corsica, and even eastern Spain. Greek colonies were also founded in Egypt and Libya. Modern Syracuse, Naples, Marseille and Istanbul had their beginnings as the Greek colonies Syracusae (Συράκουσαι), Neapolis (Νεάπολις), Massalia (Μασσαλία) and Byzantion (Βυζάντιον). These colonies played an important role in the spread of Greek influence throughout Europe and also aided in the establishment of long-distance trading networks between the Greek city-states, boosting the economy of ancient Greece.\n\nAncient Greece consisted of several hundred relatively independent city-states (poleis). This was a situation unlike that in most other contemporary societies, which were either tribal or kingdoms ruling over relatively large territories. Undoubtedly, the geography of Greece—divided and sub-divided by hills, mountains, and rivers—contributed to the fragmentary nature of ancient Greece. On the one hand, the ancient Greeks had no doubt that they were \"one people\"; they had the same religion, same basic culture, and same language. Furthermore, the Greeks were very aware of their tribal origins; Herodotus was able to extensively categorise the city-states by tribe. Yet, although these higher-level relationships existed, they seem to have rarely had a major role in Greek politics. The independence of the poleis was fiercely defended; unification was something rarely contemplated by the ancient Greeks. Even when, during the second Persian invasion of Greece, a group of city-states allied themselves to defend Greece, the vast majority of poleis remained neutral, and after the Persian defeat, the allies quickly returned to infighting.[81]\n\nThus, the major peculiarities of the ancient Greek political system were its fragmented nature (and that this does not particularly seem to have tribal origin), and the particular focus on urban centers within otherwise tiny states. The peculiarities of the Greek system are further evidenced by the colonies that they set up throughout the Mediterranean, which, though they might count a certain Greek polis as their 'mother' (and remain sympathetic to her), were completely independent of the founding city.\n\nInevitably smaller poleis might be dominated by larger neighbors, but conquest or direct rule by another city-state appears to have been quite rare. Instead the poleis grouped themselves into leagues, membership of which was in a constant state of flux. Later in the Classical period, the leagues would become fewer and larger, be dominated by one city (particularly Athens, Sparta and Thebes); and often poleis would be compelled to join under threat of war (or as part of a peace treaty). Even after Philip II of Macedon conquered the heartlands of ancient Greece, he did not attempt to annex the territory or unify it into a new province, but compelled most of the poleis to join his own Corinthian League.\n\nInitially many Greek city-states seem to have been petty kingdoms; there was often a city official carrying some residual, ceremonial functions of the king (basileus), e.g., the archon basileus in Athens.[82] However, by the Archaic period and the first historical consciousness, most had already become aristocratic oligarchies. It is unclear exactly how this change occurred. For instance, in Athens, the kingship had been reduced to a hereditary, lifelong chief magistracy (archon) by c. 1050 BC; by 753 BC this had become a decennial, elected archonship; and finally by 683 BC an annually elected archonship. Through each stage, more power would have been transferred to the aristocracy as a whole, and away from a single individual.\n\nInevitably, the domination of politics and concomitant aggregation of wealth by small groups of families was apt to cause social unrest in many poleis. In many cities a tyrant (not in the modern sense of repressive autocracies), would at some point seize control and govern according to their own will; often a populist agenda would help sustain them in power. In a system wracked with class conflict, government by a 'strongman' was often the best solution.\n\nAthens fell under a tyranny in the second half of the 6th century BC. When this tyranny was ended, the Athenians founded the world's first democracy as a radical solution to prevent the aristocracy regaining power. A citizens' assembly (the Ecclesia), for the discussion of city policy, had existed since the reforms of Draco in 621 BC; all citizens were permitted to attend after the reforms of Solon (early 6th century), but the poorest citizens could not address the assembly or run for office. With the establishment of the democracy, the assembly became the de jure mechanism of government; all citizens had equal privileges in the assembly. However, non-citizens, such as metics (foreigners living in Athens) or slaves, had no political rights at all.\n\nAfter the rise of democracy in Athens, other city-states founded democracies. However, many retained more traditional forms of government. As so often in other matters, Sparta was a notable exception to the rest of Greece, ruled through the whole period by not one, but two hereditary monarchs. This was a form of diarchy. The Kings of Sparta belonged to the Agiads and the Eurypontids, descendants respectively of Eurysthenes and Procles. Both dynasties' founders were believed to be twin sons of Aristodemus, a Heraclid ruler. However, the powers of these kings were held in check by both a council of elders (the Gerousia) and magistrates specifically appointed to watch over the kings (the Ephors).\n\nOnly free, land-owning, native born men could be citizens entitled to the full protection of the law in a city-state. In most city-states, unlike the situation in Rome, social prominence did not allow special rights. Sometimes families controlled public religious functions, but this ordinarily did not give any extra power in the government. In Athens, the population was divided into four social classes based on wealth. People could change classes if they made more money. In Sparta, all male citizens were called homoioi, meaning \"peers\". However, Spartan kings, who served as the city-state's dual military and religious leaders, came from two families.[83]\n\nWomen in Ancient Greece appear to have primarily performed domestic tasks, managed households, and borne and reared children.  \n\nSlaves had no power or status. Slaves had the right to have a family and own property, subject to their master's goodwill and permission, but they had no political rights. By 600 BC, chattel slavery had spread in Greece. By the 5th century BC, slaves made up one-third of the total population in some city-states. Between 40–80% of the population of Classical Athens were slaves.[84] Slaves outside of Sparta almost never revolted because they were made up of too many nationalities and were too scattered to organize. However, unlike later Western culture, the ancient Greeks did not think in terms of race.[85]\n\nMost families owned slaves as household servants and laborers, and even poor families might have owned a few slaves. Owners were not allowed to beat or kill their slaves. Owners often promised to free slaves in the future to encourage slaves to work hard. Unlike in Rome, freedmen did not become citizens. Instead, they were mixed into the population of metics, which included people from foreign countries or other city-states who were officially allowed to live in the state.\n\nCity-states legally owned slaves. These public slaves had a larger measure of independence than slaves owned by families, living on their own and performing specialised tasks. In Athens, public slaves were trained to look out for counterfeit coinage, while temple slaves acted as servants of the temple's deity and Scythian slaves were employed in Athens as a police force corralling citizens to political functions.\n\nSparta had a special type of slaves called helots. Helots were Messenians enslaved en masse during the Messenian Wars by the state and assigned to families where they were forced to stay. Helots raised food and did household chores so that women could concentrate on raising strong children while men could devote their time to training as hoplites. Their masters treated them harshly, and helots revolted against their masters several times. In 370/69 BC, as a result of Epaminondas' liberation of Messenia from Spartan rule, the helot system there came to an end and the helots won their freedom.[86] However, it did continue to persist in Laconia until the 2nd century BC.\n\nFor most of Greek history, education was private, except in Sparta. During the Hellenistic period, some city-states established public schools. Only wealthy families could afford a teacher. Boys learned how to read, write and quote literature. They also learned to sing and play one musical instrument and were trained as athletes for military service. They studied not for a job but to become an effective citizen. Girls also learned to read, write and do simple arithmetic so they could manage the household. They almost never received education after childhood.[87]\n\nBoys went to school at the age of seven, or went to the barracks, if they lived in Sparta. The three types of teachings were: grammatistes for arithmetic, kitharistes for music and dancing, and Paedotribae for sports.\n\nBoys from wealthy families attending the private school lessons were taken care of by a paidagogos, a household slave selected for this task who accompanied the boy during the day. Classes were held in teachers' private houses and included reading, writing, mathematics, singing, and playing the lyre and flute. When the boy became 12 years old the schooling started to include sports such as wrestling, running, and throwing discus and javelin. In Athens, some older youths attended academy for the finer disciplines such as culture, sciences, music, and the arts. The schooling ended at age 18, followed by military training in the army usually for one or two years.[88]\n\nSome of Athens' greatest such schools included the Lyceum (the so-called Peripatetic school founded by Aristotle of Stageira)[89][90] and the Platonic Academy (founded by Plato of Athens).[91][92] The education system of the wealthy ancient Greeks is also called Paideia.[93][94]\n\nAt its economic height in the 5th and 4th centuries BC, the free citizenry of Classical Greece represented perhaps the most prosperous society in the ancient world, some economic historians considering Greece one of the most advanced pre-industrial economies. In terms of wheat, wages reached an estimated 7–12 kg daily for an unskilled worker in urban Athens, 2–3 times the 3.75 kg of an unskilled rural labourer in Roman Egypt, though Greek farm incomes too were on average lower than those available to urban workers.[95]\n\nWhile slave conditions varied widely, the institution served to sustain the incomes of the free citizenry: an estimate of economic development drawn from the latter (or derived from urban incomes alone) is therefore likely to overstate the true overall level despite widespread evidence for high living standards.\n\nAt least in the Archaic period, the fragmentary nature of ancient Greece, with many competing city-states, increased the frequency of conflict but conversely limited the scale of warfare. Unable to maintain professional armies, the city-states relied on their own citizens to fight. This inevitably reduced the potential duration of campaigns, as citizens would need to return to their own professions (especially in the case of, for example, farmers). Campaigns would therefore often be restricted to summer. When battles occurred, they were usually set piece and intended to be decisive. Casualties were slight compared to later battles, rarely amounting to more than five percent of the losing side, but the slain often included the most prominent citizens and generals who led from the front.\n\nThe scale and scope of warfare in ancient Greece changed dramatically as a result of the Greco-Persian Wars. To fight the enormous armies of the Achaemenid Empire was effectively beyond the capabilities of a single city-state. The eventual triumph of the Greeks was achieved by alliances of city-states (the exact composition changing over time), allowing the pooling of resources and division of labor. Although alliances between city-states occurred before this time, nothing on this scale had been seen before. The rise of Athens and Sparta as pre-eminent powers during this conflict led directly to the Peloponnesian War, which saw further development of the nature of warfare, strategy and tactics. Fought between leagues of cities dominated by Athens and Sparta, the increased manpower and financial resources increased the scale and allowed the diversification of warfare. Set-piece battles during the Peloponnesian war proved indecisive and instead there was increased reliance on attritionary strategies, naval battles and blockades and sieges. These changes greatly increased the number of casualties and the disruption of Greek society.\n\nAthens owned one of the largest war fleets in ancient Greece. It had over 200 triremes each powered by 170 oarsmen who were seated in 3 rows on each side of the ship. The city could afford such a large fleet—it had over 34,000 oarsmen—because it owned a lot of silver mines that were worked by slaves.\n\nAccording to Josiah Ober, Greek city-states faced approximately a one-in-three chance of destruction during the archaic and classical period.[96]\n\nAncient Greek philosophy focused on the role of reason and inquiry. In many ways, it had an important influence on modern philosophy, as well as modern science. Clear unbroken lines of influence lead from ancient Greek and Hellenistic philosophers, to medieval Muslim philosophers and Islamic scientists, to the European Renaissance and Enlightenment, to the secular sciences of the modern day.\n\nNeither reason nor inquiry began with the ancient Greeks. Defining the difference between the Greek quest for knowledge and the quests of the elder civilisations, such as the ancient Egyptians and Babylonians, has long been a topic of study by theorists of civilisation.\n\nThe first known philosophers of Greece were the pre-Socratics, who attempted to provide naturalistic, non-mythical descriptions of the world. They were followed by Socrates, one of the first philosophers based in Athens during its golden age whose ideas, despite being known by second-hand accounts instead of writings of his own, laid the basis of Western philosophy. Socrates' disciple Plato, who wrote The Republic and established a radical difference between ideas and the concrete world, and Plato's disciple Aristotle, who wrote extensively about nature and ethics, are also immensely influential in Western philosophy to this day. The later Hellenistic philosophy, also originating in Greece, is defined by names such as Antisthenes (cynicism), Zeno of Citium (stoicism) and Plotinus (Neoplatonism).\n\nThe earliest Greek literature was poetry and was composed for performance rather than private consumption.[97] The earliest Greek poet known is Homer, although he was certainly part of an existing tradition of oral poetry.[98] Homer's poetry, though it was developed around the same time that the Greeks developed writing, would have been composed orally; the first poet to certainly compose their work in writing was Archilochus, a lyric poet from the mid-seventh century BC.[99] Tragedy developed around the end of the archaic period, taking elements from across the pre-existing genres of late archaic poetry.[100] Towards the beginning of the classical period, comedy began to develop—the earliest date associated with the genre is 486 BC, when a competition for comedy became an official event at the City Dionysia in Athens, though the first preserved ancient comedy is Aristophanes' Acharnians, produced in 425.[101]\n\nLike poetry, Greek prose had its origins in the archaic period, and the earliest writers of Greek philosophy, history, and medical literature all date to the sixth century BC.[102] Prose first emerged as the writing style adopted by the presocratic philosophers Anaximander and Anaximenes—though Thales of Miletus, considered the first Greek philosopher, apparently wrote nothing.[103] Prose as a genre reached maturity in the classical era,[102] and the major Greek prose genres—philosophy, history, rhetoric, and dialogue—developed in this period.[104]\n\nThe Hellenistic period saw the literary centre of the Greek world move from Athens, where it had been in the classical period, to Alexandria. At the same time, other Hellenistic kings such as the Antigonids and the Attalids were patrons of scholarship and literature, turning Pella and Pergamon respectively into cultural centres.[105] It was thanks to this cultural patronage by Hellenistic kings, and especially the Museum at Alexandria, that so much ancient Greek literature has survived.[106] The Library of Alexandria, part of the Museum, had the previously unenvisaged aim of collecting together copies of all known authors in Greek. Almost all of the surviving non-technical Hellenistic literature is poetry,[106] and Hellenistic poetry tended to be highly intellectual,[107] blending different genres and traditions, and avoiding linear narratives.[108] The Hellenistic period also saw a shift in the ways literature was consumed—while in the archaic and classical periods literature had typically been experienced in public performance, in the Hellenistic period it was more commonly read privately.[109] At the same time, Hellenistic poets began to write for private, rather than public, consumption.[110]\n\nWith Octavian's victory at Actium in 31 BC, Rome began to become a major centre of Greek literature, as important Greek authors such as Strabo and Dionysius of Halicarnassus came to Rome.[111] The period of greatest innovation in Greek literature under Rome was the \"long second century\" from approximately 80 AD to around 230 AD.[112] This innovation was especially marked in prose, with the development of the novel and a revival of prominence for display oratory both dating to this period.[112]\n\nIn Ancient Greek society, music was ever-present and considered a fundamental component of civilisation.[113]  It was an important part of public religious worship,[114] private ceremonies such as weddings and funerals,[115] and household entertainment.[116]  Men sang and played music at the symposium;[117] both men and women sang at work; and children's games involved song and dance.[118]\n\nAncient Greek music was primarily vocal, sung either by a solo singer or a chorus, and usually accompanied by an instrument; purely instrumental music was less common.[119]  The Greeks used stringed instruments, including lyres, harps, and lutes;[120] and wind instruments, of which the most important was the aulos, a reed instrument.[121]  Percussion instruments played a relatively unimportant role supporting stringed and wind instruments, and were used in certain religious cults.[122]\n\nAncient Greek mathematics contributed many important developments to the field of mathematics, including the basic rules of geometry, the idea of formal mathematical proof, and discoveries in number theory, mathematical analysis, applied mathematics, and approached close to establishing integral calculus. The discoveries of several Greek mathematicians, including Pythagoras, Euclid, and Archimedes, are still used in mathematical teaching today.\n\nThe Greeks developed astronomy, which they treated as a branch of mathematics, to a highly sophisticated level. The first geometrical, three-dimensional models to explain the apparent motion of the planets were developed in the 4th century BC by Eudoxus of Cnidus and Callippus of Cyzicus. Their younger contemporary Heraclides Ponticus proposed that the Earth rotates around its axis. In the 3rd century BC, Aristarchus of Samos was the first to suggest a heliocentric system. Archimedes in his treatise The Sand Reckoner revives Aristarchus' hypothesis that \"the fixed stars and the Sun remain unmoved, while the Earth revolves about the Sun on the circumference of a circle\". Otherwise, only fragmentary descriptions of Aristarchus' idea survive.[123] Eratosthenes, using the angles of shadows created at widely separated regions, estimated the circumference of the Earth with great accuracy.[124] In the 2nd century BC Hipparchus of Nicea made a number of contributions, including the first measurement of precession and the compilation of the first star catalog in which he proposed the modern system of apparent magnitudes.\n\nThe Antikythera mechanism, a device for calculating the movements of planets, dates from about 80 BC and was the first ancestor of the astronomical computer. It was discovered in an ancient shipwreck off the Greek island of Antikythera, between Kythera and Crete. The device became famous for its use of a differential gear, previously believed to have been invented in the 16th century, and the miniaturisation and complexity of its parts, comparable to a clock made in the 18th century. The original mechanism is displayed in the Bronze collection of the National Archaeological Museum of Athens, accompanied by a replica.\n\nThe ancient Greeks also made important discoveries in the medical field. Hippocrates was a physician of the Classical period, and is considered one of the most outstanding figures in the history of medicine. He is referred to as the \"father of medicine\"[125][126] in recognition of his lasting contributions to the field as the founder of the Hippocratic school of medicine. This intellectual school revolutionized medicine in ancient Greece, establishing it as a discipline distinct from other fields that it had traditionally been associated with (notably theurgy and philosophy), thus making medicine a profession.[127][128]\n\nThe art of ancient Greece has exercised an enormous influence on the culture of many countries from ancient times to the present day, particularly in the areas of sculpture and architecture. In the West, the art of the Roman Empire was largely derived from Greek models. In the East, Alexander the Great's conquests initiated several centuries of exchange between Greek, Central Asian and Indian cultures, resulting in Greco-Buddhist art, with ramifications as far as Japan. Following the Renaissance in Europe, the humanist aesthetic and the high technical standards of Greek art inspired generations of European artists. Well into the 19th century, the classical tradition derived from Greece dominated the art of the Western world.\n\nReligion was a central part of ancient Greek life.[129] Though the Greeks of different cities and tribes worshipped similar gods, religious practices were not uniform and the gods were thought of differently in different places. The Greeks were polytheistic, worshipping many gods, but as early as the sixth century BC a pantheon of twelve Olympians began to develop.[130] Greek religion was influenced by the practices of the Greeks' near eastern neighbours at least as early as the archaic period, and by the Hellenistic period this influence was seen in both directions.[131]\n\nThe most important religious act in ancient Greece was animal sacrifice, most commonly of sheep and goats.[132] Sacrifice was accompanied by public prayer,[133] and prayer and hymns were themselves a major part of ancient Greek religious life.[134]\n\nThe civilisation of ancient Greece has been immensely influential on language, politics, educational systems, philosophy, science, and the arts. It became the Leitkultur of the Roman Empire to the point of marginalising native Italic traditions. As Horace put it,\n\nVia the Roman Empire, Greek culture came to be foundational to Western culture in general.\nThe Byzantine Empire inherited Classical Greek-Hellenistic culture directly, without Latin intermediation, and the preservation of Classical Greek learning in medieval Byzantine tradition further exerted a strong influence on the Slavs and later on the Islamic Golden Age and the Western European Renaissance. A modern revival of Classical Greek learning took place in the Neoclassicism movement in 18th- and 19th-century Europe and the Americas.\n"
    },
    {
        "title": "Renaissance",
        "url": "https://en.wikipedia.org/wiki/Renaissance",
        "content": "\n\nThe Renaissance (UK: /rɪˈneɪsəns/ rin-AY-sənss, US: /ˈrɛnəsɑːns/ ⓘ REN-ə-sahnss)[1][2][a] is a period of history and a European cultural movement covering the 15th and 16th centuries. It marked the transition from the Middle Ages to modernity and was characterized by an effort to revive and surpass the ideas and achievements of classical antiquity. Associated with great social change in most fields and disciplines, including art, architecture, politics, literature, exploration and science, the Renaissance was first centered in the Republic of Florence, then spread to the rest of Italy and later throughout Europe. The term rinascita (\"rebirth\") first appeared in Lives of the Artists (c. 1550) by Giorgio Vasari, while the corresponding French word renaissance was adopted into English as the term for this period during the 1830s.[4][b]\n\nThe Renaissance's intellectual basis was founded in its version of humanism, derived from the concept of Roman humanitas and the rediscovery of classical Greek philosophy, such as that of Protagoras, who said that \"man is the measure of all things\". Although the invention of metal movable type sped the dissemination of ideas from the later 15th century, the changes of the Renaissance were not uniform across Europe: the first traces appear in Italy as early as the late 13th century, in particular with the writings of Dante and the paintings of Giotto.\n\nAs a cultural movement, the Renaissance encompassed innovative flowering of literary Latin and an explosion of vernacular literatures, beginning with the 14th-century resurgence of learning based on classical sources, which contemporaries credited to Petrarch; the development of linear perspective and other techniques of rendering a more natural reality in painting; and gradual but widespread educational reform. It saw myriad artistic developments and contributions from such polymaths as Leonardo da Vinci and Michelangelo, who inspired the term \"Renaissance man\".[5][6] In politics, the Renaissance contributed to the development of the customs and conventions of diplomacy, and in science to an increased reliance on observation and inductive reasoning. The period also saw revolutions in other intellectual and social scientific pursuits, as well as the introduction of modern banking and the field of accounting.[7]\n\nThe Renaissance period started during the crisis of the Late Middle Ages and conventionally ends by the 1600s with the waning of humanism, and the advents of the Reformation and Counter-Reformation, and in art the Baroque period. It had a different period and characteristics in different regions, such as the Italian Renaissance, the Northern Renaissance, the Spanish Renaissance, etc.\n\nIn addition to the standard periodization, proponents of a \"long Renaissance\" may put its beginning in the 14th century and its end in the 17th century.[c]\n\nThe traditional view focuses more on the Renaissance's early modern aspects and argues that it was a break from the past, but many historians today focus more on its medieval aspects and argue that it was an extension of the Middle Ages.[11][12] The beginnings of the period—the early Renaissance of the 15th century and the Italian Proto-Renaissance from around 1250 or 1300—overlap considerably with the Late Middle Ages, conventionally dated to c. 1350–1500, and the Middle Ages themselves were a long period filled with gradual changes, like the modern age; as a transitional period between both, the Renaissance has close similarities to both, especially the late and early sub-periods of either.\n\nThe Renaissance began in Florence, one of the many states of Italy.[13] Various theories have been proposed to account for its origins and characteristics, focusing on a variety of factors, including Florence's social and civic peculiarities at the time: its political structure, the patronage of its dominant family, the Medici,[14] and the migration of Greek scholars and their texts to Italy following the fall of Constantinople to the Ottoman Empire.[15][16][17] Other major centers were Venice, Genoa, Milan, Rome during the Renaissance Papacy, and Naples. From Italy, the Renaissance spread throughout Europe and also to American, African and Asian territories ruled by the European colonial powers of the time or where Christian missionaries were active.\n\nThe Renaissance has a long and complex historiography, and in line with general skepticism of discrete periodizations, there has been much debate among historians reacting to the 19th-century glorification of the \"Renaissance\" and individual cultural heroes as \"Renaissance men\", questioning the usefulness of Renaissance as a term and as a historical delineation.[18]\n\nSome observers have questioned whether the Renaissance was a cultural \"advance\" from the Middle Ages, instead seeing it as a period of pessimism and nostalgia for classical antiquity,[19] while social and economic historians, especially of the longue durée, have instead focused on the continuity between the two eras,[20] which are linked, as Panofsky observed, \"by a thousand ties\".[21][d]\n\nThe word has also been extended to other historical and cultural movements, such as the Carolingian Renaissance (8th and 9th centuries), Ottonian Renaissance (10th and 11th century), and the Renaissance of the 12th century.[23]\n\nThe Renaissance was a cultural movement that profoundly affected European intellectual life in the early modern period. Beginning in Italy, and spreading to the rest of Europe by the 16th century, its influence was felt in art, architecture, philosophy, literature, music, science, technology, politics, religion, and other aspects of intellectual inquiry. Renaissance scholars employed the humanist method in study, and searched for realism and human emotion in art.[24]\n\nRenaissance humanists such as Poggio Bracciolini sought out in Europe's monastic libraries the Latin literary, historical, and oratorical texts of antiquity, while the fall of Constantinople (1453) generated a wave of émigré Greek scholars bringing precious manuscripts in ancient Greek, many of which had fallen into obscurity in the West. It was in their new focus on literary and historical texts that Renaissance scholars differed so markedly from the medieval scholars of the Renaissance of the 12th century, who had focused on studying Greek and Arabic works of natural sciences, philosophy, and mathematics, rather than on such cultural texts.[citation needed]\n\nIn the revival of neoplatonism, Renaissance humanists did not reject Christianity; on the contrary, many of the Renaissance's greatest works were devoted to it, and the Church patronized many works of Renaissance art.[citation needed] But a subtle shift took place in the way that intellectuals approached religion that was reflected in many other areas of cultural life.[25][better source needed] In addition, many Greek Christian works, including the Greek New Testament, were brought back from Byzantium to Western Europe and engaged Western scholars for the first time since late antiquity. This new engagement with Greek Christian works, and particularly the return to the original Greek of the New Testament promoted by humanists Lorenzo Valla and Erasmus, helped pave the way for the Reformation.[citation needed]\n\nWell after the first artistic return to classicism had been exemplified in the sculpture of Nicola Pisano, Florentine painters led by Masaccio strove to portray the human form realistically, developing techniques to render perspective and light more naturally. Political philosophers, most famously Niccolò Machiavelli, sought to describe political life as it really was, that is to understand it rationally. A critical contribution to Italian Renaissance humanism, Giovanni Pico della Mirandola wrote De hominis dignitate (Oration on the Dignity of Man, 1486), a series of theses on philosophy, natural thought, faith, and magic defended against any opponent on the grounds of reason. In addition to studying classical Latin and Greek, Renaissance authors also began increasingly to use vernacular languages; combined with the introduction of the printing press, this allowed many more people access to books, especially the Bible.[26]\n\nIn all, the Renaissance can be viewed as an attempt by intellectuals to study and improve the secular and worldly, both through the revival of ideas from antiquity and through novel approaches to thought. Political philosopher Hans Kohn describes it as an age where \"Men looked for new foundations\"; some like Erasmus and Thomas More envisioned new reformed spiritual foundations, others. in the words of Machiavelli, una lunga sperienza delle cose moderne ed una continua lezione delle antiche (a long experience with modern life and a continuous learning from antiquity).[27]\n\nSociologist Rodney Stark, plays down the Renaissance in favor of the earlier innovations of the Italian city-states in the High Middle Ages, which married responsive government, Christianity and the birth of capitalism.[28] This analysis argues that, whereas the great European states (France and Spain) were absolute monarchies, and others were under direct Church control, the independent city-republics of Italy took over the principles of capitalism invented on monastic estates and set off a vast unprecedented Commercial Revolution that preceded and financed the Renaissance.[citation needed]\n\nHistorian Leon Poliakov offers a critical view in his seminal study of European racist thought: The Aryan Myth. According to Poliakov, the use of ethnic origin myths are first used by Renaissance humanists \"in the service of a new born chauvinism\".[29][30]\n\nMany argue that the ideas characterizing the Renaissance had their origin in Florence at the turn of the 13th and 14th centuries, in particular with the writings of Dante Alighieri (1265–1321) and Petrarch (1304–1374), as well as the paintings of Giotto di Bondone (1267–1337). Some writers date the Renaissance quite precisely; one proposed starting point is 1401, when the rival geniuses Lorenzo Ghiberti and Filippo Brunelleschi competed for the contract to build the bronze doors for the Baptistery of the Florence Cathedral (Ghiberti won).[31] Others see more general competition between artists and polymaths such as Brunelleschi, Ghiberti, Donatello, and Masaccio for artistic commissions as sparking the creativity of the Renaissance.\n\nYet it remains much debated why the Renaissance began in Italy, and why it began when it did. Accordingly, several theories have been put forward to explain its origins. Peter Rietbergen posits that various influential Proto-Renaissance movements started from roughly 1300 onwards across many regions of Europe.[32]\n\nIn stark contrast to the High Middle Ages, when Latin scholars focused almost entirely on studying Greek and Arabic works of natural science, philosophy and mathematics,[e] Renaissance scholars were most interested in recovering and studying Latin and Greek literary, historical, and oratorical texts. Broadly speaking, this began in the 14th century with a Latin phase, when Renaissance scholars such as Petrarch, Coluccio Salutati (1331–1406), Niccolò de' Niccoli (1364–1437), and Poggio Bracciolini (1380–1459) scoured the libraries of Europe in search of works by such Latin authors as Cicero, Lucretius, Livy, and Seneca.[33] By the early 15th century, the bulk of the surviving such Latin literature had been recovered; the Greek phase of Renaissance humanism was under way, as Western European scholars turned to recovering ancient Greek literary, historical, oratorical and theological texts.[34]\n\nUnlike with Latin texts, which had been preserved and studied in Western Europe since late antiquity, the study of ancient Greek texts was very limited in medieval Western Europe. Ancient Greek works on science, mathematics, and philosophy had been studied since the High Middle Ages in Western Europe and in the Islamic Golden Age (normally in translation), but Greek literary, oratorical and historical works (such as Homer, the Greek dramatists, Demosthenes and Thucydides) were not studied in either the Latin or medieval Islamic worlds; in the Middle Ages these sorts of texts were only studied by Byzantine scholars. Some argue that the Timurid Renaissance in Samarkand and Herat, whose magnificence toned with Florence as the center of a cultural rebirth,[35][36] were linked to the Ottoman Empire, whose conquests led to the migration of Greek scholars to Italian cities.[37][full citation needed][38][full citation needed][15][39] One of the greatest achievements of Renaissance scholars was to bring this entire class of Greek cultural works back into Western Europe for the first time since late antiquity.\n\nMuslim logicians, most notably Avicenna and Averroes, had inherited Greek ideas after they had invaded and conquered Egypt and the Levant. Their translations and commentaries on these ideas worked their way through the Arab West into Iberia and Sicily, which became important centers for this transmission of ideas. Between the 11th and 13th centuries, many schools dedicated to the translation of philosophical and scientific works from Classical Arabic to Medieval Latin were established in Iberia, most notably the Toledo School of Translators. This work of translation from Islamic culture, though largely unplanned and disorganized, constituted one of the greatest transmissions of ideas in history.[40]\n\nThe movement to reintegrate the regular study of Greek literary, historical, oratorical, and theological texts back into the Western European curriculum is usually dated to the 1396 invitation from Coluccio Salutati to the Byzantine diplomat and scholar Manuel Chrysoloras (c. 1355–1415) to teach Greek in Florence.[41] This legacy was continued by a number of expatriate Greek scholars, from Basilios Bessarion to Leo Allatius.\n\nThe unique political structures of Italy during the Late Middle Ages have led some to theorize that its unusual social climate allowed the emergence of a rare cultural efflorescence. Italy did not exist as a political entity in the early modern period. Instead, it was divided into smaller city-states and territories: the Neapolitans controlled the south, the Florentines and the Romans at the center, the Milanese and the Genoese to the north and west respectively, and the Venetians to the north east. 15th-century Italy was one of the most urbanized areas in Europe.[42] Many of its cities stood among the ruins of ancient Roman buildings; it seems likely that the classical nature of the Renaissance was linked to its origin in the Roman Empire's heartland.[43]\n\nHistorian and political philosopher Quentin Skinner points out that Otto of Freising (c. 1114–1158), a German bishop visiting north Italy during the 12th century, noticed a widespread new form of political and social organization, observing that Italy appeared to have exited from feudalism so that its society was based on merchants and commerce. Linked to this was anti-monarchical thinking, represented in the famous early Renaissance fresco cycle The Allegory of Good and Bad Government by Ambrogio Lorenzetti (painted 1338–1340), whose strong message is about the virtues of fairness, justice, republicanism and good administration. Holding both Church and Empire at bay, these city republics were devoted to notions of liberty. Skinner reports that there were many defences of liberty such as the Matteo Palmieri (1406–1475) celebration of Florentine genius not only in art, sculpture and architecture, but \"the remarkable efflorescence of moral, social and political philosophy that occurred in Florence at the same time\".[44]\n\nEven cities and states beyond central Italy, such as the Republic of Florence at this time, were also notable for their merchant republics, especially the Republic of Venice. Although in practice these were oligarchical, and bore little resemblance to a modern democracy, they did have democratic features and were responsive states, with forms of participation in governance and belief in liberty.[44][45][46] The relative political freedom they afforded was conducive to academic and artistic advancement.[47] Likewise, the position of Italian cities such as Venice as great trading centres made them intellectual crossroads. Merchants brought with them ideas from far corners of the globe, particularly the Levant. Venice was Europe's gateway to trade with the East, and a producer of fine glass, while Florence was a capital of textiles. The wealth such business brought to Italy meant large public and private artistic projects could be commissioned and individuals had more leisure time for study.[47]\n\nOne theory that has been advanced is that the devastation in Florence caused by the Black Death, which hit Europe between 1348 and 1350, resulted in a shift in the world view of people in 14th century Italy. Italy was particularly badly hit by the plague, and it has been speculated that the resulting familiarity with death caused thinkers to dwell more on their lives on Earth, rather than on spirituality and the afterlife.[48] It has also been argued that the Black Death prompted a new wave of piety, manifested in the sponsorship of religious works of art.[49] However, this does not fully explain why the Renaissance occurred specifically in Italy in the 14th century. The Black Death was a pandemic that affected all of Europe in the ways described, not only Italy. The Renaissance's emergence in Italy was most likely the result of the complex interaction of the above factors.[18]\n\nThe plague was carried by fleas on sailing vessels returning from the ports of Asia, spreading quickly due to lack of proper sanitation: the population of England, then about 4.2 million, lost 1.4 million people to the bubonic plague. Florence's population was nearly halved in the year 1348. As a result of the decimation in the populace the value of the working class increased, and commoners came to enjoy more freedom. To answer the increased need for labor, workers traveled in search of the most favorable position economically.[50]\n\nThe demographic decline due to the plague had economic consequences: the prices of food dropped and land values declined by 30–40% in most parts of Europe between 1350 and 1400.[51] Landholders faced a great loss, but for ordinary men and women it was a windfall. The survivors of the plague found not only that the prices of food were cheaper but also that lands were more abundant, and many of them inherited property from their dead relatives.\n\nThe spread of disease was significantly more rampant in areas of poverty. Epidemics ravaged cities, particularly children. Plagues were easily spread by lice, unsanitary drinking water, armies, or by poor sanitation. Children were hit the hardest because many diseases, such as typhus and congenital syphilis, target the immune system, leaving young children without a fighting chance. Children in city dwellings were more affected by the spread of disease than the children of the wealthy.[52]\n\nThe Black Death caused greater upheaval to Florence's social and political structure than later epidemics. Despite a significant number of deaths among members of the ruling classes, the government of Florence continued to function during this period. Formal meetings of elected representatives were suspended during the height of the epidemic due to the chaotic conditions in the city, but a small group of officials was appointed to conduct the affairs of the city, which ensured continuity of government.[53]\n\nIt has long been a matter of debate why the Renaissance began in Florence, and not elsewhere in Italy. Scholars have noted several features unique to Florentine cultural life that may have caused such a cultural movement. Many have emphasized the role played by the Medici, a banking family and later ducal ruling house, in patronizing and stimulating the arts. Some historians have postulated that Florence was the birthplace of the Renaissance as a result of luck, i.e., because \"Great Men\" were born there by chance:[54] Leonardo, Botticelli and Michelangelo were all born in Tuscany. Arguing that such chance seems improbable, other historians have contended that these \"Great Men\" were only able to rise to prominence because of the prevailing cultural conditions at the time.[55]\n\nLorenzo de' Medici (1449–1492) was the catalyst for an enormous amount of arts patronage, encouraging his countrymen to commission works from the leading artists of Florence, including Leonardo da Vinci, Sandro Botticelli, and Michelangelo Buonarroti.[14] Works by Neri di Bicci, Botticelli, Leonardo, and Filippino Lippi had been commissioned additionally by the Convent of San Donato in Scopeto in Florence.[56]\n\nThe Renaissance was certainly underway before Lorenzo de' Medici came to power – indeed, before the Medici family itself achieved hegemony in Florentine society.\n\nIn some ways, Renaissance humanism was not a philosophy but a method of learning. In contrast to the medieval scholastic mode, which focused on resolving contradictions between authors, Renaissance humanists would study ancient texts in their original languages and appraise them through a combination of reasoning and empirical evidence. Humanist education was based on the programme of Studia Humanitatis, the study of five humanities: poetry, grammar, history, moral philosophy, and rhetoric. Although historians have sometimes struggled to define humanism precisely, most have settled on \"a middle of the road definition... the movement to recover, interpret, and assimilate the language, literature, learning and values of ancient Greece and Rome\".[57] Above all, humanists asserted \"the genius of man ... the unique and extraordinary ability of the human mind\".[58]\n\nHumanist scholars shaped the intellectual landscape throughout the early modern period. Political philosophers such as Niccolò Machiavelli and Thomas More revived the ideas of Greek and Roman thinkers and applied them in critiques of contemporary government, following the Islamic steps of Ibn Khaldun.[60][61] Pico della Mirandola wrote the \"manifesto\" of the Renaissance, the Oration on the Dignity of Man, a vibrant defence of thinking.[citation needed] Matteo Palmieri (1406–1475), another humanist, is most known for his work Della vita civile (\"On Civic Life\"; printed 1528), which advocated civic humanism, and for his influence in refining the Tuscan vernacular to the same level as Latin. Palmieri drew on Roman philosophers and theorists, especially Cicero, who, like Palmieri, lived an active public life as a citizen and official, as well as a theorist and philosopher and also Quintilian. Perhaps the most succinct expression of his perspective on humanism is in a 1465 poetic work La città di vita, but an earlier work, Della vita civile, is more wide-ranging. Composed as a series of dialogues set in a country house in the Mugello countryside outside Florence during the plague of 1430, Palmieri expounds on the qualities of the ideal citizen. The dialogues include ideas about how children develop mentally and physically, how citizens can conduct themselves morally, how citizens and states can ensure probity in public life, and an important debate on the difference between that which is pragmatically useful and that which is honest.[citation needed]\n\nThe humanists believed that it is important to transcend to the afterlife with a perfect mind and body, which could be attained with education. The purpose of humanism was to create a universal man whose person combined intellectual and physical excellence and who was capable of functioning honorably in virtually any situation.[62] This ideology was referred to as the uomo universale, an ancient Greco-Roman ideal. Education during the Renaissance was mainly composed of ancient literature and history as it was thought that the classics provided moral instruction and an intensive understanding of human behavior.\n\nA unique characteristic of some Renaissance libraries is that they were open to the public. These libraries were places where ideas were exchanged and where scholarship and reading were considered both pleasurable and beneficial to the mind and soul. As freethinking was a hallmark of the age, many libraries contained a wide range of writers. Classical texts could be found alongside humanist writings. These informal associations of intellectuals profoundly influenced Renaissance culture. An essential tool of Renaissance librarianship was the catalog that listed, described, and classified a library's books.[63] Some of the richest \"bibliophiles\" built libraries as temples to books and knowledge. A number of libraries appeared as manifestations of immense wealth joined with a love of books. In some cases, cultivated library builders were also committed to offering others the opportunity to use their collections. Prominent aristocrats and princes of the Church created great libraries for the use of their courts, called \"court libraries\", and were housed in lavishly designed monumental buildings decorated with ornate woodwork, and the walls adorned with frescoes (Murray, Stuart A.P.).\n\nRenaissance art marks a cultural rebirth at the close of the Middle Ages and rise of the Modern world. One of the distinguishing features of Renaissance art was its development of highly realistic linear perspective. Giotto di Bondone (1267–1337) is credited with first treating a painting as a window into space, but it was not until the demonstrations of architect Filippo Brunelleschi (1377–1446) and the subsequent writings of Leon Battista Alberti (1404–1472) that perspective was formalized as an artistic technique.[64]\n\nThe development of perspective was part of a wider trend toward realism in the arts.[65] Painters developed other techniques, studying light, shadow, and, famously in the case of Leonardo da Vinci, human anatomy. Underlying these changes in artistic method was a renewed desire to depict the beauty of nature and to unravel the axioms of aesthetics, with the works of Leonardo, Michelangelo and Raphael representing artistic pinnacles that were much imitated by other artists.[66] Other notable artists include Sandro Botticelli, working for the Medici in Florence, Donatello, another Florentine, and Titian in Venice, among others.\n\nIn the Low Countries, a particularly vibrant artistic culture developed. The work of Hugo van der Goes and Jan van Eyck was particularly influential on the development of painting in Italy, both technically with the introduction of oil paint and canvas, and stylistically in terms of naturalism in representation. Later, the work of Pieter Brueghel the Elder would inspire artists to depict themes of everyday life.[67]\n\nIn architecture, Filippo Brunelleschi was foremost in studying the remains of ancient classical buildings. With rediscovered knowledge from the 1st-century writer Vitruvius and the flourishing discipline of mathematics, Brunelleschi formulated the Renaissance style that emulated and improved on classical forms. His major feat of engineering was building the dome of Florence Cathedral.[68] Another building demonstrating this style is the Basilica of Sant'Andrea, Mantua, built by Alberti. The outstanding architectural work of the High Renaissance was the rebuilding of St. Peter's Basilica, combining the skills of Bramante, Michelangelo, Raphael, Sangallo and Maderno.\n\nDuring the Renaissance, architects aimed to use columns, pilasters, and entablatures as an integrated system. The Roman orders types of columns are used: Tuscan and Composite. These can either be structural, supporting an arcade or architrave, or purely decorative, set against a wall in the form of pilasters. One of the first buildings to use pilasters as an integrated system was in the Old Sacristy (1421–1440) by Brunelleschi.[69] Arches, semi-circular or (in the Mannerist style) segmental, are often used in arcades, supported on piers or columns with capitals. There may be a section of entablature between the capital and the springing of the arch. Alberti was one of the first to use the arch on a monumental. Renaissance vaults do not have ribs; they are semi-circular or segmental and on a square plan, unlike the Gothic vault, which is frequently rectangular.\n\nRenaissance artists were not pagans, although they admired antiquity and kept some ideas and symbols of the medieval past. Nicola Pisano (c. 1220 – c. 1278) imitated classical forms by portraying scenes from the Bible. His Annunciation, from the Pisa Baptistry, demonstrates that classical models influenced Italian art before the Renaissance took root as a literary movement.[70]\n\nApplied innovation extended to commerce. At the end of the 15th century, Luca Pacioli published the first work on bookkeeping, making him the founder of accounting.[7]\n\nThe rediscovery of ancient texts and the invention of the printing press in about 1440 democratized learning and allowed a faster propagation of more widely distributed ideas. In the first period of the Italian Renaissance, humanists favored the study of humanities over natural philosophy or applied mathematics, and their reverence for classical sources further enshrined the Aristotelian and Ptolemaic views of the universe. Writing around 1450, Nicholas of Cusa anticipated the heliocentric worldview of Copernicus, but in a philosophical fashion.\n\nScience and art were intermingled in the early Renaissance, with polymath artists such as Leonardo da Vinci making observational drawings of anatomy and nature. Leonardo set up controlled experiments in water flow, medical dissection, and systematic study of movement and aerodynamics, and he devised principles of research method that led Fritjof Capra to classify him as the \"father of modern science\".[g] Other examples of Da Vinci's contribution during this period include machines designed to saw marbles and lift monoliths, and new discoveries in acoustics, botany, geology, anatomy, and mechanics.[73]\n\nA suitable environment had developed to question classical scientific doctrine. The discovery in 1492 of the New World by Christopher Columbus challenged the classical worldview. The works of Ptolemy (in geography) and Galen (in medicine) were found to not always match everyday observations. As the Reformation and Counter-Reformation clashed, the Northern Renaissance showed a decisive shift in focus from Aristotelean natural philosophy to chemistry and the biological sciences (botany, anatomy, and medicine).[74] The willingness to question previously held truths and search for new answers resulted in a period of major scientific advancements.\n\nSome view this as a \"scientific revolution\", heralding the beginning of the modern age,[75] others as an acceleration of a continuous process stretching from the ancient world to the present day.[76] Significant scientific advances were made during this time by Galileo Galilei, Tycho Brahe, and Johannes Kepler.[77] Copernicus, in De revolutionibus orbium coelestium (On the Revolutions of the Heavenly Spheres), posited that the Earth moved around the Sun. De humani corporis fabrica (On the Workings of the Human Body) by Andreas Vesalius, gave a new confidence to the role of dissection, observation, and the mechanistic view of anatomy.[78]\n\nAnother important development was in the process for discovery, the scientific method,[78] focusing on empirical evidence and the importance of mathematics, while discarding much of Aristotelian science. Early and influential proponents of these ideas included Copernicus, Galileo, and Francis Bacon.[79][80] The new scientific method led to great contributions in the fields of astronomy, physics, biology, and anatomy.[h][81]\n\nDuring the Renaissance, extending from 1450 to 1650,[82] every continent was visited and mostly mapped by Europeans, except the south polar continent now known as Antarctica. This development is depicted in the large world map Nova Totius Terrarum Orbis Tabula made by the Dutch cartographer Joan Blaeu in 1648 to commemorate the Peace of Westphalia.\n\nIn 1492, Christopher Columbus sailed across the Atlantic Ocean from Spain seeking a direct route to India of the Delhi Sultanate. He accidentally stumbled upon the Americas, but believed he had reached the East Indies.\n\nIn 1606, the Dutch navigator Willem Janszoon sailed from the East Indies in the Dutch East India Company ship Duyfken and landed in Australia. He charted about 300 km of the west coast of Cape York Peninsula in Queensland. More than thirty Dutch expeditions followed, mapping sections of the north, west, and south coasts. In 1642–1643, Abel Tasman circumnavigated the continent, proving that it was not joined to the imagined south polar continent.\n\nBy 1650, Dutch cartographers had mapped most of the coastline of the continent, which they named New Holland, except the east coast which was charted in 1770 by James Cook.\n\nThe long-imagined south polar continent was eventually sighted in 1820. Throughout the Renaissance it had been known as Terra Australis, or 'Australia' for short. However, after that name was transferred to New Holland in the nineteenth century, the new name of 'Antarctica' was bestowed on the south polar continent.[83]\n\nFrom this changing society emerged a common, unifying musical language, in particular the polyphonic style of the Franco-Flemish school. The development of printing made distribution of music possible on a wide scale. Demand for music as entertainment and as an activity for educated amateurs increased with the emergence of a bourgeois class. Dissemination of chansons, motets, and masses throughout Europe coincided with the unification of polyphonic practice into the fluid style that culminated in the second half of the sixteenth century in the work of composers such as Giovanni Pierluigi da Palestrina, Orlande de Lassus, Tomás Luis de Victoria, and William Byrd.\n\nThe new ideals of humanism, although more secular in some aspects, developed against a Christian backdrop, especially in the Northern Renaissance. Much, if not most, of the new art was commissioned by or in dedication to the Roman Catholic Church.[25] However, the Renaissance had a profound effect on contemporary theology, particularly in the way people perceived the relationship between man and God.[25] Many of the period's foremost theologians were followers of the humanist method, including Erasmus, Huldrych Zwingli, Thomas More, Martin Luther, and John Calvin.\n\nThe Renaissance began in times of religious turmoil. The Late Middle Ages was a period of political intrigue surrounding the Papacy, culminating in the Western Schism, in which three men simultaneously claimed to be true Bishop of Rome.[84] While the schism was resolved by the Council of Constance (1414), a resulting reform movement known as Conciliarism sought to limit the power of the pope. Although the papacy eventually emerged supreme in ecclesiastical matters by the Fifth Council of the Lateran (1511), it was dogged by continued accusations of corruption, most famously in the person of Pope Alexander VI, who was accused variously of simony, nepotism, and fathering children (most of whom were married off, presumably for the consolidation of power) while a cardinal.[85]\n\nChurchmen such as Erasmus and Luther proposed reform to the Church, often based on humanist textual criticism of the New Testament.[25] In October 1517, Luther published the Ninety-five Theses, challenging papal authority and criticizing its perceived corruption, particularly with regard to instances of sold indulgences.[i] The 95 Theses led to the Reformation, a break with the Roman Catholic Church that previously claimed hegemony in Western Europe. Humanism and the Renaissance therefore played a direct role in sparking the Reformation, as well as in many other contemporaneous religious debates and conflicts.\n\nPope Paul III came to the papal throne (1534–1549) after the sack of Rome in 1527, with uncertainties prevalent in the Catholic Church following the Reformation. Nicolaus Copernicus dedicated De revolutionibus orbium coelestium (On the Revolutions of the Celestial Spheres) to Paul III, who became the grandfather of Alessandro Farnese, who had paintings by Titian, Michelangelo, and Raphael, as well as an important collection of drawings, and who commissioned the masterpiece of Giulio Clovio, arguably the last major illuminated manuscript, the Farnese Hours.\n\nBy the 15th century, writers, artists, and architects in Italy were well aware of the transformations that were taking place and were using phrases such as modi antichi (in the antique manner) or alle romana et alla antica (in the manner of the Romans and the ancients) to describe their work. In the 1330s Petrarch referred to pre-Christian times as antiqua (ancient) and to the Christian period as nova (new).[86] From Petrarch's Italian perspective, this new period (which included his own time) was an age of national eclipse.[86] Leonardo Bruni was the first to use tripartite periodization in his History of the Florentine People (1442).[87] Bruni's first two periods were based on those of Petrarch, but he added a third period because he believed that Italy was no longer in a state of decline. Flavio Biondo used a similar framework in Decades of History from the Deterioration of the Roman Empire (1439–1453).\n\nHumanist historians argued that contemporary scholarship restored direct links to the classical period, thus bypassing the Medieval period, which they then named for the first time the \"Middle Ages\". The term first appears in Latin in 1469 as media tempestas (middle times).[88] The term rinascita (rebirth) first appeared, however, in its broad sense in Giorgio Vasari's Lives of the Artists, 1550, revised 1568.[89][90] Vasari divides the age into three phases: the first phase contains Cimabue, Giotto, and Arnolfo di Cambio; the second phase contains Masaccio, Brunelleschi, and Donatello; the third centers on Leonardo da Vinci and culminates with Michelangelo. It was not just the growing awareness of classical antiquity that drove this development, according to Vasari, but also the growing desire to study and imitate nature.[91]\n\nIn the 15th century, the Renaissance spread rapidly from its birthplace in Florence to the rest of Italy and soon to the rest of Europe. The invention of the printing press by German printer Johannes Gutenberg allowed the rapid transmission of these new ideas. As it spread, its ideas diversified and changed, being adapted to local culture. In the 20th century, scholars began to break the Renaissance into regional and national movements.\n\nThe Elizabethan era in the second half of the 16th century is usually regarded as the height of the English Renaissance. Many scholars see its beginnings in the early 16th century during the reign of Henry VIII.[92]\n\nThe English Renaissance is different from the Italian Renaissance in several ways. The dominant art forms of the English Renaissance were literature and music, which had a rich flowering.[93] Visual arts in the English Renaissance were much less significant than in the Italian Renaissance. The English Renaissance period in art began far later than the Italian, which had moved into Mannerism by the 1530s.[94]\n\nIn literature the later part of the 16th century saw the flowering of Elizabethan literature, with poetry heavily influenced by Italian Renaissance literature but Elizabethan theatre a distinctive native style. Writers include William Shakespeare (1564–1616), Christopher Marlowe (1564–1593), Edmund Spenser (1552–1599), Sir Thomas More (1478–1535), and Sir Philip Sidney (1554–1586). English Renaissance music competed with that in Europe with composers such as Thomas Tallis (1505–1585), John Taverner (1490–1545), and William Byrd (1540–1623). Elizabethan architecture produced the large prodigy houses of courtiers, and in the next century Inigo Jones (1573–1652), who introduced Palladian architecture to England.[95]\n\nElsewhere, Sir Francis Bacon (1561–1626) was the pioneer of modern scientific thought, and is commonly regarded as one of the founders of the Scientific Revolution.[96][97]\n\nThe word \"Renaissance\" is borrowed from the French language, where it means \"re-birth\". It was first used in the eighteenth century and was later popularized by French historian Jules Michelet (1798–1874) in his 1855 work, Histoire de France (History of France).[98][99]\n\nIn 1495 the Italian Renaissance arrived in France, imported by King Charles VIII after his invasion of Italy. A factor that promoted the spread of secularism was the inability of the Church to offer assistance against the Black Death. Francis I imported Italian art and artists, including Leonardo da Vinci, and built ornate palaces at great expense. Writers such as François Rabelais, Pierre de Ronsard, Joachim du Bellay, and Michel de Montaigne, painters such as Jean Clouet, and musicians such as Jean Mouton also borrowed from the spirit of the Renaissance.\n\nIn 1533, a fourteen-year-old Catherine de' Medici (1519–1589), born in Florence to Lorenzo de' Medici, Duke of Urbino and Madeleine de La Tour d'Auvergne, married Henry II of France, second son of King Francis I and Queen Claude. Though she became famous and infamous for her role in the French Wars of Religion, she made a direct contribution in bringing arts, sciences, and music (including the origins of ballet) to the French court from her native Florence.\n\nIn the second half of the 15th century, the Renaissance spirit spread to Germany and the Low Countries, where the development of the printing press (ca. 1450) and Renaissance artists such as Albrecht Dürer (1471–1528) predated the influence from Italy. In the early Protestant areas of the country humanism became closely linked to the turmoil of the Reformation, and the art and writing of the German Renaissance frequently reflected this dispute.[100] However, the Gothic style and medieval scholastic philosophy remained exclusively until the turn of the 16th century. Emperor Maximilian I of Habsburg (ruling 1493–1519) was the first truly Renaissance monarch of the Holy Roman Empire.\n\nAfter Italy, Hungary was the first European country where the Renaissance appeared.[101] The Renaissance style came directly from Italy during the Quattrocento (1400s) to Hungary first in the Central European region, thanks to the development of early Hungarian-Italian relationships — not only in dynastic connections, but also in cultural, humanistic and commercial relations – growing in strength from the 14th century. The relationship between Hungarian and Italian Gothic styles was a second reason – exaggerated breakthrough of walls is avoided, preferring clean and light structures. Large-scale building schemes provided ample and long term work for the artists, for example, the building of the Friss (New) Castle in Buda, the castles of Visegrád, Tata, and Várpalota. In Sigismund's court there were patrons such as Pippo Spano, a descendant of the Scolari family of Florence, who invited Manetto Ammanatini and Masolino da Pannicale to Hungary.[102]\n\nThe new Italian trend combined with existing national traditions to create a particular local Renaissance art. Acceptance of Renaissance art was furthered by the continuous arrival of humanist thought in the country. Many young Hungarians studying at Italian universities came closer to the Florentine humanist center, so a direct connection with Florence evolved. The growing number of Italian traders moving to Hungary, specially to Buda, helped this process. New thoughts were carried by the humanist prelates, among them Vitéz János, archbishop of Esztergom, one of the founders of Hungarian humanism.[103] During the long reign of Emperor Sigismund of Luxemburg the Royal Castle of Buda became probably the largest Gothic palace of the late Middle Ages. King Matthias Corvinus (r. 1458–1490) rebuilt the palace in early Renaissance style and further expanded it.[104][105]\n\nAfter the marriage in 1476 of King Matthias to Beatrice of Naples, Buda became one of the most important artistic centers of the Renaissance north of the Alps.[106] The most important humanists living in Matthias' court were Antonio Bonfini and the famous Hungarian poet Janus Pannonius.[106] András Hess set up a printing press in Buda in 1472. Matthias Corvinus's library, the Bibliotheca Corviniana, was Europe's greatest collections of secular books: historical chronicles, philosophic and scientific works in the 15th century. His library was second only in size to the Vatican Library. (However, the Vatican Library mainly contained Bibles and religious materials.)[107] In 1489, Bartolomeo della Fonte of Florence wrote that Lorenzo de' Medici founded his own Greek-Latin library encouraged by the example of the Hungarian king. Corvinus's library is part of UNESCO World Heritage.[108]\n\nMatthias started at least two major building projects.[109] The works in Buda and Visegrád began in about 1479.[110] Two new wings and a hanging garden were built at the royal castle of Buda, and the palace at Visegrád was rebuilt in Renaissance style.[110][111] Matthias appointed the Italian Chimenti Camicia and the Dalmatian Giovanni Dalmata to direct these projects. [110] Matthias commissioned the leading Italian artists of his age to embellish his palaces: for instance, the sculptor Benedetto da Majano and the painters Filippino Lippi and Andrea Mantegna worked for him.[112] A copy of Mantegna's portrait of Matthias survived.[113] Matthias also hired the Italian military engineer Aristotele Fioravanti to direct the rebuilding of the forts along the southern frontier.[114] He had new monasteries built in Late Gothic style for the Franciscans in Kolozsvár, Szeged and Hunyad, and for the Paulines in Fejéregyháza.[115][116] In the spring of 1485, Leonardo da Vinci travelled to Hungary on behalf of Sforza to meet King Matthias Corvinus, and was commissioned by him to paint a Madonna.[117]\n\nMatthias enjoyed the company of Humanists and had lively discussions on various topics with them.[118] The fame of his magnanimity encouraged many scholars—mostly Italian—to settle in Buda.[119] Antonio Bonfini, Pietro Ranzano, Bartolomeo Fonzio, and Francesco Bandini spent many years in Matthias's court.[120][118] This circle of educated men introduced the ideas of Neoplatonism to Hungary.[121][122] Like all intellectuals of his age, Matthias was convinced that the movements and combinations of the stars and planets exercised influence on individuals' life and on the history of nations.[123] Martius Galeotti described him as \"king and astrologer\", and Antonio Bonfini said Matthias \"never did anything without consulting the stars\".[124] Upon his request, the famous astronomers of the age, Johannes Regiomontanus and Marcin Bylica, set up an observatory in Buda and installed it with astrolabes and celestial globes.[125] Regiomontanus dedicated his book on navigation that was used by Christopher Columbus to Matthias.[119]\n\nOther important figures of Hungarian Renaissance include Bálint Balassi (poet), Sebestyén Tinódi Lantos (poet), Bálint Bakfark (composer and lutenist), and Master MS (fresco painter).\n\nCulture in the Netherlands at the end of the 15th century was influenced by the Italian Renaissance through trade via Bruges, which made Flanders wealthy. Its nobles commissioned artists who became known across Europe.[126] In science, the anatomist Andreas Vesalius led the way; in cartography, Gerardus Mercator's map assisted explorers and navigators. In art, Dutch and Flemish Renaissance painting ranged from the strange work of Hieronymus Bosch[127] to the everyday life depictions of Pieter Brueghel the Elder.[126]\n\nErasmus was arguably the Netherlands' best known humanist and Catholic intellectual during the Renaissance.[32]\n\nThe Renaissance in Northern Europe has been termed the \"Northern Renaissance\". While Renaissance ideas were moving north from Italy, there was a simultaneous southward spread of some areas of innovation, particularly in music.[128] The music of the 15th-century Burgundian School defined the beginning of the Renaissance in music, and the polyphony of the Netherlanders, as it moved with the musicians themselves into Italy, formed the core of the first true international style in music since the standardization of Gregorian Chant in the 9th century.[128] The culmination of the Netherlandish school was in the music of the Italian composer Giovanni Pierluigi da Palestrina. At the end of the 16th century Italy again became a center of musical innovation, with the development of the polychoral style of the Venetian School, which spread northward into Germany around 1600. In Denmark, the Renaissance sparked the translation of the works of Saxo Grammaticus into Danish as well as Frederick II and Christian IV ordering the redecoration or construction of several important works of architecture, i.e. Kronborg, Rosenborg and Børsen.[129] Danish astronomer Tycho Brahe greatly contributed to turn astronomy into the first modern science and also helped launch the Scientific Revolution.[130][131]\n\nThe paintings of the Italian Renaissance differed from those of the Northern Renaissance. Italian Renaissance artists were among the first to paint secular scenes, breaking away from the purely religious art of medieval painters. Northern Renaissance artists initially remained focused on religious subjects, such as the contemporary religious upheaval portrayed by Albrecht Dürer. Later, the works of Pieter Bruegel the Elder influenced artists to paint scenes of daily life rather than religious or classical themes. It was also during the Northern Renaissance that Flemish brothers Hubert and Jan van Eyck perfected the oil painting technique, which enabled artists to produce strong colors on a hard surface that could survive for centuries.[132] A feature of the Northern Renaissance was its use of the vernacular in place of Latin or Greek, which allowed greater freedom of expression. This movement had started in Italy with the decisive influence of Dante Alighieri on the development of vernacular languages; in fact the focus on writing in Italian has neglected a major source of Florentine ideas expressed in Latin.[133] The spread of the printing press technology boosted the Renaissance in Northern Europe as elsewhere, with Venice becoming a world center of printing.\n\nThe Polish Renaissance lasted from the late 15th to the late 16th century and was the Golden Age of Polish culture. Ruled by the Jagiellonian dynasty, the Kingdom of Poland (from 1569 known as the Polish–Lithuanian Commonwealth) actively participated in the broad European Renaissance. An early Italian humanist who came to Poland in the mid-15th century was Filippo Buonaccorsi, who was employed as royal advisor and councillor. The tomb of John I Albert, completed in 1505 by Francesco Fiorentino, is the first example of a Renaissance composition in the country.[134][135] Many Italian artists subsequently came to Poland with Bona Sforza of Milan, when she married King Sigismund I in 1518.[136] This was supported by temporarily strengthened monarchies in both areas, as well as by newly established universities.[137]\n\nThe Renaissance was a period when the multi-national Polish state experienced a substantial period of cultural growth thanks in part to a century without major wars, aside from conflicts in the sparsely populated eastern and southern borderlands. Architecture became more refined and decorative. Mannerism played an important part in shaping what is now considered to be the truly Polish architectural style – high attics above the cornice with pinnacles and pilasters.[138] It was also the time when the first major works of Polish literature were published, particularly those of Mikołaj Rey and Jan Kochanowski, and the Polish language became the lingua franca of East-Central Europe.[139] The Jagiellonian University transformed into a major institution of higher education for the region and hosted many notable scholars, chiefly Nicolaus Copernicus and Conrad Celtes. Three more academies were founded at Königsberg (1544), Vilnius (1579), and Zamość (1594). The Reformation spread peacefully throughout the country, giving rise to the Nontrinitarian Polish Brethren.[140] Living conditions improved, cities grew, and exports of agricultural products enriched the population, especially the nobility (szlachta) and magnates. The nobles gained dominance in the new political system of Golden Liberty, a counterweight to monarchical absolutism.[141]\n\nAlthough Italian Renaissance had a modest impact in Portuguese arts, Portugal was influential in broadening the European worldview,[142] stimulating humanist inquiry. Renaissance arrived through the influence of wealthy Italian and Flemish merchants who invested in the profitable commerce overseas. As the pioneer headquarters of European exploration, Lisbon flourished in the late 15th century, attracting experts who made several breakthroughs in mathematics, astronomy and naval technology, including Pedro Nunes, João de Castro, Abraham Zacuto, and Martin Behaim. Cartographers Pedro Reinel, Lopo Homem, Estêvão Gomes, and Diogo Ribeiro made crucial advances in mapping the world. Apothecary Tomé Pires and physicians Garcia de Orta and Cristóvão da Costa collected and published works on plants and medicines, soon translated by Flemish pioneer botanist Carolus Clusius.\n\nIn architecture, the huge profits of the spice trade financed a sumptuous composite style in the first decades of the 16th century, the Manueline, incorporating maritime elements.[143] The primary painters were Nuno Gonçalves, Gregório Lopes, and Vasco Fernandes. In music, Pedro de Escobar and Duarte Lobo produced four songbooks, including the Cancioneiro de Elvas.\n\nIn literature, Luís de Camões inscribed the Portuguese feats overseas in the epic poem Os Lusíadas. Sá de Miranda introduced Italian forms of verse and Bernardim Ribeiro developed pastoral romance, while plays by Gil Vicente fused it with popular culture, reporting the changing times. Travel literature especially flourished: João de Barros, Fernão Lopes de Castanheda, António Galvão, Gaspar Correia, Duarte Barbosa, and Fernão Mendes Pinto, among others, described new lands and were translated and spread with the new printing press.[142] After joining the Portuguese exploration of Brazil in 1500, Amerigo Vespucci coined the term New World,[144] in his letters to Lorenzo di Pierfrancesco de' Medici.\n\nThe intense international exchange produced several cosmopolitan humanist scholars, including Francisco de Holanda, André de Resende, and Damião de Góis, a friend of Erasmus who wrote with rare independence on the reign of King Manuel I. Diogo de Gouveia and André de Gouveia made relevant teaching reforms via France. Foreign news and products in the Portuguese factory in Antwerp attracted the interest of Thomas More[145] and Albrecht Dürer to the wider world.[146] There, profits and know-how helped nurture the Dutch Renaissance and Golden Age, especially after the arrival of the wealthy cultured Jewish community expelled from Portugal.\n\nThe Renaissance arrived in the Iberian peninsula through the Mediterranean possessions of the Crown of Aragon and the city of Valencia. Many early Spanish Renaissance writers come from the Crown of Aragon, including Ausiàs March and Joanot Martorell. In the Crown of Castile, the early Renaissance was heavily influenced by the Italian humanism, starting with writers and poets such as Íñigo López de Mendoza, marqués de Santillana, who introduced the new Italian poetry to Spain in the early 15th century. Other writers, such as Jorge Manrique, Fernando de Rojas, Juan del Encina, Juan Boscán Almogáver, and Garcilaso de la Vega, kept a close resemblance to the Italian canon. Miguel de Cervantes's masterpiece Don Quixote is credited as the first Western novel. Renaissance humanism flourished in the early 16th century, with influential writers such as philosopher Juan Luis Vives, grammarian Antonio de Nebrija and natural historian Pedro de Mexía. The poet and philosopher Luisa de Medrano, celebrated among her Renaissance contemporaries as one of the puellae doctae (\"learned girls\"), was the first female professor in Europe at the University of Salamanca.\n\nLater Spanish Renaissance tended toward religious themes and mysticism, with poets such as Luis de León, Teresa of Ávila, and John of the Cross, and treated issues related to the exploration of the New World, with chroniclers and writers such as Inca Garcilaso de la Vega and Bartolomé de las Casas, giving rise to a body of work, now known as Spanish Renaissance literature. The late Renaissance in Spain produced political and religious authors such as Tomás Fernández de Medrano and artists such as El Greco and composers such as Tomás Luis de Victoria and Antonio de Cabezón.\n\nThe Italian artist and critic Giorgio Vasari (1511–1574) first used the term rinascita in his book The Lives of the Artists (published 1550). In the book Vasari attempted to define what he described as a break with the barbarities of Gothic art: the arts (he held) had fallen into decay with the collapse of the Roman Empire and only the Tuscan artists, beginning with Cimabue (1240–1301) and Giotto (1267–1337) began to reverse this decline in the arts. Vasari saw ancient art as central to the rebirth of Italian art.[147]\n\nHowever, only in the 19th century did the French word renaissance achieve popularity in describing the self-conscious cultural movement based on revival of Roman models that began in the late 13th century. French historian Jules Michelet (1798–1874) defined \"The Renaissance\" in his 1855 work Histoire de France as an entire historical period, whereas previously it had been used in a more limited sense.[23] For Michelet, the Renaissance was more a development in science than in art and culture. He asserted that it spanned the period from Columbus to Copernicus to Galileo; that is, from the end of the 15th century to the middle of the 17th century.[98] Moreover, Michelet distinguished between what he called, \"the bizarre and monstrous\" quality of the Middle Ages and the democratic values that he, as a vocal Republican, chose to see in its character.[18] A French nationalist, Michelet also sought to claim the Renaissance as a French movement.[18]\n\nThe Swiss historian Jacob Burckhardt (1818–1897) in his The Civilization of the Renaissance in Italy (1860), by contrast, defined the Renaissance as the period between Giotto and Michelangelo in Italy, that is, the 14th to mid-16th centuries. He saw in the Renaissance the emergence of the modern spirit of individuality, which the Middle Ages had stifled.[148] His book was widely read and became influential in the development of the modern interpretation of the Italian Renaissance.[149]\n\nMore recently, some historians have been much less keen to define the Renaissance as a historical age, or even as a coherent cultural movement. The historian Randolph Starn, of the University of California Berkeley, stated in 1998:\n\nRather than a period with definitive beginnings and endings and consistent content in between, the Renaissance can be (and occasionally has been) seen as a movement of practices and ideas to which specific groups and identifiable persons variously responded in different times and places. It would be in this sense a network of diverse, sometimes converging, sometimes conflicting cultures, not a single, time-bound culture.[20]\n\nThere is debate about the extent to which the Renaissance improved on the culture of the Middle Ages. Both Michelet and Burckhardt were keen to describe the progress made in the Renaissance toward the modern age. Burckhardt likened the change to a veil being removed from man's eyes, allowing him to see clearly.[54]\n\nIn the Middle Ages both sides of human consciousness – that which was turned within as that which was turned without – lay dreaming or half awake beneath a common veil. The veil was woven of faith, illusion, and childish prepossession, through which the world and history were seen clad in strange hues.[150]\nOn the other hand, many historians now point out that most of the negative social factors popularly associated with the medieval period – poverty, warfare, religious and political persecution, for example – seem to have worsened in this era, which saw the rise of Machiavellian politics, the Wars of Religion, the corrupt Borgia Popes, and the intensified witch-hunts of the 16th century. Many people who lived during the Renaissance did not view it as the \"golden age\" imagined by certain 19th-century authors, but were concerned by these social maladies.[151] Significantly, though, the artists, writers, and patrons involved in the cultural movements in question believed they were living in a new era that was a clean break from the Middle Ages.[89] Some Marxist historians prefer to describe the Renaissance in material terms, holding the view that the changes in art, literature, and philosophy were part of a general economic trend from feudalism toward capitalism, resulting in a bourgeois class with leisure time to devote to the arts.[152]\n\nJohan Huizinga (1872–1945) acknowledged the existence of the Renaissance but questioned whether it was a positive change. In his book The Autumn of the Middle Ages, he argued that the Renaissance was a period of decline from the High Middle Ages, destroying much that was important.[19] The Medieval Latin language, for instance, had evolved greatly from the classical period and was still a living language used in the church and elsewhere. The Renaissance obsession with classical purity halted its further evolution and saw Latin revert to its classical form. This view is however somewhat contested by recent studies. Robert S. Lopez has contended that it was a period of deep economic recession.[153] Meanwhile, George Sarton and Lynn Thorndike have both argued that scientific progress was perhaps less original than has traditionally been supposed.[154] Finally, Joan Kelly argued that the Renaissance led to greater gender dichotomy, lessening the agency women had had during the Middle Ages.[155]\n\nSome historians have begun to consider the word Renaissance to be unnecessarily loaded, implying an unambiguously positive rebirth from the supposedly more primitive \"Dark Ages\", the Middle Ages. Most political and economic historians now prefer to use the term \"early modern\" for this period (and a considerable period afterwards), a designation intended to highlight the period as a transitional one between the Middle Ages and the modern era.[156] Others such as Roger Osborne have come to consider the Italian Renaissance as a repository of the myths and ideals of western history in general, and instead of rebirth of ancient ideas as a period of great innovation.[157]\n\nThe art historian Erwin Panofsky observed of this resistance to the concept of \"Renaissance\":\n\nIt is perhaps no accident that the factuality of the Italian Renaissance has been most vigorously questioned by those who are not obliged to take a professional interest in the aesthetic aspects of civilization – historians of economic and social developments, political and religious situations, and, most particularly, natural science – but only exceptionally by students of literature and hardly ever by historians of Art.[158]\nThe term Renaissance has also been used to define periods outside of the 15th and 16th centuries in the earlier Medieval period. Charles H. Haskins (1870–1937), for example, made a case for a Renaissance of the 12th century.[159] Other historians have argued for a Carolingian Renaissance in the 8th and 9th centuries, Ottonian Renaissance in the 10th century and for the Timurid Renaissance of the 14th century. The Islamic Golden Age has been also sometimes termed with the Islamic Renaissance.[160] The Macedonian Renaissance is a term used for a period in the Roman Empire in the 9th-11th centuries CE.\n\nOther periods of cultural rebirth in Modern times have also been termed \"renaissances\", such as the Bengal Renaissance, Tamil Renaissance, Nepal Bhasa renaissance, al-Nahda or the Harlem Renaissance. The term can also be used in cinema. In animation, the Disney Renaissance is a period that spanned the years from 1989 to 1999 which saw the studio return to the level of quality not witnessed since their Golden Age of Animation. The San Francisco Renaissance was a vibrant period of exploratory poetry and fiction writing in San Francisco in the mid-20th century.\n\nRapid accumulation of knowledge, which has characterized the development of science since the 17th century, had never occurred before that time. The new kind of scientific activity emerged only in a few countries of Western Europe, and it was restricted to that small area for about two hundred years. (Since the 19th century, scientific knowledge has been assimilated by the rest of the world)."
    },
    {
        "title": "Wonders of the World",
        "url": "https://en.wikipedia.org/wiki/Seven_Wonders_of_the_World",
        "content": "\n\nVarious lists of the Wonders of the World have been compiled from antiquity to the present day, in order to catalogue the world's most spectacular natural features and human-built structures.\n\nThe Seven Wonders of the Ancient World is the oldest known list of this type, documenting the most iconic and remarkable human-made creations of classical antiquity; it was based on guidebooks popular among Hellenic sightseers and as such only includes works located around the Mediterranean rim and in the ancient Near East. The number seven was chosen because the Greeks believed it represented perfection and plenty, and because it reflected the number of planets known in ancient times (five) plus the Sun and Moon.[1]\n\nThe Greek historian Herodotus (484 – c. 425 BC) and the scholar Callimachus of Cyrene (c. 305–240 BC), at the Museum of Alexandria, made early lists of seven wonders. These lists have not survived, however, except as references in other writings.\n\nThe classic Seven Wonders were:\n\nIn the 19th and early 20th centuries, some writers emulated the classical list by creating their own lists with names such as \"Wonders of the Middle Ages\", \"Seven Wonders of the Middle Ages\", \"Seven Wonders of the Medieval Mind\", and \"Architectural Wonders of the Middle Ages\".[2] It is unlikely that any of these lists actually originated in the Middle Ages since the concept of a \"Middle Age\" did not become popular until at least the 16th century and the word \"medieval\" was not invented until the Enlightenment era. Brewer's Dictionary of Phrase and Fable refers to them as \"later list[s]\",[3] suggesting the lists were created after the Middle Ages.\n\nMany of the structures on these lists were built much earlier than the Middle Ages but were well known throughout the world.[4][5] Typically representative of such lists are:[3][4][6][7]\n\nOther structures sometimes included on such lists include:\n\n\n\nFollowing in the tradition of the classical list, modern people and organisations have made their own lists of wonderful things, both ancient and modern, natural and artificial. Some of the most notable lists are presented below.\n\nIn 1994, the American Society of Civil Engineers compiled a list of Seven Wonders of the Modern World, paying tribute to the \"greatest civil engineering achievements of the 20th century\".[11][12]\n\nIn November 2006, the American national newspaper USA Today and the American television show Good Morning America revealed a list of the \"New Seven Wonders\", both natural and human-made, as chosen by six judges.[14] The Grand Canyon was added as an eighth wonder on November 24, 2006, in response to viewer feedback.[15]\n\nSimilar to the other lists of wonders, there is no consensus on a list of seven natural wonders of the world, and there has been debate over how large such a list should be. One of many existing versions of this list was compiled by CNN in 1997:[16]\n\nIn 2001, an initiative was started by the Swiss corporation New7Wonders Foundation to choose the New 7 Wonders of the World from a selection of 200 existing monuments through online votes.[17] The Great Pyramid of Giza—part of the Giza Pyramids, the only remaining wonder of the traditional Seven Wonders of the Ancient World, was not one of the winners announced in 2007 but was added as an honorary candidate.[18][19]\n\n\n\nA similar contemporary effort to create a list of seven natural (as opposed to human-made) wonders chosen through a global poll, called the New 7 Wonders of Nature, was organized from 2007 to 2011 by the same group as the New 7 Wonders of the World campaign.\n\nNew 7 Wonders Cities, a third list organized by New7Wonders and determined by another global vote, includes entire cities:\n\nThe list of \"Seven Wonders of the Underwater World\" was drawn up by CEDAM International, an American-based non-profit group for divers that is dedicated to ocean preservation and research. In 1989, CEDAM brought together a panel of marine scientists, including Eugenie Clark, to choose underwater areas which they considered worthy of protection. The results were announced at The National Aquarium in Washington, D.C., by actor Lloyd Bridges, star of TV's Sea Hunt:[21]\n\nBritish author Deborah Cadbury wrote Seven Wonders of the Industrial World, a book telling the stories of seven great feats of engineering of the 19th and early 20th centuries.[22] In 2003, the BBC aired a seven-part docudrama exploring the same feats, with Cadbury as a producer.[23]\n\nIn a 1999 article, Astronomy magazine listed the \"Seven Wonders of the Solar System\".  This article was later made into a video.[24]\n\nMany authors and organisations have composed lists of the wonders of the world that have been published in book or magazine form.\n\nSeven Wonders of the World is a 1956 film in which Lowell Thomas searches the world for natural and artificial wonders and invites the audience to try to update the ancient Wonders of the World list.\n"
    },
    {
        "title": "Great Barrier Reef",
        "url": "https://en.wikipedia.org/wiki/Great_Barrier_Reef",
        "content": "\n\n\n\nThe Great Barrier Reef is the world's largest coral reef system,[1][2] composed of over 2,900 individual reefs[3] and 900 islands stretching for over 2,300 kilometres (1,400 mi) over an area of approximately 344,400 square kilometres (133,000 sq mi).[4][5] The reef is located in the Coral Sea, off the coast of Queensland, Australia, separated from the coast by a channel 160 kilometres (100 mi) wide in places and over 61 metres (200 ft) deep.[6] The Great Barrier Reef can be seen from outer space and is the world's biggest single structure made by living organisms.[7] This reef structure is composed of and built by billions of tiny organisms, known as coral polyps.[8] It supports a wide diversity of life and was selected as a World Heritage Site in 1981.[1][2] CNN labelled it one of the Seven Natural Wonders of the World in 1997.[9] Australian World Heritage places included it in its list in 2007.[10] The Queensland National Trust named it a state icon of Queensland in 2006.[11]\n\nA large part of the reef is protected by the Great Barrier Reef Marine Park, which helps to limit the impact of human use, such as fishing and tourism. Other environmental pressures on the reef and its ecosystem include runoff of humanmade pollutants, climate change accompanied by mass coral bleaching, dumping of dredging sludge and cyclic population outbreaks of the crown-of-thorns starfish.[12] According to a study published in October 2012 by the Proceedings of the National Academy of Sciences, the reef has lost more than half its coral cover since 1985, a finding reaffirmed by a 2020 study which found over half of the reef's coral cover to have been lost between 1995 and 2017, with the effects of a widespread 2020 bleaching event not yet quantified.[13][14]\n\nThe Great Barrier Reef has long been known to and used by the Aboriginal Australian and Torres Strait Islander peoples, and is an important part of local groups' cultures and spirituality. The reef is a very popular destination for tourists, especially in the Whitsunday Islands and Cairns regions. Tourism is an important economic activity for the region, generating over AUD$3 billion per year.[15] In November 2014, Google launched Google Underwater Street View in 3D of the Great Barrier Reef.[16]\n\nA March 2016 report stated that coral bleaching was more widespread than previously thought, seriously affecting the northern parts of the reef as a result of warming ocean temperatures.[17] In October 2016, Outside published an obituary for the reef;[18] the article was criticised for being premature and hindering efforts to bolster the resilience of the reef.[19] In March 2017, the journal Nature published a paper showing that huge sections of an 800-kilometre (500 mi) stretch in the northern part of the reef had died in the course of 2016 of high water temperatures, an event that the authors put down to the effects of global climate change.[20] The percentage of baby corals being born on the Great Barrier Reef dropped drastically in 2018 and scientists are describing it as the early stage of a \"huge natural selection event unfolding\". Many of the mature breeding adults died in the bleaching events of 2016–17, leading to low coral birth rates. The types of corals that reproduced also changed, leading to a \"long-term reorganisation of the reef ecosystem if the trend continues.\"[21]\n\nThe Great Barrier Reef Marine Park Act 1975 (section 54) stipulates an Outlook Report on the Reef's health, pressures, and future every five years. The last report was published in 2019.[22] In March 2022, another mass bleaching event has been confirmed, which raised further concerns about the future of this reef system, especially when considering the possible effects of El Niño weather phenomenon.[23]\n\nThe Australian Institute of Marine Science conducts annual surveys of the Great Barrier Reef's status, and the 2022 report showed the greatest recovery in 36 years. It is mainly due to the regrowth of two-thirds of the reef by the fast-growing Acropora coral, which is the dominant coral there.[24]\n\nThe Great Barrier Reef has long been known to and used by the Aboriginal Australian and Torres Strait Islander peoples, and is an important part of local groups' cultures and spirituality.[citation needed][clarification needed]\n\nThe first European to sight the Great Barrier Reef was James Cook in 1770, who sailed and mapped the east coast of Australia. On 11 June 1770 Cook's ship, HMS Endeavour, ran aground on a shoal south of the present-day location of Cooktown, requiring seven weeks to repair.\n\nIt was Matthew Flinders who named the Great Barrier Reef, after his more detailed mapping of it in 1802.[25][26] Flinders used various terms to describe the reefs comprising what we now call the Great Barrier Reef including \"great reef\", for one such reef, \"barrier reef\", for any reef preventing a sailing vessel in, or waves from, the open sea, from reaching the coast, and \"Barrier Reefs\", for the collection of such reefs.[26]\n\nThe Great Barrier Reef is a distinct feature of the East Australian Cordillera division. It reaches from Torres Strait (between Bramble Cay, its northernmost island, and the south coast of Papua New Guinea) in the north to the unnamed passage between Lady Elliot Island (its southernmost island) and Fraser Island in the south. Lady Elliot Island is located 1,915 km (1,190 mi) southeast of Bramble Cay as the crow flies.[27] It includes the smaller Murray Islands.[28]\n\nThe plate tectonic theory indicates Australia has moved northwards at a rate of 7 cm (2.8 in) per year, starting during the Cenozoic.[29][30]: 18  Eastern Australia experienced a period of tectonic uplift, which moved the drainage divide in Queensland 400 km (250 mi) inland. Also during this time, Queensland experienced volcanic eruptions leading to central and shield volcanoes and basalt flows.[30]: 19  Some of these became volcanic islands.[30]: 26  After the Coral Sea Basin formed, coral reefs began to grow in the Basin, but until about 25 million years ago, northern Queensland was still in temperate waters south of the tropics – too cool to support coral growth.[30]: 27  The Great Barrier Reef's development history is complex; after Queensland drifted into tropical waters, it was largely influenced by reef growth and decline as sea level changed.[30]: 27–28 \n\nReefs can increase in diameter by 1 to 3 centimetres (0.39 to 1.18 in) per year, and grow vertically anywhere from 1 to 25 cm (0.39 to 9.84 in) per year; however, they grow only above a depth of 150 metres (490 ft) due to their need for sunlight, and cannot grow above sea level.[31] When Queensland edged into tropical waters 24 million years ago, some coral grew,[30]: 29  but a sedimentation regime quickly developed with erosion of the Great Dividing Range; creating river deltas, oozes and turbidites, unsuitable conditions for coral growth. 10 million years ago, the sea level significantly lowered, which further enabled sedimentation. The reef's substrate may have needed to build up from the sediment until its edge was too far away for suspended sediments to inhibit coral growth. In addition, approximately 400,000 years ago there was a particularly warm Interglacial period with higher sea levels and a 4 °C (7 °F) water temperature change.[30]: 37 \n\nThe land that formed the substrate of the current Great Barrier Reef was a coastal plain formed from the eroded sediments of the Great Dividing Range with some larger hills (most of which were themselves remnants of older reefs[32] or, in rare cases, volcanoes[30]: 26 ).[30]: 18 \nThe Reef Research Centre, a Cooperative Research Centre, has found coral 'skeleton' deposits that date back half a million years.[33] The Great Barrier Reef Marine Park Authority (GBRMPA) considers the earliest evidence of complete reef structures to have been 600,000 years ago.[34] According to the GBRMPA, the current, living reef structure is believed to have begun growing on the older platform about 20,000 years ago.[34] The Australian Institute of Marine Science agrees, placing the beginning of the growth of the current reef at the time of the Last Glacial Maximum. At around that time, sea level was 120 metres (390 ft) lower than it is today.[32]\n\nFrom 20,000 years ago until 6,000 years ago, sea level rose steadily around the world. As it rose, the corals could then grow higher on the newly submerged maritime margins of the hills of the coastal plain. By around 13,000 years ago the sea level was only 60 metres (200 ft) lower than the present day, and corals began to surround the hills of the coastal plain, which were, by then, continental islands. As the sea level rose further still, most of the continental islands were submerged. The corals could then overgrow the submerged hills, to form the present cays and reefs. Sea level here has not risen significantly in the last 6,000 years.[32] The CRC Reef Research Centre estimates the age of the present, living reef structure at 6,000 to 8,000 years old.[33] The shallow water reefs that can be seen in air-photographs and satellite images cover an area of 20,679 km2, most (about 80%) of which[35] has grown on top of limestone platforms that are relics of past (Pleistocene) phases of reef growth.[30]\n\nThe remains of an ancient barrier reef similar to the Great Barrier Reef can be found in The Kimberley, Western Australia.[36]\n\nThe Great Barrier Reef World Heritage Area has been divided into 70 bioregions,[37] of which 30 are reef bioregions.[38][39] In the northern part of the Great Barrier Reef, ribbon reefs and deltaic reefs have formed; these structures are not found in the rest of the reef system.[33] A previously undiscovered reef, 500 metres tall and 1.5 km wide at the base, was found in the northern area in 2020.[40] There are no atolls in the system,[30]: 7  and reefs attached to the mainland are rare.[30]: 18 \n\nFringing reefs are distributed widely, but are most common towards the southern part of the Great Barrier Reef, attached to high islands, for example, the Whitsunday Islands. Lagoonal reefs are found in the southern Great Barrier Reef, and further north, off the coast of Princess Charlotte Bay. Crescentic reefs are the most common shape of reef in the middle of the system, for example the reefs surrounding Lizard Island. Crescentic reefs are also found in the far north of the Great Barrier Reef Marine Park, and in the Swain Reefs (20–22 degrees south). Planar reefs are found in the northern and southern parts, near Cape York Peninsula, Princess Charlotte Bay, and Cairns. Most of the islands on the reef are found on planar reefs.[30]: 158–160 \n\nWonky holes can have localised impact on the reef, providing upwellings of fresh water, sometimes rich in nutrients contributing to eutrophication.[41][42]\n\nNavigation through and around the reefs is a major challenge. More than 20 ships were recorded lost in the region between 1791 and 1850, Surveys between 1815 and 1860 by Phillip Parker King in the Mermaid, Francis Price Blackwood in HMS Fly, Owen Stanley in the Rattlesnake, and Henry Mangles Denham in the Herald led to considerable navigational improvements, as they outlined the contrasting advantages and perils of the Inner Route (between Australia's east coast and the western edge of the reefs) and the Outer Route, in the open sea.[43]\n\nThe Great Barrier Reef supports an extraordinary diversity of life, including many vulnerable or endangered species, some of which may be endemic to the reef system.[44][45]\n\nThirty species of cetaceans have been recorded in the Great Barrier Reef, including the dwarf minke whale, Indo-Pacific humpback dolphin, and the humpback whale. Large populations of dugongs live there.[45][46][47] More than 1,500 fish species live on the reef, including the clownfish, red bass, red-throat emperor, and several species of snapper and coral trout.[46] Forty-nine species mass spawn, while eighty-four other species spawn elsewhere in their range.[48] Seventeen species of sea snake live on the Great Barrier Reef in warm waters up to 50 metres (160 ft) deep and are more common in the southern than in the northern section. None found in the Great Barrier Reef World Heritage Area are endemic, nor are any endangered.[49]\n\nSix species of sea turtles come to the reef to breed: the green sea turtle, leatherback sea turtle, hawksbill turtle, loggerhead sea turtle, flatback turtle, and the olive ridley. The green sea turtles on the Great Barrier Reef have two genetically distinct populations, one in the northern part of the reef and the other in the southern part.[50] Fifteen species of seagrass in beds attract the dugongs and turtles,[46] and provide fish habitat.[30]: 133  The most common genera of seagrasses are Halophila and Halodule.[51]\n\nSaltwater crocodiles live in mangrove and salt marshes on the coast near the reef.[52] Nesting has not been reported, and the salt water crocodile population in the GBRWHA is wide-ranging but low density.[49] Around 125 species of shark, stingray, skates or chimaera live on the reef.[53][54] Close to 5,000 species of mollusc have been recorded on the reef, including the giant clam and various nudibranchs and cone snails.[46] Forty-nine species of pipefish and nine species of seahorse have been recorded.[49] At least seven species of frog inhabit the islands.[55]\n\n215 species of birds (including 22 species of seabirds and 32 species of shorebirds) visit the reef or nest or roost on the islands,[30]: 450–451  including the white-bellied sea eagle and roseate tern.[46] Most nesting sites are on islands in the northern and southern regions of the Great Barrier Reef, with 1.4 to 1.7 million birds using the sites to breed.[56][57] The islands of the Great Barrier Reef also support 2,195 known plant species; three of these are endemic. The northern islands have 300–350 plant species which tend to be woody, whereas the southern islands have 200 which tend to be herbaceous; the Whitsunday region is the most diverse, supporting 1,141 species. The plants are propagated by birds.[55]\n\nThere are at least 330 species of ascidians on the reef system with the diameter of 1–10 cm (0.4–4 in). Between 300 and 500 species of bryozoans live on the reef.[54] Four hundred coral species, both hard corals and soft corals inhabit the reef.[46] The majority of these spawn gametes, breeding in mass spawning events that are triggered by the rising sea temperatures of spring and summer, the lunar cycle, and the diurnal cycle. Reefs in the inner Great Barrier Reef spawn during the week after the full moon in October, while the outer reefs spawn in November and December.[58] Its common soft corals belong to 36 genera.[59] Five hundred species of marine algae or seaweed live on the reef,[46] including thirteen species of genus Halimeda, which deposit calcareous mounds up to 100 metres (110 yd) wide, creating mini-ecosystems on their surface which have been compared to rainforest cover.[30]: 185 \n\nClimate change, pollution, crown-of-thorns starfish and fishing are the primary threats to the health of this reef system. Other threats include shipping accidents, oil spills, and tropical cyclones.[60] Skeletal Eroding Band, a disease of bony corals caused by the protozoan Halofolliculina corallasia, affects 31 coral species.[61] According to a 2012 study by the National Academy of Sciences, since 1985, the Great Barrier Reef has lost more than half of its corals with two-thirds of the loss occurring from 1998 due to the factors listed before.[62] In 2022, the northern and central parts of the reef had the highest amount of coral cover since monitoring began, but the cover in the southern part had decreased and  bleaching events occurred more frequently.[63]\n\nThe Great Barrier Reef Marine Park Authority considers the greatest threat to the Great Barrier Reef to be climate change, causing ocean warming which increases coral bleaching.[64][65] Mass coral bleaching events due to marine heatwaves occurred in the summers of 1998, 2002, 2006, 2016, 2017 and 2020,[66][13][67] and coral bleaching is expected to become an annual occurrence.[68] In 2020, a study found that the Great Barrier Reef has lost more than half of its corals since 1995 due to warmer seas driven by climate change.[67][69] As global warming continues, corals will not be able to keep up with increasing ocean temperatures. Coral bleaching events lead to increased disease susceptibility, which causes detrimental ecological effects for reef communities.[70]\n\nIn July 2017 UNESCO published in a draft decision, expressing serious concern about the impact of coral bleaching on the Great Barrier Reef.  The draft decision also warned Australia that it will not meet the targets of the Reef 2050 report without considerable work to improve water quality.[71]\n\nClimate change has implications for other forms of reef life – some fish's preferred temperature range leads them to seek new habitat, thus increasing chick mortality in predatory seabirds. Climate change will also affect the sea turtle's population and available habitat.[72]\n\nBleaching events in benthic coral communities (deeper than 20 metres or 66 feet) in the Great Barrier reef are not as well documented as those at shallower depths, but recent research has shown that benthic communities are just as negatively impacted in the face of rising ocean temperatures. Five Great Barrier Reef species of large benthic corals were found bleached under elevated temperatures, affirming that benthic corals are vulnerable to thermal stress.[73]\n\nA threat for the Great Barrier Reef is the rising levels of ocean acidification. Ocean acidification occurs when excess atmospheric carbon dioxide gets absorbed into the ocean. This causes a decrease in the pH and this alters the chemistry of the ocean's water. This reduces the amount of aragonite, a key mineral for coral to grow, in the water. The Great Barrier Reef is predicted to lose aragonite at a rate of about 0.1 per decade this century.[74] The acidic water breaks down the materials that coral and shell building creatures need to grow. [75]\n\nAnother key threat faced by the Great Barrier Reef is pollution and declining water quality. The rivers of north-eastern Australia pollute the Reef during tropical flood events. Over 90% of this pollution comes from farm runoff.[76] 80% of the land adjacent to the Great Barrier Reef is used for farming including intensive cropping of sugar cane, and major beef cattle grazing. Farming practices damage the reef due to overgrazing, increased run-off of agricultural sediments, nutrients and chemicals including fertilisers, herbicides and pesticides representing a major health risk for the coral and biodiversity of the reefs.[77]\n\nAccording to a 2016 report, while higher regulation contributes to less overall pollution from \"other land uses, such as industrial, mining, port development, dredging and urban development\", these can still be locally significant.[78] Sediments containing high levels of copper and other heavy metals sourced from the Ok Tedi Mine in Papua New Guinea are a potential pollution risk for the far northern Great Barrier Reef and Torres Strait regions.[79] Some 67% of corals died in the reef's worst-hit northern section, the ARC Centre of Excellence for Coral Reef Studies report said.[80]\n\nThe runoff problem is exacerbated by the loss of coastal wetlands which act as a natural filter for toxins and help deposit sediment.[81][82][83] It is thought that the poor water quality is due to increased light and oxygen competition from algae.[84]\n\nFarming fertiliser runoff release nitrogen, phosphorus, and potassium into the oceanic ecosystem, and these limiting nutrients cause massive algal growth which eventually leads to a reduction in oxygen available for other creatures in a process called eutrophication. This decreases the biodiversity in the affected areas, altering the species composition. A study by Katharina Fabricius and Glen Death of Australian Institute of Marine Science found that hard corals numbers were almost double on reefs that were far from agricultural areas.[77]\n\nFertilizers also increase the amount of phytoplankton available for the crown-of-thorns starfish larvae to consume. A study showed that a doubling of the chlorophyll in the water leads to a tenfold increase in the crown-of-thorns starfish larvae's survival rate.[77]\n\nSediment runoff from farming carries chemicals into the reef environment also reduces the amount of light available to the corals decreasing their ability to extract energy from their environment.[77]\n\nPesticides used in farming are made up of heavy metals such as lead, mercury, arsenic and other toxins are released into the wider environment due to erosion of farm soil, which has a detrimental effect on the coral.[77]\n\nMining company Queensland Nickel discharged nitrate-laden water into the Great Barrier Reef in 2009 and 2011 – on the later occasion releasing 516 tonnes (508 long tons; 569 short tons) of waste water. The Great Barrier Reef Marine Park Authority (GBRMPA) stated \"We have strongly encouraged the company to investigate options that do not entail releasing the material to the environment and to develop a management plan to eliminate this potential hazard; however, GBRMPA does not have legislative control over how the Yabulu tailings dam is managed\".[85]\n\nThe crown-of-thorns starfish preys on coral polyps. Large outbreaks of these starfish can devastate reefs. In 2000, an outbreak contributed to a loss of 66% of live coral cover on sampled reefs in a study by the Reef Research Centre (RRC).[86] Outbreaks are believed to occur in natural cycles, worsened by poor water quality and overfishing of the starfish's predators.[86][87]\n\nThe unsustainable overfishing of keystone species, such as the giant Triton, can disrupt food chains vital to reef life. Fishing also impacts the reef through increased water pollution from boats, by-catch of unwanted species (such as dolphins and turtles) and habitat destruction from trawling, anchors and nets.[88] As of the middle of 2004, approximately one-third of the Great Barrier Reef Marine Park is protected from species removal of any kind, including fishing, without written permission.[89]\n\nShipping accidents are a pressing concern, as several commercial shipping routes pass through the Great Barrier Reef.[90]\nAlthough the route through the Great Barrier Reef is not easy, reef pilots consider it safer than outside the reef in the event of mechanical failure, since a ship can sit safely while being repaired.[91] There have been over 1,600 known shipwrecks in the Great Barrier Reef region.[92]\nOn 3 April 2010, the bulk coal carrier Shen Neng 1 ran aground on Douglas Shoals,[93] spilling up to four tonnes of oil into the water and causing extensive damage to the reef.[94]\n\nThe government of Queensland has a \"shark control\" program (shark culling) that deliberately kills sharks throughout Queensland, including in the Great Barrier Reef.[95][96] Environmentalists[who?] and scientists[clarification needed] say that this program harms the marine ecosystem; they also say it is \"outdated, cruel and ineffective\".[96] The Queensland \"shark control\" program uses shark nets and drum lines with baited hooks to kill sharks in the Great Barrier Reef – there are 173 lethal drum lines in the Great Barrier Reef.[95][96][97] In Queensland, sharks found alive on the baited hooks are shot.[98] Queensland's \"shark control\" program killed about 50,000 sharks from 1962 to 2018.[99] Also, Queensland's \"shark control\" program has also killed many other animals (such as dolphins and turtles) – the program killed 84,000 marine animals from 1962 to 2015, including in the Great Barrier Reef.[100] In 2018, Humane Society International filed a lawsuit against the government of Queensland to stop shark culling in the Great Barrier Reef.[97]\n\nIn March 2015, the Australian and Queensland's governments formed a plan for the protection and preservation of the reef's universal heritage until 2050. This 35 years plan, titled \"Reef 2050 Plan\" is a document proposing possible measures for the long-term management of the pollution, climate change and other issues that threaten the life span and value of this global heritage. The plan contains all the elements for measurement and improvements, including; long-term sustainability plan, water quality improvement plan and the investment plan for the protection and preservation of The Reef until 2050.[101]\n\nHowever, whereas the 2050 plan aims to incorporate protective measures such as improving water quality, reef restoration, killing of predatory starfish, it does not incorporate additional measures to address what may be the root cause the problem – climate change, which is caused by greenhouse gas emissions. As such, experts doubted whether it would be enough to save the fragile environment.[71][102][103] Another issue is that the time left to the 1.5 °C warming threshold (the temperature limit that coral reefs can still cope with[104]) is very limited.[105]\n\nAs part of the Reef 2050 plan, an AUD$443 million grant was given to the Great Barrier Reef Foundation in 2018. The announcement of the grant was subject to backlash as the grant had avoided proper tender and transparency processes.\n\nThe Great Barrier Reef contributes to the overall wellbeing of the marine biome.[106]  Numerous species of aquatic plants, fish and megafauna use the reef for feeding, shelter and mating.[107]  Threats such as ocean acidification, pollution runoff and outbreaks of destructive species like the crown-of-thorns starfish have brought about the decline of this ecosystem.[108] These threats to the reef are not only a danger to the organisms inhabiting it, but also the economy of this region, a large part of which relies on revenue from ecotourism of the Great Barrier Reef.[109]\n\nThe Australian government has had the goal of protecting this World Heritage Site since 1972 when they created The Australian Institute of Marine Science.[110] The Australian and Queensland governments have contributed about $142.5 million to their National Environmental Science Program which is how they've collected much of their data regarding threats to the Great Barrier Reef.[110] In addition, the Reef 2050 Water Quality Improvement Plan was announced in 2018 in order to help transition local communities, agricultural organizations and industries to more sustainable practices.[111]  This plan will join the Queensland government and the Great Barrier Reef Marine Park Authority to manage the amounts of runoff that reach the Great Barrier Reef as well as mitigating crown-of-thorns starfish population flare-ups.[111]\n\nThe Great Barrier Reef has long been known to and used by the Aboriginal Australian and Torres Strait Islander peoples. Aboriginal Australians have been living in the area for at least 40,000 years,[112] and Torres Strait Islanders since about 10,000 years ago.[113] For these 70 or so clan groups, the reef is also an important cultural feature.[114]\n\nIn 1768, Louis de Bougainville encountered the reef, but did not explore it.[115] On 11 June 1770, HM Bark Endeavour, captained by explorer James Cook, ran aground on the Great Barrier Reef, sustaining considerable damage. Lightening the ship and re-floating it during an incoming tide eventually saved it.[116] One of the most famous wrecks was HMS Pandora, which sank on 29 August 1791, killing 35 men. The Queensland Museum has led archaeological digs to wreck of Pandora since 1983.[117] Because the reef had no atolls, it was largely unstudied in the 19th century.[30]: 7  During this time, some of the reef's islands were mined for deposits of guano, and lighthouses were built as beacons throughout the system.[30]: 452  as in Raine Island, the earliest example.[118] In 1922, the Great Barrier Reef Committee began carrying out much of the early research on the reef.[30]: 9 \n\nRoyal Commissions disallowed oil drilling in the Great Barrier Reef, in 1975 the Government of Australia created the Great Barrier Reef Marine Park and prohibited various activities.[119] The Great Barrier Reef Marine Park does not include the entire Great Barrier Reef Province.[27] The park is managed, in partnership with the Government of Queensland, through the Great Barrier Reef Marine Park Authority to ensure that it is used in a sustainable manner. A combination of zoning, management plans, permits, education and incentives (such as eco-tourism certification) are employed in the effort to conserve the reef.[60][120]\n\nIn 1999, the Australian Parliament passed the Environment Protection and Biodiversity Conservation Act, which improved the operation of national environmental law by providing guidance about regional biodiversity conservation priorities. The marine bioregional planning process came from the implementation of this law. This process conserves marine biodiversity by considering the whole ecosystem a species is in and how different species interact in the marine environment.\n\nThere are two steps to this process. The first step is to identify regional conservation priorities in the five (currently) different marine regions. The second step is to identify marine reserves (protected areas or marine parks) to be added to Australia's National Representative System of Marine Protected Areas. Like protected areas on land, marine reserves are created to protect biodiversity for generations to come. Marine reserves are identified based on criteria written in a document created by Australian and New Zealand Environment and Conservation Council called \"Guidelines for establishing the national representative system of marine protected areas\", also known as just \"the Guidelines\". These guidelines are nationally recognised and implemented at the local level based on the Australian policy for implementation outlined in the \"Goals and Principles for the Establishment of the National Representative System of Marine Protected Areas in Commonwealth Waters\". These policies are in place to make sure that a marine reserve is only added to the NRSMPA after careful evaluation of different data.\n\nThe priorities for each region are created based on human and environmental threats and the Marine Bioregional Plans are drafted to address these priorities. To assess different region's priorities, three steps are taken, first, a bioregional profile is created, second, a bioregional plan is drafted, and third, the plan is finalised. After the plan is finalised, activity in different bioregions may become limited based on particular threats an activity may pose.[121]\n\nIn 2001, the GBRMPA released a report about the declining water quality in the Great Barrier Reef and detailed the importance of this issue. In response to this report, in 2003, the Australian and Queensland governments launched a joint initiative to improve the quality of water entering the Great Barrier Reef.[122] The decline in the quality of water over the past 150 years (due to development) has contributed to coral bleaching, algal blooms, and pesticide pollution. These forms of pollution have made the reef less resilient to climate change.\n\nWhen the plan was introduced in October 2003, it originally contained 65 actions built on previous legislation. Their immediate goal was to halt and reverse the decline in water quality entering the reef by 2013. By 2020, they hope that the quality of the water entering in the reef improves enough so that it doesn't have a detrimental impact on the health of the Great Barrier Reef. To achieve these goals they decided to reduce pollutants in the water entering the reef and to rehabilitate and conserve areas of the reef that naturally help reduce water pollutants. To achieve the objectives described above, this plan focuses on non-point sources of pollution, which cannot be traced to a single source such as a waste outlet.\n\nThe plan specifically targets nutrients, pesticides and sediment that make their way into the reef as a result of agricultural activities. Other non-point sources of pollution that are attributed to urban areas are covered under different legislation. In 2009, the plan was updated. The updated version states that to date, none of the efforts undertaken to improve the quality of water entering the reef has been successful. The new plan attempts to address this issue by \"targeting priority outcomes, integrating industry and community initiatives and incorporating new policy and regulatory frameworks (Reef Plan 5)\". This updated version has improved the clarity of the previous plan and targets set by that plan, have improved accountability and further improved monitoring and assessment. The 2009 report found that 41 out of the 65 actions met their original goals, however, 18 were not progressing well according to evaluation criteria as well as 6 were rated as having unsatisfactory levels of progress.\n\nSome key achievements made since the plan's initial passing in 2003 were the establishment of the Reef Quality Partnership to set targets, report findings and monitor progress towards targets, improved land condition by landowners was rewarded with extended leases, Water Quality Improvement Plans were created to identify regional targets and identified management changes that needed to be made to reach those targets, Nutrient Management Zones have been created to combat sediment loss in particular areas, education programs have been started to help gather support for sustainable agriculture, changes to land management practices have taken place through the implementation of the Farm Management Systems and codes of practice, the creation of the Queensland Wetland program and other achievements were made to help improve the water quality flowing into the coral reefs.\n\nA taskforce of scientists was also created to assess the impact of different parts of the plan on the quality of water flowing into the coral reefs. They found that many of the goals have yet to be reached but found more evidence that states that improving the water quality of the Great Barrier Reef will improve its resilience to climate change. The Reefocus summit in 2008, which is also detailed in the report, came to similar conclusions. After this, a stakeholder working group was formed that worked between several groups as well as the Australian and Queensland governments to update reef goals and objectives. The updated version of the plan focuses on strategic priority areas and actions to achieve 2013 goals. Also quantitative targets have been made to critically assess whether targets are being met.\n\nSome examples of the water quality goals outlined by this plan are that by 2013, there will be a 50% reduction in nitrogen and phosphorus loads at the end of catchments and that by 2020, there will be a reduction in sediment load by 20%. The plan also outlines a number of steps that must be taken by landholders to help improve grazing, soil, nutrient, and chemical management practices. There are also a number of supporting initiatives to take place outlined in the plan to help create a framework to improve land use practices which will in turn improve water quality.\n\nThrough these means the governments of Australia and Queensland hope to improve water quality by 2013. The 2013 outlook report and revised water quality plan will assess what needs to be done in the future to improve water quality and the livelihoods of the wildlife that resides there.[123]\n\nIn July 2004, a new zoning plan took effect for the entire Marine Park, and has been widely acclaimed as a new global benchmark for marine ecosystem conservation.[124] The rezoning was based on the application of systematic conservation planning techniques, using marxan software.[125] While protection across the Marine Park was improved, the highly protected zones increased from 4.5% to over 33.3%.[126] At the time, it was the largest Marine Protected Area in the world, although in 2006, the new Northwestern Hawaiian Islands National Monument became the largest.[127]\n\nIn 2006, a review of the Great Barrier Reef Marine Park Act of 1975 recommended that there should be no further zoning plan changes until 2013, and that every five years, a peer-reviewed outlook report should be published, examining the reef's health, management, and environmental pressures.[5][128] In each outlook report, several assessments are required. Each assessment has a set of assessment criteria that allows for better presentation of available evidence. Each assessment is judged by these criteria and given a grade. Every outlook report follows the same judging and grading process so that information can be tracked over time. No new research is done to produce the report. Only readily available information goes into the report so little of what is known about the Reef is actually featured in each outlook report.[129]\n\nIn December 2013, Greg Hunt, the Australian environment minister, approved a plan for dredging to create three shipping terminals as part of the construction of a coalport. According to corresponding approval documents, the process will create around 3 million cubic metres of dredged seabed that will be dumped within the Great Barrier Reef marine park area.[130]\n\nOn 31 January 2014, the GBRMPA issued a dumping permit that will allow three million cubic metres of sea bed from Abbot Point, north of Bowen, to be transported and unloaded in the waters of the Great Barrier Reef Marine Park. Potential significant harms have been identified in relation to dredge spoil and the process of churning up the sea floor in the area and exposing it to air: firstly, new research shows the finer particles of dredge spoil can cloud the water and block sunlight, thereby starving sea grass and coral up to distances of 80 km away from the point of origin due to the actions of wind and currents. Furthermore, dredge spoil can literally smother reef or sea grass to death, while storms can repeatedly resuspend these particles so that the harm caused is ongoing; secondly, disturbed sea floor can release toxic substances into the surrounding environment.[131]\n\nThe dredge spoil from the Abbot Point port project is to be dumped 24 kilometres (15 mi) away, near Bowen in north Queensland, and the approval from the Authority will result in the production of an extra 70 million tonnes of coal annually, worth between A$1.4 billion and $2.8 billion.[131] Authority chairman, Dr Russell Reichelt, stated after the confirmation of the approval:\n\n\nThis approval is in line with the agency's view that port development along the Great Barrier Reef coastline should be limited to existing ports. As a deepwater port that has been in operation for nearly 30 years, Abbot Point is better placed than other ports along the Great Barrier Reef coastline to undertake expansion as the capital and maintenance dredging required will be significantly less than what would be required in other areas. It's important to note the seafloor of the approved disposal area consists of sand, silt and clay and does not contain coral reefs or seagrass beds.[131]\nThe approval was provided with a corresponding set of 47 new environmental conditions that include the following:\n\nThe Australian Federal Government announced on 13 November that there would now be a ban on the dumping of dredge spoil in the Great Barrier Reef Marine Park. The World Heritage Committee asked Environment Minister Greg Hunt to investigate alternative options to dump on land instead. The Queensland government and the Commonwealth have now accepted the alternative option and advice from The World Heritage Committee and will now commence dumping on land.\n[133]\n\nDue to its vast biodiversity, warm clear waters and accessibility from the tourist boats called \"live aboards\", the reef is a very popular destination, especially for scuba divers. Tourism on the Great Barrier Reef is concentrated in Cairns and also The Whitsundays due to their accessibility. These areas make up 7–8% of the park's area.[60] The Whitsundays and Cairns have their own Plans of Management.[134] Many cities along the Queensland coast offer daily boat trips. Several continental and coral cay islands are now resorts, including Green Island and Lady Elliot Island. As of 1996, 27 islands on the Great Barrier Reef supported resorts.[60]\n\nIn 1996, most of the tourism in the region was domestically generated and the most popular visiting times were during the Australian winter. At this time, it was estimated that tourists to the Great Barrier Reef contributed A$776 million per annum.[135] As the largest commercial activity in the region, it was estimated in 2003 that tourism generated over A$4 billion annually,[136] and the 2005 estimate increased to A$5.1 billion.[134] A Deloitte report published by the Great Barrier Reef Marine Park Authority in March 2013 states that the Reef's 2,000 kilometres of coastline attracts tourism worth A$6.4 billion annually and employs more than 64,000 people.[137]\n\nApproximately two million people visit the Great Barrier Reef each year.[138] Although most of these visits are managed in partnership with the marine Tourism industry, there is a concern among the general public that tourism is harmful to the Great Barrier Reef.[60]\n\nA variety of boat tours and cruises are offered, from single day trips, to longer voyages. Boat sizes range from dinghies to superyachts.[139] Glass-bottomed boats and underwater observatories are also popular, as are helicopter flights.[140][141] By far, the most popular tourist activities on the Great Barrier Reef are snorkelling and diving, for which pontoons are often used, and the area is often enclosed by nets. The outer part of the Great Barrier Reef is favoured for such activities, due to water quality.[citation needed]\n\nManagement of tourism in the Great Barrier Reef is geared towards making tourism ecologically sustainable. A daily fee is levied that goes towards research of the Great Barrier Reef.[134] This fee ends up being 20% of the GBRMPA's income.[142]\nPolicies on cruise ships, bareboat charters, and anchorages limit the traffic on the Great Barrier Reef.[134]\n\nThe problems that surround ecotourism in the Great Barrier Reef revolve around permanent tourism platforms. Platforms are large, ship-like vessels that act as a base for tourists while scuba diving and snorkelling in the Great Barrier Reef. Seabirds will land on the platforms and defecate which will eventually be washed into the sea. The feces carry nitrogen, phosphorus and often DDT and mercury, which cause aspergillosis, yellow-band disease, and black band disease. Areas without tourism platforms have 14 out of 9,468 (1.1%) diseased corals versus areas with tourism platforms that have 172 out of 7,043 (12%) diseased corals.[143] Tourism is a major economic activity for the region. Thus, while non-permanent platforms could be possible in some areas, overall, permanent platforms are likely a necessity. Solutions have been suggested to siphon bird waste into gutters connecting to tanks helping lower runoff that causes coral disease.[144]\n\nThe Great Barrier Reef Marine Park Authority has also placed many permanent anchorage points around the general use areas. These act to reduce damage to the reef due to anchoring destroying soft coral, chipping hard coral, and disturbing sediment as it is dragged across the bottom. Tourism operators also must comply with speed limits when travelling to or from tourist destinations, to prevent excessive wake from the boats disturbing the reef ecosystem.[citation needed]\n\nThe fishing industry in the Great Barrier Reef, controlled by the Queensland Government, is worth A$1 billion annually.[15] It employs approximately 2000 people, and fishing in the Great Barrier Reef is pursued commercially, for recreation, and as a traditional means for feeding one's family.[114]\n\nUnder the Native Title Act 1993, native title holders retain the right to legally hunt dugongs and green turtles for \"personal, domestic or non-commercial communal needs\".[145][unreliable source?]\n\nFour traditional owners groups agreed to cease the hunting of dugongs in the area in 2011 due to their declining numbers, partially accelerated by seagrass damage from Cyclone Yasi.[146]\n\nCave dive sites:\n"
    },
    {
        "title": "Amazon rainforest",
        "url": "https://en.wikipedia.org/wiki/Amazon_rainforest",
        "content": "\n\nThe Amazon rainforest,[a] also called Amazon jungle or Amazonia, is a moist broadleaf tropical rainforest in the Amazon biome that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 km2 (2,700,000 sq mi),[2] of which 6,000,000 km2 (2,300,000 sq mi) are covered by the rainforest.[3] This region includes territory belonging to nine nations and 3,344 indigenous territories.\n\nThe majority of the forest, 60%, is in Brazil, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Bolivia, Ecuador, French Guiana, Guyana, Suriname, and Venezuela. Four nations have \"Amazonas\" as the name of one of their first-level administrative regions, and France uses the name \"Guiana Amazonian Park\" for French Guiana's protected rainforest area. The Amazon represents over half of the total area of remaining rainforests on Earth,[4] and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees in about 16,000 species.[5]\n\nMore than 30 million people of 350 different ethnic groups live in the Amazon, which are subdivided into 9 different national political systems and 3,344 formally acknowledged indigenous territories. Indigenous peoples make up 9% of the total population, and 60 of the groups remain largely isolated.[6]\n\nLarge scale deforestation is occurring in the forest, creating different harmful effects. Economic losses due to deforestation in Brazil could be approximately 7 times higher in comparison to the cost of all commodities produced through deforestation. In 2023, the World Bank published a report proposing a non-deforestation based economic program in the region.[7][8]\n\nThe name Amazon is said to arise from a war Francisco de Orellana fought with the Tapuyas and other tribes. The women of the tribe fought alongside the men, as was their custom.[9] Orellana derived the name Amazonas from the Amazons of Greek mythology, described by Herodotus and Diodorus.[9]\n\nBased on archaeological evidence from an excavation at Caverna da Pedra Pintada, human inhabitants first settled in the Amazon region at least 11,200 years ago.[11] Subsequent development led to late-prehistoric settlements along the periphery of the forest by AD 1250, which induced alterations in the forest cover.[12]\n\nFor a long time, it was thought that the Amazon rainforest was never more than sparsely populated, as it was impossible to sustain a large population through agriculture given the poor soil. Archeologist Betty Meggers was a prominent proponent of this idea, as described in her book Amazonia: Man and Culture in a Counterfeit Paradise. She claimed that a population density of 0.2 inhabitants per square kilometre (0.52/sq mi) is the maximum that can be sustained in the rainforest through hunting, with agriculture needed to host a larger population.[13] However, recent anthropological findings have suggested that the region was actually densely populated. The Upano Valley sites in present-day eastern Ecuador predate all known complex Amazonian societies.[14]\n\nSome 5 million people may have lived in the Amazon region in AD 1500, divided between dense coastal settlements, such as that at Marajó, and inland dwellers.[15] Based on projections of food production, one estimate suggests over 8 million people living in the Amazon in 1492.[16] By 1900, the native indigenous population had fallen to 1 million and by the early 1980s it was less than 200,000.[15]\n\nThe first European to travel the length of the Amazon River was Francisco de Orellana in 1542.[17] The BBC's Unnatural Histories presents evidence that Orellana, rather than exaggerating his claims as previously thought, was correct in his observations that a complex civilization was flourishing along the Amazon in the 1540s. The Pre-Columbian agriculture in the Amazon Basin was sufficiently advanced to support prosperous and populous societies. It is believed that civilization was later devastated by the spread of diseases from Europe, such as smallpox.[18] This civilization was investigated by the British explorer Percy Fawcett in the early twentieth century.  The results of his expeditions were inconclusive, and he disappeared mysteriously on his last trip.  His name for this lost civilization was the City of Z.[citation needed]\n\nSince the 1970s, numerous geoglyphs have been discovered on deforested land dating between AD 1–1250, furthering claims about Pre-Columbian civilizations.[19][20] Ondemar Dias is accredited with first discovering the geoglyphs in 1977, and Alceu Ranzi is credited with furthering their discovery after flying over Acre.[18][21] The BBC's Unnatural Histories presented evidence that the Amazon rainforest, rather than being a pristine wilderness, has been shaped by man for at least 11,000 years through practices such as forest gardening and terra preta.[18] Terra preta is found over large areas in the Amazon forest; and is now widely accepted as a product of indigenous soil management. The development of this fertile soil allowed agriculture and silviculture in the previously hostile environment; meaning that large portions of the Amazon rainforest are probably the result of centuries of human management, rather than naturally occurring as has previously been supposed.[22] In the region of the Xingu tribe, remains of some of these large settlements in the middle of the Amazon forest were found in 2003 by Michael Heckenberger and colleagues of the University of Florida. Among those were evidence of roads, bridges and large plazas.[23]\n\nIn the Amazonas, there has been fighting and wars between the neighboring tribes of the Jivaro. Several tribes of the Jivaroan group, including the Shuar, practised headhunting for trophies and headshrinking.[24] The accounts of missionaries to the area in the borderlands between Brazil and Venezuela have recounted constant infighting in the Yanomami tribes. More than a third of the Yanomamo males, on average, died from warfare.[25][when?]\n\nThe Munduruku were a warlike tribe that expanded along the Tapajós river and its tributaries and were feared by neighboring tribes. In the early 19th century, the Munduruku were pacified and subjugated by the Brazilians.[26]\n\nDuring the Amazon rubber boom it is estimated that diseases brought by immigrants, such as typhus and malaria, killed 40,000 native Amazonians.[27]\n\nIn the 1950s, Brazilian explorer and defender of indigenous people, Cândido Rondon, supported the Villas-Bôas brothers' campaign, which faced strong opposition from the government and the ranchers of Mato Grosso and led to the establishment of the first Brazilian National Park for indigenous people along the Xingu River in 1961.[28]\n\nIn 1961, British explorer Richard Mason was killed by an uncontacted Amazon tribe known as the Panará.[29]\n\nThe Matsés made their first permanent contact with the outside world in 1969. Before that date, they were effectively at-war with the Peruvian government.[30]\n\nNine countries share the Amazon basin—most of the rainforest, 58.4%, is contained within the borders of Brazil. The other eight countries are Peru with 12.8%, Bolivia with 7.7%, Colombia with 7.1%, Venezuela with 6.1%, Guyana with 3.1%, Suriname with 2.5%, French Guiana with 1.4% and Ecuador with 1%.[31]\n\nThe rainforest likely formed during the Eocene era (from 56 million years to 33.9 million years ago). It appeared following a global reduction of tropical temperatures when the Atlantic Ocean had widened sufficiently to provide a warm, moist climate to the Amazon basin. The rainforest has been in existence for at least 55 million years, and most of the region remained free of savanna-type biomes at least until the current ice age when the climate was drier and savanna more widespread.[32][33]\n\nFollowing the Cretaceous–Paleogene extinction event, the extinction of the dinosaurs and the wetter climate may have allowed the tropical rainforest to spread out across the continent. From 66 to 34 Mya, the rainforest extended as far south as 45°. Climate fluctuations during the last 34 million years have allowed savanna regions to expand into the tropics. During the Oligocene, for example, the rainforest spanned a relatively narrow band. It expanded again during the Middle Miocene, then retracted to a mostly inland formation at the last glacial maximum.[34] However, the rainforest still managed to thrive during these glacial periods, allowing for the survival and evolution of a broad diversity of species.[35]\n\nDuring the mid-Eocene, it is believed that the drainage basin of the Amazon was split along the middle of the continent by the Purus Arch. Water on the eastern side flowed toward the Atlantic, while to the west water flowed toward the Pacific across the Amazonas Basin. As the Andes Mountains rose, however, a large basin was created that enclosed a lake; now known as the Solimões Basin. Within the last 5–10 million years, this accumulating water broke through the Purus Arch, joining the easterly flow toward the Atlantic.[36][37]\n\nThere is evidence that there have been significant changes in the Amazon rainforest vegetation over the last 21,000 years through the last glacial maximum (LGM) and subsequent deglaciation. Analyses of sediment deposits from Amazon basin paleolakes and the Amazon Fan indicate that rainfall in the basin during the LGM was lower than for the present, and this was almost certainly associated with reduced moist tropical vegetation cover in the basin.[38] In present day, the Amazon receives approximately 9 feet of rainfall annually. There is a debate, however, over how extensive this reduction was. Some scientists argue that the rainforest was reduced to small, isolated refugia separated by open forest and grassland;[39] other scientists argue that the rainforest remained largely intact but extended less far to the north, south, and east than is seen today.[40] This debate has proved difficult to resolve because the practical limitations of working in the rainforest mean that data sampling is biased away from the center of the Amazon basin, and both explanations are reasonably well supported by the available data.\n\nMore than 56% of the dust fertilizing the Amazon rainforest comes from the Bodélé depression in Northern Chad in the Sahara desert. The dust contains phosphorus, important for plant growth. The yearly Sahara dust replaces the equivalent amount of phosphorus washed away yearly in Amazon soil from rains and floods.[41]\n\nNASA's CALIPSO satellite has measured the amount of dust transported by wind from the Sahara to the Amazon: an average of 182 million tons of dust are windblown out of the Sahara each year (some dust falls into the Atlantic), 15% of which of falls over the Amazon basin (22 million tons of it consisting of phosphorus).[42]\n\nCALIPSO uses a laser range finder to scan the Earth's atmosphere for the vertical distribution of dust and other aerosols. and regularly tracks the Sahara-Amazon dust plume. CALIPSO has measured variations in the dust amounts transported – an 86 percent drop between the highest amount of dust transported in 2007 and the lowest in 2011. This is possibly causing by rainfall variations is the Sahel, a strip of semi-arid land on the southern border of the Sahara..[43]\n\nAmazon phosphorus also comes as smoke due to biomass burning in Africa.[44][45]\n\nWet tropical forests are the most species-rich biome, and tropical forests in the Americas are consistently more species rich than the wet forests in Africa and Asia.[46] As the largest tract of tropical rainforest in the Americas, the Amazonian rainforests have unparalleled biodiversity. One in ten known species in the world lives in the Amazon rainforest.[47] This constitutes the largest collection of living plants and animal species in the world.[48]\n\nThe region is home to about 2.5 million insect species,[49] tens of thousands of plants, and some 2,000 birds and mammals. To date, at least 40,000 plant species,[50] 2,200 fishes,[51] 1,294 birds, 427 mammals, 428 amphibians, and 378 reptiles have been scientifically classified in the region.[52] One in five of all bird species are found in the Amazon rainforest, and one in five of the fish species live in Amazonian rivers and streams. Scientists have described between 96,660 and 128,843 invertebrate species in Brazil alone.[53]\n\nThe biodiversity of plant species is the highest on Earth with one 2001 study finding a quarter square kilometer (62 acres) of Ecuadorian rainforest supports more than 1,100 tree species.[54] A study in 1999 found one square kilometer (247 acres) of Amazon rainforest can contain about 90,790 tonnes of living plants. The average plant biomass is estimated at 356 ± 47 tonnes per hectare.[55] To date, an estimated 438,000 species of plants of economic and social interest have been registered in the region with many more remaining to be discovered or catalogued.[56] The total number of tree species in the region is estimated at 16,000.[5]\n\nThe green leaf area of plants and trees in the rainforest varies by about 25% as a result of seasonal changes. Leaves expand during the dry season when sunlight is at a maximum, then undergo abscission in the cloudy wet season. These changes provide a balance of carbon between photosynthesis and respiration.[57]\n\nEach hectare of the Amazon rainforest contains around 1 billion of invertebrates. The amount of species per hectare in the Amazon rainforest can be presented in the next table:[58]\n\nThe rainforest contains several species that can pose a hazard. Among the largest predatory creatures are the black caiman, jaguar, cougar, and anaconda. In the river, electric eels can produce an electric shock that can stun or kill, while piranha are known to bite and injure humans.[59] Various species of poison dart frogs secrete lipophilic alkaloid toxins through their flesh. There are also numerous parasites and disease vectors. Vampire bats dwell in the rainforest and can spread the rabies virus.[60] Malaria, yellow fever and dengue fever can also be contracted in the Amazon region.\n\nThe biodiversity in the Amazon is becoming increasingly threatened, primarily by habitat loss from deforestation as well as increased frequency of fires. Over 90% of Amazonian plant and vertebrate species (13,000–14,000 in total) may have been impacted to some degree by fires.[61]\n\nDeforestation is the conversion of forested areas to non-forested areas. The main sources of deforestation in the Amazon are human settlement and the development of the land.[64] In 2022, about 20% of the Amazon rainforest has already been deforested and a further 6% was \"highly degraded\".[65] Research suggests that upon reaching about 20–25% (hence 0–5% more), the tipping point to flip it into a non-forest ecosystem – degraded savannah – (in eastern, southern and central Amazonia) will be reached.[66][67][68] This process of savanisation would take decades to take full effect.[65]\n\nPrior to the early 1960s, access to the forest's interior was highly restricted, and the forest remained basically intact.[69] Farms established during the 1960s were based on crop cultivation and the slash and burn method. However, the colonists were unable to manage their fields and the crops because of the loss of soil fertility and weed invasion.[70] The soils in the Amazon are productive for just a short period of time, so farmers are constantly moving to new areas and clearing more land.[70] These farming practices led to deforestation and caused extensive environmental damage.[71] Deforestation is considerable, and areas cleared of forest are visible to the naked eye from outer space.\n\nIn the 1970s, construction began on the Trans-Amazonian highway. This highway represented a major threat to the Amazon rainforest.[72] The highway still has not been completed, limiting the environmental damage.\n\nBetween 1991 and 2000, the total area of forest lost in the Amazon rose from 415,000 to 587,000 km2 (160,000 to 227,000 sq mi), with most of the lost forest becoming pasture for cattle.[73] Seventy percent of formerly forested land in the Amazon, and 91% of land deforested since 1970, have been used for livestock pasture.[74][75] Currently, Brazil is the largest global producer of soybeans. New research however, conducted by Leydimere Oliveira et al., has shown that the more rainforest is logged in the Amazon, the less precipitation reaches the area and so the lower the yield per hectare becomes. So despite the popular perception, there has been no economical advantage for Brazil from logging rainforest zones and converting these to pastoral fields.[76]\n\nThe needs of soy farmers have been used to justify many of the controversial transportation projects that are currently developing in the Amazon. The first two highways successfully opened up the rainforest and led to increased settlement and deforestation. The mean annual deforestation rate from 2000 to 2005 (22,392 km2 or 8,646 sq mi per year) was 18% higher than in the previous five years (19,018 km2 or 7,343 sq mi per year).[77] Although deforestation declined significantly in the Brazilian Amazon between 2004 and 2014, there has been an increase to the present day.[78]\n\nBrazil's President, Jair Bolsonaro, has supported the relaxation of regulations placed on agricultural land. He has used his time in office to allow for more deforestation and more exploitation of the Amazon's rich natural resources. Deforestation reached a 15 year high in 2021.[79]\n\nSince the discovery of fossil fuel reservoirs in the Amazon rainforest, oil drilling activity has steadily increased, peaking in the Western Amazon in the 1970s and ushering another drilling boom in the 2000s.[80] Oil companies have to set up their operations by opening new roads through the forests, which often contributes to deforestation in the region.[81] 9.4% of the territory of the Amazon is affected by oil fields.[82]\n\nMining is also a major driver of deforestation. 17% of the area of the Amazon Rainforest is affected by mining.[82]\n\nThe transition to solar and wind energy, digitalization, raised the demand for cassiterite (the main ore of tin used also for financing gold mining), manganese and copper, which attracrted many illegal miners to the Amazon. This led to deforestation, different environmental and social problems. Hydropower also creates significant problems in the Amazon. Such activities are defined by the World Rainforest Movement as \"Green extractivism\".[83][84]\n\nThe European Union–Mercosur free trade agreement, which would form one of the world's largest free trade areas, has been denounced by environmental activists and indigenous rights campaigners.[85] The fear is that the deal could lead to more deforestation of the Amazon rainforest as it expands market access to Brazilian beef.[86]\n\nAccording to a November 2021 report by Brazil's INPE, based on satellite data, deforestation has increased by 22% over 2020 and is at its highest level since 2006.[87][88]\n\nThere were 72,843 fires in Brazil in 2019, with more than half within the Amazon region.[89][90][91]  In August 2019 there were a record number of fires.[92] Deforestation in the Brazilian Amazon rose more than 88% in June 2019 compared with the same month in 2018.[93]\n\nThe increased area of fire-impacted forest coincided with a relaxation of environmental regulations from the Brazilian government. Notably, before those regulations were put in place in 2008 the fire-impacted area was also larger compared to the regulation period of 2009–2018. As these fire continue to move closer to the heart of the Amazon basin, their impact on biodiversity will only increase in scale, as the cumulative fire-impacted area is correlated with the number of species impacted.[61]\nEnvironmentalists are concerned about loss of biodiversity that will result from destruction of the forest, and also about the release of the carbon contained within the vegetation, which could accelerate global warming. Amazonian evergreen forests account for about 10% of the world's terrestrial primary productivity and 10% of the carbon stores in ecosystems[94] – of the order of 1.1 × 1011 metric tonnes of carbon.[95] Amazonian forests are estimated to have accumulated 0.62 ± 0.37 tons of carbon per hectare per year between 1975 and 1996.[95] In 2021 it was reported that the Amazon for the first time emitted more greenhouse gases than it absorbed.[96] Though often referenced as producing more than a quarter of the Earth's oxygen, this often stated, but misused statistic actually refers to oxygen turnover.  The net contribution of the ecosystem is approximately zero.[97]\n\nOne computer model of future climate change caused by greenhouse gas emissions shows that the Amazon rainforest could become unsustainable under conditions of severely reduced rainfall and increased temperatures, leading to an almost complete loss of rainforest cover in the basin by 2100.,[98][99] and severe economic, natural capital and ecosystem services impacts of not averting the tipping point.[100] However, simulations of Amazon basin climate change across many different models are not consistent in their estimation of any rainfall response, ranging from weak increases to strong decreases.[101] The result indicates that the rainforest could be threatened through the 21st century by climate change in addition to deforestation.\n\nIn 1989, environmentalist C.M. Peters and two colleagues stated there is economic as well as biological incentive to protecting the rainforest. One hectare in the Peruvian Amazon has been calculated to have a value of $6820 if intact forest is sustainably harvested for fruits, latex, and timber; $1000 if clear-cut for commercial timber (not sustainably harvested); or $148 if used as cattle pasture.[102]\n\nAs indigenous territories continue to be destroyed by deforestation and ecocide (such as in the Peruvian Amazon),[103] indigenous peoples' rainforest communities continue to disappear, while others, like the Urarina continue to struggle to fight for their cultural survival and the fate of their forested territories. Meanwhile, the relationship between non-human primates in the subsistence and symbolism of indigenous lowland South American peoples has gained increased attention, as have ethno-biology and community-based conservation efforts.\n\nFrom 2002 to 2006, the conserved land in the Amazon rainforest almost tripled and deforestation rates dropped up to 60%. About 1,000,000 km2 (250,000,000 acres) have been put onto some sort of conservation, which adds up to a current amount of 1,730,000 km2 (430,000,000 acres).[104]\n\nIn April 2019, the Ecuadorian court stopped oil exploration activities in 180,000 hectares (440,000 acres) of the Amazon rainforest.[105]\n\nIn July 2019, the Ecuadorian court forbade the government to sell territory with forests to oil companies.[106]\n\nIn September 2019, the US and Brazil agreed to promote private-sector development in the Amazon. They also pledged a $100m  biodiversity conservation fund for the Amazon led by the private sector. Brazil's foreign minister stated that opening the rainforest to economic development was the only way to protect it.[107]\n\nA 2009 study found that a 4 °C rise (above pre-industrial levels) in global temperatures by 2100 would kill 85% of the Amazon rainforest while a temperature rise of 3 °C would kill some 75% of the Amazon.[109]\n\nA new study by an international team of environmental scientists in the Brazilian Amazon shows that protection of freshwater biodiversity can be increased by up to 600% through integrated freshwater-terrestrial planning \n.[110]\n\nDeforestation in the Amazon rainforest region has a negative impact on local climate.[111] It was one of the main causes of the severe drought  of 2014–2015 in Brazil.[112][113] This is because the moisture from the forests is important to the rainfall in Brazil, Paraguay and Argentina. Half of the rainfall in the Amazon area is produced by the forests.[114]\n\nResults of a 2021 scientific synthesis indicate that, in terms of global warming,  the Amazon basin with the Amazon rainforest is currently emitting more greenhouse gases than it absorbs overall. Climate change impacts and human activities in the area – mainly wildfires, current land-use and deforestation – are causing a release of forcing agents that likely result in a net warming effect.[115][108][116]\n\nIn 2022 the supreme court of Ecuador decided that \"\"under no circumstances can a project be carried out that generates excessive sacrifices to the collective rights of communities and nature.\" It also required the government to respect the opinion of Indigenous peoples of the Americas about different industrial projects on their land. Advocates of the decision argue that it will have consequences far beyond Ecuador. In general, ecosystems are in better shape when indigenous peoples own or manage the land.[117]\n\nDue to the conservation policies of Luiz Inácio Lula da Silva in the first 10 months of 2023 deforestation in the Brazilian Amazon decreased by around 50% compared to the same period in 2022. This was despite a severe drought, one of the worst on record, that exacerbated the situation. Climate change, El Nino, deforestation increases the likelihood of drought condition in the Amazon.[118]\n\nAccording to Amazon Conservation's MAAP forest monitoring program, the deforestation rate in the Amazon from the January 1 to November 8, 2023, decreased by 56% in comparison to the same period in 2022. The main cause is the decline in deforestation rate in Brazil, due to the government's policies, while Columbia, Peru and Bolivia also reduced deforestation.[119]\n\nIn January 2024 published data showed a 50% decline in deforestation rate in the Amazon rainforest and 43% rise in vegetation loss in the neighbor Cerrado during the year of 2023 in comparison to 2022. Both biomes together lose 12,980 km², 18% less than in 2022.[120]\n\nThe use of remotely sensed data is dramatically improving conservationists' knowledge of the Amazon basin. Given the objectivity and lowered costs of satellite-based land cover and -change analysis, it appears likely that remote sensing technology will be an integral part of assessing the extents, locations and damage of deforestation in the basin.[121] Furthermore, remote sensing is the best and perhaps only possible way to study the Amazon on a large scale.[122]\n\nThe use of remote sensing for the conservation of the Amazon is also being used by the indigenous tribes of the basin to protect their tribal lands from commercial interests. Using handheld GPS devices and programs like Google Earth, members of the Trio Tribe, who live in the rainforests of southern Suriname, map out their ancestral lands to help strengthen their territorial claims.[123] Currently, most tribes in the Amazon do not have clearly defined boundaries, making it easier for commercial ventures to target their territories.\n\nTo accurately map the Amazon's biomass and subsequent carbon-related emissions, the classification of tree growth stages within different parts of the forest is crucial. In 2006, Tatiana Kuplich organized the trees of the Amazon into four categories: mature forest, regenerating forest [less than three years], regenerating forest [between three and five years of regrowth], and regenerating forest [eleven to eighteen years of continued development].[124] The researcher used a combination of synthetic aperture radar (SAR) and Thematic Mapper (TM) to accurately place the different portions of the Amazon into one of the four classifications.\n\nIn 2005, parts of the Amazon basin experienced the worst drought in one hundred years,[125] and there were indications that 2006 may have been a second successive year of drought.[126] A 2006 article in the UK newspaper The Independent reported the Woods Hole Research Center results, showing that the forest in its present form could survive only three years of drought.[127][128] Scientists at the Brazilian National Institute of Amazonian Research argued in the article that this drought response, coupled with the effects of deforestation on regional climate, are pushing the rainforest towards a \"tipping point\" where it would irreversibly start to die.[129] It concluded that the forest is on the brink of[vague] being turned into savanna or desert, with catastrophic consequences for the world's climate.[citation needed] A study published in Nature Communications in October 2020 found that about 40% of the Amazon rainforest is at risk of becoming a savanna-like ecosystem due to reduced rainfall.[130] A study published in Nature climate change provided direct empirical evidence that more than three-quarters of the Amazon rainforest has been losing resilience since the early 2000s, risking dieback with profound implications for biodiversity, carbon storage and climate change at a global scale.[131]\n\nAccording to the World Wide Fund for Nature, the combination of climate change and deforestation increases the drying effect of dead trees that fuels forest fires.[132]\n\nIn 2010, the Amazon rainforest experienced another severe drought, in some ways more extreme than the 2005 drought. The affected region was approximately 3,000,000 km2 (1,160,000 sq mi) of rainforest, compared with 1,900,000 km2 (734,000 sq mi) in 2005. The 2010 drought had three epicenters where vegetation died off, whereas in 2005, the drought was focused on the southwestern part. The findings were published in the journal Science. In a typical year, the Amazon absorbs 1.5 gigatons of carbon dioxide; during 2005 instead 5 gigatons were released and in 2010 8 gigatons were released.[133][134] Additional severe droughts occurred in 2010, 2015, and 2016.[135]\n\nIn 2019 Brazil's protections of the Amazon rainforest were slashed, resulting in a severe loss of trees.[136] According to Brazil's National Institute for Space Research (INPE), deforestation in the Brazilian Amazon rose more than 50% in the first three months of 2020 compared to the same three-month period in 2019.[137]\n\nIn 2020, a 17 percent rise was noted in the Amazon wildfires, marking the worst start to the fire season in a decade. The first 10 days of August 2020 witnessed 10,136 fires. An analysis of the government figures reflected 81 per cent increase in fires in federal reserves, in comparison with the same period in 2019.[138] However, President Jair Bolsonaro turned down the existence of fires, calling it a \"lie\", despite the data produced by his own government.[139] Satellites in September recorded 32,017 hotspots in the world's largest rainforest, a 61% rise from the same month in 2019.[140] In addition, October saw a huge surge in the number of hotspots in the forest (more than 17,000 fires are burning in the Amazon's rainforest) – with more than double the amount detected in the same month last year.[141]\n\nIn 2023 the World Bank, published a report named: \"A Balancing Act for Brazil's Amazonian States: An Economic Memorandum\". The report stating that economic losses due to deforestation in Brazil could reach around 317 billion dollars per year, approximately 7 times higher in comparison to the cost of all commodities produced through deforestation, proposed non-deforestation based economic program in the region of the Amazon rainforest.[7][8]\n\nSilvopasture (integrating trees, forage and grazing) can help to stop deforestation in the region.[142]\n\nAccording to WWF, ecotourism could help the Amazon to reduce deforestation and climate change. Ecotourism is currently still little practiced in the Amazon, partly due to lack of information about places where implementation is possible. Ecotourism is a sector that can also be taken up by the Indigenous community in the Amazon as a source of income and revenue. An ecotourism project in the Brazilian section of the rainforest had been under consideration by Brazil's State Secretary for the Environment and Sustainable Development in 2009, along the Aripuanã River, in the Aripuanã Sustainable Development Reserve.[143] Also, some community-based ecotourism exists in the Mamirauá Sustainable Development Reserve.[144] Ecotourism is also practiced in the Peruvian section of the rainforest. A few ecolodges are for instance present between Cusco and Madre de Dios.[145]\n\nIn May 2023 Brazil's bank federation decided to implement a new sustainability standard demanding from meatpackers to ensure their meat is not coming from illegally deforested area. Credits will not be given to those who will not meet the new standards. The decision came after the European Union decides to implement regulations to stop deforestation. Brazil beef exporters, said the standard is not just because it is not applied to land owners.[146] 21 banks representing 81% of the credit market in Brazil agree to follow those rules.[147]\n\nAccording to a statement of the Colombian government deforestation rates in the Colombian Amazon fell by 70% in the first 9 months of 2023 compared to the same period in the previous year, what can be attributed to the conservation policies of the government. One of them is paying local residents for conserving the forest.[148]\n"
    },
    {
        "title": "Sahara",
        "url": "https://en.wikipedia.org/wiki/Sahara",
        "content": "\n\nThe Sahara (/səˈhɑːrə/, /səˈhærə/) is a desert spanning across North Africa. With an area of 9,200,000 square kilometres (3,600,000 sq mi), it is the largest hot desert in the world and the third-largest desert overall, smaller only than the deserts of Antarctica and the northern Arctic.[1][2][3]\n\nThe name \"Sahara\" is derived from Arabic: صَحَارَى, romanized: ṣaḥārā /sˤaħaːraː/, a broken plural form of ṣaḥrā' (صَحْرَاء /sˤaħraːʔ/), meaning \"desert\".[4][5][6][7]\n\nThe desert covers much of North Africa, excluding the fertile region on the Mediterranean Sea coast, the Atlas Mountains of the Maghreb, and the Nile Valley in Egypt and the Sudan.[8]\n\nIt stretches from the Red Sea in the east and the Mediterranean in the north to the Atlantic Ocean in the west, where the landscape gradually changes from desert to coastal plains. To the south it is bounded by the Sahel, a belt of semi-arid tropical savanna around the Niger River valley and the Sudan region of sub-Saharan Africa. The Sahara can be divided into several regions, including the western Sahara, the central Ahaggar Mountains, the Tibesti Mountains, the Aïr Mountains, the Ténéré desert, and the Libyan Desert.\n\nFor several hundred thousand years, the Sahara has alternated between desert and savanna grassland in a 20,000-year cycle[9] caused by the precession of Earth's axis (about 26,000 years) as it rotates around the Sun, which changes the location of the North African monsoon.\n\nThe Sahara covers large parts of Algeria, Chad, Egypt, Libya, Mali, Mauritania, Niger, Western Sahara and Sudan, and parts of southern Morocco and Tunisia. It covers 9 million square kilometres (3,500,000 sq mi), 31% of the African continent. If all areas with a mean annual precipitation of less than 250 mm (9.8 in) were included, the Sahara would be 11 million square kilometres (4,200,000 sq mi). It is one of three distinct physiographic provinces of the African massive physiographic division. The Sahara is so large and bright that, in theory, it could be detected from other stars as a surface feature of Earth, with near-current technology.[10]\n\nThe Sahara is mainly rocky hamada (stone plateaus); ergs (sand seas – large areas covered with sand dunes) form only a minor part, contrary to common misconception,[11] but many of the sand dunes are over 180 metres (590 ft) high.[12] Wind or rare rainfall shape the desert features: sand dunes, dune fields, sand seas, stone plateaus, gravel plains (reg), dry valleys (wadi), dry lakes (oued), and salt flats (shatt or chott).[13] Unusual landforms include the Richat Structure in Mauritania.\n\nSeveral deeply dissected mountains, many volcanic, rise from the desert, including the Aïr Mountains, Ahaggar Mountains, Saharan Atlas, Tibesti Mountains, Adrar des Iforas, and the Red Sea Hills. The highest peak in the Sahara is Emi Koussi, a shield volcano in the Tibesti range of northern Chad.\n\nThe central Sahara is hyperarid, with sparse vegetation. The northern and southern reaches of the desert, along with the highlands, have areas of sparse grassland and desert shrub, with trees and taller shrubs in wadis, where moisture collects. In the central, hyperarid region, there are many subdivisions of the great desert: Tanezrouft, the Ténéré, the Libyan Desert, the Eastern Desert, the Nubian Desert and others. These extremely arid areas often receive no rain for years.\n\nTo the north, the Sahara skirts the Mediterranean Sea in Egypt and portions of Libya, but in Cyrenaica and the Maghreb, the Sahara borders the Mediterranean forest, woodland, and scrub eco-regions of northern Africa, all of which have a Mediterranean climate characterized by hot summers and cool and rainy winters. According to the botanical criteria of Frank White[14] and geographer Robert Capot-Rey,[15][16] the northern limit of the Sahara corresponds to the northern limit of date palm cultivation and the southern limit of the range of esparto, a grass typical of the Mediterranean climate portion of the Maghreb and Iberia. The northern limit also corresponds to the 100 mm (3.9 in) isohyet of annual precipitation.[17]\n\nTo the south, the Sahara is bounded by the Sahel, a belt of dry tropical savanna with a summer rainy season that extends across Africa from east to west. The southern limit of the Sahara is indicated botanically by the southern limit of Cornulaca monacantha (a drought-tolerant member of the Chenopodiaceae), or northern limit of Cenchrus biflorus, a grass typical of the Sahel.[15][16] According to climatic criteria, the southern limit of the Sahara corresponds to the 150 mm (5.9 in) isohyet of annual precipitation (this is a long-term average, since precipitation varies annually).[17]\n\nImportant cities located in the Sahara include Nouakchott, the capital of Mauritania; Tamanrasset, Ouargla, Béchar, Hassi Messaoud, Ghardaïa, and El Oued in Algeria; Timbuktu in Mali; Agadez in Niger; Ghat in Libya; and Faya-Largeau in Chad.\n\nThe Sahara is the world's largest hot desert.[18][19] It is located in the horse latitudes under the subtropical ridge, a significant belt of semi-permanent subtropical warm-core high pressure where the air from the upper troposphere usually descends, warming and drying the lower troposphere and preventing cloud formation.\n\nThe permanent absence of clouds allows unhindered light and thermal radiation. The stability of the atmosphere above the desert prevents any convective overturning, thus making rainfall virtually non-existent. As a consequence, the weather tends to be sunny, dry and stable with a minimal chance of rainfall. Subsiding, diverging, dry air masses associated with subtropical high-pressure systems are extremely unfavorable for the development of convectional showers. The subtropical ridge is the predominant factor that explains the hot desert climate (Köppen climate classification BWh) of this vast region. The descending airflow is the strongest and the most effective over the eastern part of the Great Desert, in the Libyan Desert: this is the sunniest, driest and the most nearly \"rainless\" place on the planet, rivaling the Atacama Desert, lying in Chile and Peru.\n\nThe rainfall inhibition and the dissipation of cloud cover are most accentuated over the eastern section of the Sahara rather than the western. The prevailing air mass lying above the Sahara is the continental tropical (cT) air mass, which is hot and dry. Hot, dry air masses primarily form over the North-African desert from the heating of the vast continental land area, and it affects the whole desert during most of the year. Because of this extreme heating process, a thermal low is usually noticed near the surface, and is the strongest and the most developed during the summertime. The Sahara High represents the eastern continental extension of the Azores High,[citation needed] centered over the North Atlantic Ocean. The subsidence of the Sahara High nearly reaches the ground during the coolest part of the year, while it is confined to the upper troposphere during the hottest periods.\n\nThe effects of local surface low pressure are extremely limited because upper-level subsidence still continues to block any form of air ascent. Also, to be protected against rain-bearing weather systems by the atmospheric circulation itself, the desert is made even drier by its geographical configuration and location. Indeed, the extreme aridity of the Sahara is not only explained by the subtropical high pressure: the Atlas Mountains of Algeria, Morocco and Tunisia also help to enhance the aridity of the northern part of the desert. These major mountain ranges act as a barrier, causing a strong rain shadow effect on the leeward side by dropping much of the humidity brought by atmospheric disturbances along the polar front which affects the surrounding Mediterranean climates.\n\nThe primary source of rain in the Sahara is the Intertropical Convergence Zone, a continuous belt of low-pressure systems near the equator which bring the brief, short and irregular rainy season to the Sahel and southern Sahara. Rainfall in this giant desert has to overcome the physical and atmospheric barriers that normally prevent the production of precipitation. The harsh climate of the Sahara is characterized by: extremely low, unreliable, highly erratic rainfall; extremely high sunshine duration values; high temperatures year-round; negligible rates of relative humidity; a significant diurnal temperature variation; and extremely high levels of potential evaporation which are the highest recorded worldwide.[20]\n\n\nThe sky is usually clear above the desert, and the sunshine duration is extremely high everywhere in the Sahara. Most of the desert has more than 3,600 hours of bright sunshine per year (over 82% of daylight hours), and a wide area in the eastern part has over 4,000 hours of bright sunshine per year (over 91% of daylight hours). The highest values are very close to the theoretical maximum value. A value of 4300 hours (98%) of the time would be[clarification needed] recorded in Upper Egypt (Aswan, Luxor) and in the Nubian Desert (Wadi Halfa).[21] The annual average direct solar irradiation is around 2,800 kWh/(m2 year) in the Great Desert. The Sahara has a huge potential for solar energy production.\nThe high position of the Sun, the extremely low relative humidity, and the lack of vegetation and rainfall make the Great Desert the hottest large region in the world, and the hottest place on Earth during summer in some spots. The average high temperature exceeds 38 to 40 °C (100.4 to 104.0 °F) during the hottest month nearly everywhere in the desert except at very high altitudes. The world's highest officially recorded average daily high temperature[clarification needed] was 47 °C (116.6 °F) in a remote desert town in the Algerian Desert called Bou Bernous, at an elevation of 378 metres (1,240 ft) above sea level,[21] and only Death Valley, California rivals it.[22]\n\nOther hot spots in Algeria such as Adrar, Timimoun, In Salah, Ouallene, Aoulef, Reggane with an elevation between 200 and 400 metres (660 and 1,310 ft) above sea level get slightly lower summer average highs, around 46 °C (114.8 °F) during the hottest months of the year. Salah, well known in Algeria for its extreme heat, has average high temperatures of 43.8 °C (110.8 °F), 46.4 °C (115.5 °F), 45.5 °C (113.9 °F) and 41.9 °C (107.4 °F) in June, July, August and September respectively. There are even hotter spots in the Sahara, but they are located in extremely remote areas, especially in the Azalai, lying in northern Mali. The major part of the desert experiences around three to five months when the average high strictly[clarification needed] exceeds 40 °C (104 °F); while in the southern central part of the desert, there are up to six or seven months when the average high temperature strictly[clarification needed] exceeds 40 °C (104 °F). Some examples of this are Bilma, Niger and Faya-Largeau, Chad. The annual average daily temperature exceeds 20 °C (68 °F) everywhere and can approach 30 °C (86 °F) in the hottest regions year-round. However, most of the desert has a value in excess of 25 °C (77 °F).\n\nSand and ground temperatures are even more extreme. During daytime, the sand temperature is extremely high: it can easily reach 80 °C (176 °F) or more.[23] A sand temperature of 83.5 °C (182.3 °F) has been recorded in Port Sudan.[23] Ground temperatures of 72 °C (161.6 °F) have been recorded in the Adrar of Mauritania and a value of 75 °C (167 °F) has been measured in Borkou, northern Chad.[23]\n\nDue to lack of cloud cover and very low humidity, the desert usually has high diurnal temperature variations between days and nights. However, it is a myth that the nights are especially cold after extremely hot days in the Sahara.[citation needed] On average, nighttime temperatures tend to be 13–20 °C (23–36 °F) cooler than in the daytime. The smallest variations are found along the coastal regions due to high humidity and are often even lower than a 10 °C (18 °F) difference, while the largest variations are found in inland desert areas where the humidity is the lowest, mainly in the southern Sahara. Still, it is true that winter nights can be cold, as it can drop to the freezing point and even below, especially in high-elevation areas.[clarification needed] The frequency of subfreezing winter nights in the Sahara is strongly influenced by the North Atlantic Oscillation (NAO), with warmer winter temperatures during negative NAO events and cooler winters with more frosts when the NAO is positive.[24] This is because the weaker clockwise flow around the eastern side of the subtropical anticyclone during negative NAO winters, although too dry to produce more than negligible precipitation, does reduce the flow of dry, cold air from higher latitudes of Eurasia into the Sahara significantly.[25]\n\nThe average annual rainfall ranges from very low in the northern and southern fringes of the desert to nearly non-existent over the central and the eastern part. The thin northern fringe of the desert receives more winter cloudiness and rainfall due to the arrival of low pressure systems over the Mediterranean Sea along the polar front, although very attenuated by the rain shadow effects of the mountains and the annual average rainfall ranges from 100 millimetres (4 in) to 250 millimetres (10 in). For example, Biskra, Algeria, and Ouarzazate, Morocco, are found in this zone. The southern fringe of the desert along the border with the Sahel receives summer cloudiness and rainfall due to the arrival of the Intertropical Convergence Zone from the south and the annual average rainfall ranges from 100 millimetres (4 in) to 250 millimetres (10 in). For example, Timbuktu, Mali and Agadez, Niger are found in this zone.\n\nThe vast central hyper-arid core of the desert is virtually never affected by northerly or southerly atmospheric disturbances and permanently remains under the influence of the strongest anticyclonic weather regime, and the annual average rainfall can drop to less than 1 millimetre (0.04 in). In fact, most of the Sahara receives less than 20 millimetres (0.8 in). Of the 9,000,000 square kilometres (3,500,000 sq mi) of desert land in the Sahara, an area of about 2,800,000 square kilometres (1,100,000 sq mi) (about 31% of the total area) receives an annual average rainfall amount of 10 millimetres (0.4 in) or less, while some 1,500,000 square kilometres (580,000 sq mi) (about 17% of the total area) receives an average of 5 millimetres (0.2 in) or less.[26]\n\nThe annual average rainfall is virtually zero over a wide area of some 1,000,000 square kilometres (390,000 sq mi) in the eastern Sahara comprising deserts of: Libya, Egypt and Sudan (Tazirbu, Kufra, Dakhla, Kharga, Farafra, Siwa, Asyut, Sohag, Luxor, Aswan, Abu Simbel, Wadi Halfa) where the long-term mean approximates 0.5 millimetres (0.02 in) per year.[26] Rainfall is very unreliable and erratic in the Sahara as it may vary considerably year by year. In full contrast to the negligible annual rainfall amounts, the annual rates of potential evaporation are extraordinarily high, roughly ranging from 2,500 millimetres (100 in) per year to more than 6,000 millimetres (240 in) per year in the whole desert.[27] Nowhere else on Earth has air been found as dry and evaporative as in the Sahara region. However, at least two instances of snowfall have been recorded in Sahara, in February 1979 and December 2016, both in the town of Ain Sefra.[28]\n\nOne theory for the formation of the Sahara is that the monsoon in Northern Africa was weakened because of glaciation during the Quaternary period, starting two or three million years ago. Another theory is that the monsoon was weakened when the ancient Tethys Sea dried up during the Tortonian period around 7 million years ago.[29]\n\nThe climate of the Sahara has undergone enormous variations between wet and dry over the last few hundred thousand years,[30] believed to be caused by long-term changes in the North African climate cycle that alters the path of the North African Monsoon – usually southward. The cycle is caused by a 41,000-year cycle in which the tilt of the earth changes between 22° and 24.5°.[26] At present, it is in a dry period, but it is expected that the Sahara will become green again in 15,000 years. When the North African monsoon is at its strongest, annual precipitation and subsequent vegetation in the Sahara region increase, resulting in conditions commonly referred to as the \"green Sahara\". For a relatively weak North African monsoon, the opposite is true, with decreased annual precipitation and less vegetation resulting in a phase of the Sahara climate cycle known as the \"desert Sahara\".[31]\n\nThe idea that changes in insolation (solar heating) caused by long-term changes in Earth's orbit are a controlling factor for the long-term variations in the strength of monsoon patterns across the globe was first suggested by Rudolf Spitaler in the late nineteenth century,[32] The hypothesis was later formally proposed and tested by the meteorologist John Kutzbach in 1981.[33] Kutzbach's ideas about the impacts of insolation on global monsoonal patterns have become widely accepted today as the underlying driver of long-term monsoonal cycles. Kutzbach never formally named his hypothesis and as such it is referred to here as the \"Orbital Monsoon Hypothesis\" as suggested by Ruddiman in 2001.[32]\n\nDuring the last glacial period, the Sahara was much larger than it is today, extending south beyond its current boundaries.[34] The end of the glacial period brought more rain to the Sahara, from about 8000 BCE to 6000 BCE, perhaps because of low pressure areas over the collapsing ice sheets to the north.[35] Once the ice sheets were gone, the northern Sahara dried out. In the southern Sahara, the drying trend was initially counteracted by the monsoon, which brought rain further north than it does today. By around 4200 BCE, however, the monsoon retreated south to approximately where it is today,[36] leading to the gradual desertification of the Sahara.[37] The Sahara is now as dry as it was about 13,000 years ago.[30]\n\nLake Chad is the remnant of a former inland sea, paleolake Mega-Chad, which existed during the African humid period. At its largest extent, sometime before 5000 BCE, Lake Mega-Chad was the largest of four Saharan paleolakes, and is estimated to have covered an area of 350,000 km2.[38]\n\nThe Sahara pump theory describes this cycle. During periods of a wet or \"Green Sahara\", the Sahara becomes a savanna grassland and various flora and fauna become more common. Following inter-pluvial arid periods, the Sahara area then reverts to desert conditions and the flora and fauna are forced to retreat northwards to the Atlas Mountains, southwards into West Africa, or eastwards into the Nile Valley. This separates populations of some of the species in areas with different climates, forcing them to adapt, possibly giving rise to allopatric speciation.\n\nIt is also proposed that humans accelerated the drying-out period from 6000 to 2500 BCE by pastoralists overgrazing available grassland.[40]\n\nThe growth of speleothems (which requires rainwater) was detected in Hol-Zakh, Ashalim, Even-Sid, Ma'ale-ha-Meyshar, Ktora Cracks, Nagev Tzavoa Cave, and elsewhere, and has allowed tracking of prehistoric rainfall. The Red Sea coastal route was extremely arid before 140 and after 115 kya (thousands of years ago). Slightly wetter conditions appear at 90–87 kya, but it still was just one tenth the rainfall around 125 kya. In the southern Negev Desert speleothems did not grow between 185 and 140 kya (MIS 6), 110–90 (MIS 5.4–5.2), nor after 85 kya nor during most of the interglacial period (MIS 5.1), the glacial period and Holocene. This suggests that the southern Negev was arid-to-hyper-arid in these periods.[41]\n\nDuring the Last Glacial Maximum (LGM) the Sahara was more extensive than it is now with the extent of the tropical forests being greatly reduced,[42] and the lower temperatures reduced the strength of the Hadley Cell. This is a climate cell which causes rising tropical air of the Inter-Tropical Convergence Zone (ITCZ) to bring rain to the tropics, while dry descending air, at about 20 degrees north, flows back to the equator and brings desert conditions to this region. It is associated with high rates of wind-blown mineral dust, and these dust levels are found as expected in marine cores from the north tropical Atlantic. But around 12,500 BCE the amount of dust in the cores in the Bølling/Allerød phase suddenly plummets and shows a period of much wetter conditions in the Sahara, indicating a Dansgaard-Oeschger (DO) event (a sudden warming followed by a slower cooling of the climate). The moister Saharan conditions had begun about 12,500 BCE, with the extension of the ITCZ northward in the northern hemisphere summer, bringing moist wet conditions and a savanna climate to the Sahara, which (apart from a short dry spell associated with the Younger Dryas) peaked during the Holocene thermal maximum climatic phase at 4000 BCE when mid-latitude temperatures seem to have been between 2 and 3 degrees warmer than in the recent past. Analysis of Nile River deposited sediments in the delta also shows this period had a higher proportion of sediments coming from the Blue Nile, suggesting higher rainfall also in the Ethiopian Highlands. This was caused principally by a stronger monsoonal circulation throughout the sub-tropical regions, affecting India, Arabia and the Sahara.[citation needed] Lake Victoria only recently became the source of the White Nile and dried out almost completely around 15 kya.[43]\n\nThe sudden subsequent movement of the ITCZ southwards with a Heinrich event (a sudden cooling followed by a slower warming), linked to changes with the El Niño-Southern Oscillation cycle, led to a rapid drying out of the Saharan and Arabian regions, which quickly became desert. This is linked to a marked decline in the scale of the Nile floods between 2700 and 2100 BCE.[44]\n\nThe Sahara comprises several distinct ecoregions. With their variations in temperature, rainfall, elevation, and soil, these regions harbor distinct communities of plants and animals.\n\nThe flora of the Sahara is highly diversified based on the bio-geographical characteristics of this vast desert. Floristically, the Sahara has three zones based on the amount of rainfall received – the Northern (Mediterranean), Central and Southern Zones. There are two transitional zones – the Mediterranean-Sahara transition and the Sahel transition zone.[51]\n\nThe Saharan flora comprises around 2800 species of vascular plants. Approximately a quarter of these are endemic. About half of these species are common to the flora of the Arabian deserts.[52]\n\nThe central Sahara is estimated to include five hundred species of plants, which is extremely low considering the huge extent of the area. Plants such as acacia trees, palms, succulents, spiny shrubs, and grasses have adapted to the arid conditions, by growing lower to avoid water loss by strong winds, by storing water in their thick stems to use it in dry periods, by having long roots that travel horizontally to reach the maximum area of water and to find any surface moisture, and by having small thick leaves or needles to prevent water loss by evapotranspiration. Plant leaves may dry out totally and then recover.\n\nSeveral species of fox live in the Sahara including: the fennec fox, pale fox and Rüppell's fox. The addax, a large white antelope, can go nearly a year in the desert without drinking. The dorcas gazelle is a north African gazelle that can also go for a long time without water. Other notable gazelles include the rhim gazelle and dama gazelle.\n\nThe Saharan cheetah (northwest African cheetah) lives in Algeria, Togo, Niger, Mali, Benin, and Burkina Faso. There remain fewer than 250 mature cheetahs, which are very cautious, fleeing any human presence. The cheetah avoids the sun from April to October, seeking the shelter of shrubs such as balanites and acacias. They are unusually pale.[53][54] The other cheetah subspecies (northeast African cheetah) lives in Chad, Sudan and the eastern region of Niger. However, it is currently extinct in the wild in Egypt and Libya. There are approximately 2000 mature individuals left in the wild.\n\nOther animals include the monitor lizards, hyrax, sand vipers, and small populations of African wild dog,[55] in perhaps only 14 countries[56] and red-necked ostrich. Other animals exist in the Sahara (birds in particular) such as African silverbill and black-faced firefinch, among others. There are also small desert crocodiles in Mauritania and the Ennedi Plateau of Chad.[57]\n\nThe deathstalker scorpion can be 10 cm (3.9 in) long. Its venom contains large amounts of agitoxin and scyllatoxin and is very dangerous; however, a sting from this scorpion rarely kills a healthy adult. The Saharan silver ant is unique in that due to the extreme high temperatures of their habitat, and the threat of predators, the ants are active outside their nest for only about ten minutes per day.[58]\n\nDromedary camels and goats are the domesticated animals most commonly found in the Sahara. Because of its qualities of endurance and speed, the dromedary is the favourite animal used by nomads.\n\nHuman activities are more likely to affect the habitat in areas of permanent water (oases) or where water comes close to the surface. Here, the local pressure on natural resources can be intense. The remaining populations of large mammals have been greatly reduced by hunting for food and recreation. In recent years development projects have started in the deserts of Algeria and Tunisia using irrigated water pumped from underground aquifers. These schemes often lead to soil degradation and salinization.\n\nResearchers from Hacettepe University have reported that Saharan soil may have bio-available iron and also some essential macro and micro nutrient elements suitable for use as fertilizer for growing wheat.[59]\n\nPeople lived on the edge of the desert thousands of years ago,[60] since the end of the last glacial period. In the Central Sahara, engraved and painted rock art were created perhaps as early as 10,000 years ago, spanning the Bubaline Period, Kel Essuf Period, Round Head Period, Pastoral Period, Caballine Period, and Cameline Period.[61] The Sahara was then a much wetter place than it is today. Over 30,000 petroglyphs of river animals such as crocodiles[62] survive, with half found in the Tassili n'Ajjer in southeast Algeria. Fossils of dinosaurs,[63] including Afrovenator, Jobaria and Ouranosaurus, have also been found here. The modern Sahara, though, is not lush in vegetation, except in the Nile Valley, at a few oases, and in the northern highlands, where Mediterranean plants such as the olive tree are found to grow. Shifts in Earth's axis increased temperatures and decreased precipitation, which caused an abrupt beginning of North Africa desertification about 5,400 years ago.[36]\n\nThe Kiffian culture is a prehistoric industry, or domain, that existed between 10,000 and 8,000 years ago in the Sahara, during the Neolithic Subpluvial. Human remains from this culture were found in 2000 at a site known as Gobero, located in Niger in the Ténéré Desert.[64] The site is known as the largest and earliest grave of Stone Age people in the Sahara.[65] The Kiffians were skilled hunters. Bones of many large savannah animals that were discovered in the same area suggest that they lived on the shores of a lake that was present during the Holocene Wet Phase, a period when the Sahara was verdant and wet.[65] The Kiffian people were tall, standing over six feet in height.[64] Craniometric analysis indicates that this early Holocene population was closely related to the Late Pleistocene Iberomaurusians and early Holocene Capsians of the Maghreb, as well as mid-Holocene Mechta groups.[66] Traces of the Kiffian culture do not exist after 8,000 years ago, as the Sahara went through a dry period for the next thousand years.[67] After this time, the Tenerian culture colonized the area.\n\nGobero was discovered in 2000 during an archaeological expedition led by Paul Sereno, which sought dinosaur remains. Two distinct prehistoric cultures were discovered at the site: the early Holocene Kiffian culture, and the middle Holocene Tenerian culture. The post-Kiffian desiccation lasted until around 4600 BCE, when the earliest artefacts associated with the Tenerians have been dated to. Some 200 skeletons have been discovered at Gobero. The Tenerians were considerably shorter in height and less robust than the earlier Kiffians. Craniometric analysis also indicates that they were osteologically distinct. The Kiffian skulls are akin to those of the Late Pleistocene Iberomaurusians, early Holocene Capsians, and mid-Holocene Mechta groups, whereas the Tenerian crania are more like those of Mediterranean groups.[68][69] Graves show that the Tenerians observed spiritual traditions, as they were buried with artifacts such as jewelry made of hippo tusks and clay pots. The most interesting find is a triple burial, dated to 5300 years ago, of an adult female and two children, estimated through their teeth as being five and eight years old, hugging each other. Pollen residue indicates they were buried on a bed of flowers. The three are assumed to have died within 24 hours of each other, but as their skeletons hold no apparent trauma (they did not die violently) and they have been buried so elaborately – unlikely if they had died of a plague – the cause of their deaths is a mystery.\n\nUan Muhuggiag appears to have been inhabited from at least the 6th millennium BCE to about 2700 BCE, although not necessarily continuously.[70] The most noteworthy find at Uan Muhuggiag is the well-preserved mummy of a young boy of approximately 2+1⁄2 years old. The child was in a fetal position, then embalmed, then placed in a sack made of antelope skin, which was insulated by a layer of leaves.[71] The boy's organs were removed, as evidenced by incisions in his stomach and thorax, and an organic preservative was inserted to stop his body from decomposing.[72] An ostrich eggshell necklace was also found around his neck.[70] Radiocarbon dating determined the age of the mummy to be approximately 5600 years old, which makes it about 1000 years older than the earliest previously recorded mummy in ancient Egypt.[73] In 1958–59, an archaeological expedition led by Antonio Ascenzi conducted anthropological, radiological, histological and chemical analyses on the Uan Muhuggiag mummy. The team claimed that the mummy was a 30-month-old child of uncertain sex. They also found a long incision on the specimen's abdominal wall, which indicated that the body had been initially mummified by evisceration and later underwent natural desiccation. The team also stated that the mummy possessed \"Negroid features.\"[74] However, modern genetics has since proven that the final claim is unscientific and not supported by evidence.[75][76]  A more recent publication referenced a laboratory examination of the cutaneous  features of the child mummy in which the results verified that the child possessed a dark skin complexion.[77] One other individual, an adult, was found at Uan Muhuggiag, buried in a crouched position.[70] However, the body showed no evidence of evisceration or any other method of preservation. The body was estimated to date from about 7500 BP.[78]\n\nDuring the Neolithic Era, before the onset of desertification around 9500 BCE, the central Sudan had been a rich environment supporting a large population ranging across what is now barren desert, like the Wadi el-Qa'ab. By the 5th millennium BCE, the people who inhabited what is now called Nubia were full participants in the \"agricultural revolution\", living a settled lifestyle with domesticated plants and animals. Saharan rock art of cattle and herdsmen suggests the presence of a cattle cult like those found in Sudan and other pastoral societies in Africa today.[79] Megaliths found at Nabta Playa are overt examples of probably the world's first known archaeoastronomy devices, predating Stonehenge by some 2,000 years.[80] This complexity, as observed at Nabta Playa, and as expressed by different levels of authority within the society there, likely formed the basis for the structure of both the Neolithic society at Nabta and the Old Kingdom of Egypt.[81]\nArchaeological evidence has attested that population settlements occurred in Nubia as early as the Late Pleistocene era and from the 5th millennium BCE onwards, whereas there is \"no or scanty evidence\" of human presence in the Egyptian Nile Valley during these periods, which may be due to problems in site preservation.[82]\n\nBy 6000 BCE predynastic Egyptians in the southwestern corner of Egypt were herding cattle and constructing large buildings. Subsistence in organized and permanent settlements in predynastic Egypt by the middle of the 6th millennium BCE centered predominantly on cereal and animal agriculture: cattle, goats, pigs and sheep. Metal objects replaced prior ones of stone. Tanning of animal skins, pottery and weaving were commonplace in this era also. There are indications of seasonal or only temporary occupation of the Al Fayyum in the 6th millennium BCE, with food activities centering on fishing, hunting and food-gathering. Stone arrowheads, knives and scrapers from the era are commonly found.[83] Burial items included pottery, jewelry, farming and hunting equipment, and assorted foods including dried meat and fruit. Burial in desert environments appears to enhance Egyptian preservation rites, and the dead were buried facing due west.[84] Several scholars have argued that the African origins of the Egyptian civilisation derived from pastoral communities which emerged in both the Egyptian and Sudanese regions of the Nile Valley in the fifth millennium BCE.[85]\n\nBy 3400 BCE, the Sahara was as dry as it is today, due to reduced precipitation and higher temperatures resulting from a shift in Earth's orbit.[36] As a result of this aridification, it became a largely impenetrable barrier to humans, with the remaining settlements mainly being concentrated around the numerous oases that dot the landscape. Little trade or commerce is known to have passed through the interior in subsequent periods, the only major exception being the Nile Valley. The Nile, however, was impassable at several cataracts, making trade and contact by boat difficult.\n\nIn 4000 BCE, the start of sophisticated social structure (e.g., trade of cattle as valued assets) developed among herders amid the Pastoral Period of the Sahara.[86] Saharan pastoral culture (e.g., fields of tumuli, lustrous stone rings, axes) was intricate.[87] By 1800 BCE, Saharan pastoral culture expanded throughout the Saharan and Sahelian regions.[86] The initial stages of sophisticated social structure among Saharan herders served as the segue for the development of sophisticated hierarchies found in African settlements, such as Dhar Tichitt.[86] After migrating from the Central Sahara, proto-Mande peoples established their civilization in the Tichitt region[88] of the Western Sahara[89] The Tichitt Tradition of eastern Mauritania dates from 2200 BCE[90][91] to 200 BCE.[92][93] Tichitt culture, at Dhar Néma, Dhar Tagant, Dhar Tichitt, and Dhar Walata, included a four-tiered hierarchal social structure, farming of cereals, metallurgy, numerous funerary tombs, and a rock art tradition[94] At Dhar Tichitt and Dhar Walata, pearl millet may have also been independently tamed amid the Neolithic.[95] Dhar Tichitt, which includes Dakhlet el Atrouss, may have served as the primary regional center for the multi-tiered hierarchical social structure of the Tichitt Tradition,[96] and the Malian Lakes Region, which includes Tondidarou, may have served as a second regional center of the Tichitt Tradition.[97] The urban[89] Tichitt Tradition may have been the earliest large-scale, complexly organized society in West Africa,[98] and an early civilization of the Sahara,[88][90] which may have served as the segue for state formation in West Africa.[87]\n\nAs areas where the Tichitt cultural tradition were present, Dhar Tichitt and Dhar Walata were occupied more frequently than Dhar Néma.[98] Farming of crops (e.g., millet) may have been a feature of the Tichitt cultural tradition as early as 3rd millennium BCE in Dhar Tichitt.[98]\n\nAs part of a broader trend of iron metallurgy developed in the West African Sahel amid 1st millennium BCE, iron items (350 BCE – 100 CE) were found at Dhar Tagant, iron metalworking and/or items (800 BCE – 400 BCE) were found at Dia Shoma and Walaldé, and the iron remnants (760 BCE – 400 BCE) found at Bou Khzama and Djiganyai.[98] The iron materials that were found are evidence of iron metalworking at Dhar Tagant.[93] In the late period of the Tichitt Tradition at Dhar Néma, tamed pearl millet was used to temper the tuyeres of an oval-shaped low shaft furnace; this furnace was one out of 16 iron furnaces located on elevated ground.[92] Iron metallurgy may have developed before the second half of 1st millennium BCE, as indicated by pottery dated between 800 BCE and 200 BCE.[92] At Dhar Walata and Dhar Tichitt, copper was also used.[89]\n\nAfter its decline in Mauritania, the Tichitt Tradition spread to the Middle Niger region (e.g., Méma, Macina, Dia Shoma, Jenne Jeno) of Mali where it developed into and persisted as Faïta Facies ceramics between 1300 BCE and 400 BCE among rammed earth architecture and iron metallurgy (which had developed after 900 BCE).[99] Thereafter, the Ghana Empire developed in the 1st millennium CE.[99]\n\nThe people of Phoenicia, who flourished from 1200 to 800 BCE, created a chain of settlements along the coast of North Africa and traded extensively with its inhabitants. This put them in contact with the people of ancient Libya, who were the ancestors of people who speak Berber languages in North Africa and the Sahara today.\n\nThe Libyco-Berber alphabet of the ancient Libyans of north Africa seems to have been based on Phoenician, and its descendant Tifinagh is still used today by the (Berber) Tuareg of the central Sahara.\n\nThe Periplus of the Phoenician navigator Hanno, who lived sometime in the 5th century BCE, claims that he founded settlements along the Atlantic coast of Africa, possibly including the Western Sahara. The identification of the places discussed is controversial, and archeological confirmation is lacking.\n\nBy 500 BCE, Greeks arrived in the desert. Greek traders spread along the eastern coast of the desert, establishing trading colonies along the Red Sea. The Carthaginians explored the Atlantic coast of the desert, but the turbulence of the waters and the lack of markets caused a lack of presence further south than modern Morocco. Centralized states thus surrounded the desert on the north and east; it remained outside the control of these states. Raids from the nomadic Berber people of the desert were of constant concern to those living on the edge of the desert.\n\nAn urban civilization, the Garamantes, arose around 500 BCE in the Sahara, in a valley that is now called the Wadi al-Ajal in Fezzan, Libya.[30] The Garamantes built a prosperous empire in the heart of the desert.[100] The Garamantes achieved this development by digging tunnels far into the mountains flanking the valley to tap fossil water and bring it to their fields. The Garamantes grew populous and strong, conquering their neighbors, and capturing and enslaving many individuals who were forced to work by extending the tunnels. The ancient Greeks and the Romans knew of the Garamantes and regarded them as uncivilized nomads. However, they traded with them, and a Roman bath has been found in the Garamantes' capital of Garama. Archaeologists have found eight major towns and many other important settlements in the Garamantes' territory. The Garamantes' civilization eventually collapsed after they had depleted available water in the aquifers and could no longer sustain the effort to extend the tunnels further into the mountains.[101]\n\nBetween the first century BCE and the fourth century CE, several Roman expeditions into the Sahara were conducted by groups of military and commercial units of Romans.\n\nThe Byzantine Empire ruled the northern shores of the Sahara from the 5th to the 7th centuries. After the Muslim conquest of Arabia, specifically the Arabian peninsula, the Muslim conquest of North Africa began in the mid-7th to early 8th centuries and Islamic influence expanded rapidly on the Sahara. By the end of 641 all of Egypt was in Muslim hands. Trade across the desert intensified, and a significant slave trade crossed the desert. It has been estimated that from the 10th to 19th centuries some 6,000 to 7,000 slaves were transported north each year.[102]\n\nThe Beni Ḥassān and other nomadic Arab tribes dominated the Sanhaja Berber tribes of the western Sahara after the Char Bouba war of the 17th century. As a result, Arabian culture and language came to dominate, and the Berber tribes underwent some Arabization.\n\nIn the 16th century the northern fringe of the Sahara, such as coastal regencies in present-day Algeria and Tunisia, as well as some parts of present-day Libya, together with the semi-autonomous kingdom of Egypt, were occupied by the Ottoman Empire. From 1517 Egypt was a valued part of the Ottoman Empire, ownership of which provided the Ottomans with control over the Nile Valley, the east Mediterranean and North Africa. The benefit of the Ottoman Empire was the freedom of movement for citizens and goods. Traders exploited the Ottoman land routes to handle the spices, gold and silk from the East, manufactured goods from Europe, and the slave and gold traffic from Africa. Arabic continued as the local language and Islamic culture was much reinforced. The Sahel and southern Sahara regions were home to several independent states or to roaming Tuareg clans.\n\nEuropean colonialism in the Sahara began in the 19th century. France conquered the regency of Algiers from the Ottomans in 1830, and French rule spread south from French Algeria and eastwards from Senegal into the upper Niger to include present-day Algeria, Chad, Mali then French Sudan including Timbuktu (1893), Mauritania, Morocco (1912), Niger, and Tunisia (1881). By the beginning of the 20th century, the trans-Saharan trade had clearly declined because goods were moved through more modern and efficient means, such as airplanes, rather than across the desert.[103]\n\nThe French took advantage of long-standing animosity between the Chaamba Arabs and the Tuareg. The newly raised Méhariste camel corps were originally recruited mainly from the Chaamba nomadic tribe. In 1902, the French penetrated the Hoggar Mountains and defeated Ahaggar Tuareg in the battle of Tit.\n\nThe French Colonial Empire was the dominant presence in the Sahara. It established regular air links from Toulouse (HQ of famed Aéropostale), to Oran and over the Hoggar to Timbuktu and West to Bamako and Dakar, as well as trans-Sahara bus services run by La Compagnie Transsaharienne (est. 1927).[104] A remarkable film shot by famous aviator Captain René Wauthier in 1933 documents the first crossing by a large truck convoy from Algiers to Tchad, across the Sahara.[105]\n\nEgypt, under Muhammad Ali and his successors, conquered Nubia in 1820–22, founded Khartoum in 1823, and conquered Darfur in 1874. Egypt, including Sudan, became a British protectorate in 1882. Egypt and Britain lost control of the Sudan from 1882 to 1898 as a result of the Mahdist War. After its capture by British troops in 1898, the Sudan became an Anglo-Egyptian condominium.\n\nSpain captured present-day Western Sahara after 1874, although Rio del Oro remained largely under Sahrawi influence. In 1912, Italy captured parts of what was to be named Libya from the Ottomans. To promote the Roman Catholic religion in the desert, Pope Pius IX appointed a delegate Apostolic of the Sahara and the Sudan in 1868; later in the 19th century his jurisdiction was reorganized into the Vicariate Apostolic of Sahara.\n\nEgypt became independent of Britain in 1936, although the Anglo-Egyptian treaty of 1936 allowed Britain to keep troops in Egypt and to maintain the British-Egyptian condominium in the Sudan. British military forces were withdrawn in 1954.\n\nMost of the Saharan states achieved independence after World War II: Libya in 1951; Morocco, Sudan, and Tunisia in 1956; Chad, Mali, Mauritania, and Niger in 1960; and Algeria in 1962. Spain withdrew from Western Sahara in 1975, and it was partitioned between Mauritania and Morocco. Mauritania withdrew in 1979; Morocco continues to hold the territory (see Western Sahara conflict).[106]\n\nTuareg people in Mali rebelled several times during the 20th century before finally forcing the Malian armed forces to withdraw below the line demarcating Azawad from southern Mali during the 2012 rebellion.[107] Islamist rebels in the Sahara calling themselves al-Qaeda in the Islamic Maghreb have stepped up their violence in recent years.[108]\n\nIn the post–World War II era, several mines and communities have developed to use the desert's natural resources. These include large deposits of oil and natural gas in Algeria and Libya, and large deposits of phosphates in Morocco and Western Sahara.[109] Libya's Great Man-Made River is the world's largest irrigation project.[110] The project uses a pipeline system that pumps fossil water from the Nubian Sandstone Aquifer System to cities in the populous Libyan northern Mediterranean coast including Tripoli and Benghazi.[111]\n\nA number of Trans-African highways have been proposed across the Sahara, including the Cairo–Dakar Highway along the Atlantic coast, the Trans-Sahara Highway from Algiers on the Mediterranean to Kano in Nigeria, the Tripoli – Cape Town Highway from Tripoli in Libya to N'Djamena in Chad, and the Cairo – Cape Town Highway which follows the Nile. Each of these highways is partially complete, with significant gaps and unpaved sections.\n\nThe people of the Sahara are of various origins. Among them the Amazigh including the Tuareg, various Arabized Amaziɣ groups such as the Hassaniya-speaking Sahrawis, whose populations include the Znaga, a tribe whose name is a remnant of the pre-historic Zenaga language. Other major groups of people include the: Toubou, Nubians, Zaghawa, Kanuri, Hausa, Songhai, Beja, and Fula/Fulani (French: Peul; Fula: Fulɓe). The archaeological evidence from the Holocene period has shown that Nilo-Saharan speaking groups had populated the central and southern Sahara before the influx of Berber and Arabic speakers, around 1500 years ago, who now largely populate the Sahara in the modern era.[112]\n\nArabic dialects are the most widely spoken languages in the Sahara. Arabic, Berber and its variants now regrouped under the term Amazigh (which includes the Guanche language spoken by the original Berber inhabitants of the Canary Islands) and Beja languages are part of the Afro-Asiatic or Hamito-Semitic family.[citation needed] Unlike neighboring West Africa and the central governments of the states that comprise the Sahara, the French language bears little relevance to inter-personal discourse and commerce within the region, its people retaining staunch ethnic and political affiliations with Tuareg and Berber leaders and culture.[113] The legacy of the French colonial era administration is primarily manifested in the territorial reorganization enacted by the Third and Fourth republics, which engendered artificial political divisions within a hitherto isolated and porous region.[114] Diplomacy with local clients was conducted primarily in Arabic, which was the traditional language of bureaucratic affairs. Mediation of disputes and inter-agency communication was served by interpreters contracted by the French government, who, according to Keenan, \"documented a space of intercultural mediation,\" contributing much to preserving the indigenous cultural identities in the region.[115]\n"
    },
    {
        "title": "Classical music",
        "url": "https://en.wikipedia.org/wiki/Classical_music",
        "content": "\n\nClassical music generally refers to the art music of the Western world, considered to be distinct from Western folk music or popular music traditions. It is sometimes distinguished as Western classical music, as the term \"classical music\" can also be applied to non-Western art musics. Classical music is often characterized by formality and complexity in its musical form and harmonic organization,[1] particularly with the use of polyphony.[2] Since at least the ninth century it has been primarily a written tradition,[2] spawning a sophisticated notational system, as well as accompanying literature in analytical, critical, historiographical, musicological and philosophical practices. A foundational component of Western culture, classical music is frequently seen from the perspective of individual or groups of composers, whose compositions, personalities and beliefs have fundamentally shaped its history.\n\nRooted in the patronage of churches and royal courts in Western Europe,[1] surviving early medieval music is chiefly religious, monophonic and vocal, with the music of ancient Greece and Rome influencing its thought and theory. The earliest extant music manuscripts date from the Carolingian Empire (800–888),[3] around the time which Western plainchant gradually unified into what is termed Gregorian chant.[4] Musical centers existed at the Abbey of Saint Gall, the Abbey of Saint Martial and Saint Emmeram's Abbey, while the 11th century saw the development of staff notation and increasing output from medieval music theorists. By the mid-12th century France became the major European musical center:[3] The religious Notre-Dame school first fully explored organized rhythms and polyphony, while secular music flourished with the troubadour and trouvère traditions led by poet-musician nobles.[5] This culminated in the court sponsored French ars nova and Italian Trecento, which evolved into ars subtilior, a stylistic movement of extreme rhythmic diversity.[5] Beginning in the early 15th century, Renaissance composers of the influential Franco-Flemish School built off the harmonic principles in the English contenance angloise, bringing choral music to new standards, particularly the mass and motet.[6] Northern Italy soon emerged as the central musical region, where the Roman School engaged in highly sophisticated methods of polyphony in genres such as the madrigal,[6] which inspired the brief English Madrigal School.\n\nThe Baroque period (1580–1750) saw the relative standardization of common-practice tonality,[7] as well as the increasing importance of musical instruments, which grew into ensembles of considerable size. Italy remained dominant, being the birthplace of opera, the soloist centered concerto genre, the organized sonata form as well as the large scale vocal-centered genres of oratorio and cantata. The fugue technique championed by Johann Sebastian Bach exemplified the Baroque tendency for complexity, and as a reaction the simpler and song-like galant music and empfindsamkeit styles were developed. In the shorter but pivotal Classical period (1730–1820) composers such as Wolfgang Amadeus Mozart, Joseph Haydn, and Ludwig van Beethoven created widely admired representatives of absolute music,[8][9] including symphonies, string quartets and concertos. The subsequent Romantic music (1800–1910) focused instead on programmatic music, for which the art song, symphonic poem and various piano genres were important vessels. During this time virtuosity was celebrated, immensity was encouraged, while philosophy and nationalism were embedded—all aspects that converged in the operas of Richard Wagner. By the 20th century, stylistic unification gradually dissipated while the prominence of popular music greatly increased. Many composers actively avoided past techniques and genres in the lens of modernism, with some abandoning tonality in place of serialism, while others found new inspiration in folk melodies or impressionist sentiments. After World War II, for the first time audience members valued older music over contemporary works, a preference which has been catered to by the emergence and widespread availability of commercial recordings.[10] Trends of the mid-20th century to the present day include New Simplicity, New Complexity, Minimalism, Spectral music, and more recently Postmodern music and Postminimalism. Increasingly global, practitioners from the Americas, Africa and Asia have obtained crucial roles,[3] while symphony orchestras and opera houses now appear across the world.\n\nBoth the English term classical and the German equivalent Klassik developed from the French classique, itself derived from the Latin word classicus, which originally referred to the highest class of Ancient Roman citizens.[11][n 1] In Roman usage, the term later became a means to distinguish revered literary figures;[11] the Roman author Aulus Gellius commended writers such as Demosthenes and Virgil as classicus.[13] By the Renaissance, the adjective had acquired a more general meaning: an entry in Randle Cotgrave's 1611 A Dictionarie of the French and English Tongues is among the earliest extant definitions, translating classique as \"classical, formall  [sic], orderlie, in due or fit ranke; also, approved, authenticall, chiefe, principall\".[11][14] The musicologist Daniel Heartz summarizes this into two definitions: 1) a \"formal discipline\" and 2) a \"model of excellence\".[11] Like Gellius, later Renaissance scholars who wrote in Latin used classicus in reference to writers of classical antiquity;[12][n 2] however, this meaning only gradually developed, and was for a while subordinate to the broader classical ideals of formality and excellence.[15] Literature and visual arts—for which substantial Ancient Greek and Roman examples existed—did eventually adopt the term \"classical\" as relating to classical antiquity, but virtually no music of that time was available to Renaissance musicians, limiting the connection between classical music and the Greco-Roman world.[15][n 3]\n\nIt was in 18th-century England that the term 'classical' \"first came to stand for a particular canon of works in performance.\"[15] London had developed a prominent public concert music scene, unprecedented and unmatched by other European cities.[11] The royal court had gradually lost its monopoly on music, in large part from instability that the Commonwealth of England's dissolution and the Glorious Revolution enacted on court musicians.[11][n 4] In 1672, the former court musician John Banister began giving popular public concerts at a London tavern;[n 5] his popularity rapidly inaugurated the prominence of public concerts in the London.[19] The conception of \"classical\"—or more often \"ancient music\"—emerged, which was still built on the principles of formality and excellence, and according to Heartz \"civic ritual, religion and moral activism figured significantly in this novel construction of musical taste\".[15] The performance of such music was specialized by the Academy of Ancient Music and later at the Concerts of Antient Music series, where the work of select 16th and 17th composers was featured,[20] especially George Frideric Handel.[15][n 6] In France, the reign of Louis XIV (r. 1638–1715) saw a cultural renaissance, by the end of which writers such as Molière, Jean de La Fontaine and Jean Racine were considered to have surpassed the achievements of classical antiquity.[21] They were thus characterized as \"classical\", as was the music of Jean-Baptiste Lully (and later Christoph Willibald Gluck), being designated as \"l'opéra française classique\".[21] In the rest of continental Europe, the abandonment of defining \"classical\" as analogous to the Greco-Roman World was slower, primarily because the formation of canonical repertoires was either minimal or exclusive to the upper classes.[15]\n\nMany European commentators of the early 19th century found new unification in their definition of classical music: to juxtapose the older composers Wolfgang Amadeus Mozart, Joseph Haydn, and (excluding some of his later works) Ludwig van Beethoven as \"classical\" against the emerging style of Romantic music.[22][23][24] These three composers in particular were grouped into the First Viennese School, sometimes called the \"Viennese classics\",[n 7] a coupling that remains problematic by reason of none of the three being born in Vienna and the minimal time Haydn and Mozart spent in the city.[25] While this was an often expressed characterization, it was not a strict one. In 1879 the composer Charles Kensington Salaman defined the following composers as classical: Bach, Handel, Haydn, Mozart, Beethoven, Weber, Spohr and Mendelssohn.[26] More broadly, some writers used the term \"classical\" to generally praise well-regarded outputs from various composers, particularly those who produced many works in an established genre.[11][n 8]\n\nThe contemporary understanding of the term \"classical music\" remains vague and multifaceted.[30][31] Other terms such as \"art music\", \"canonic music\", \"cultivated music\" and \"serious music\" are largely synonymous.[32] The term \"classical music\" is often indicated or implied to concern solely the Western world,[33] and conversely, in many academic histories the term \"Western music\" excludes non-classical Western music.[34][n 9] Another complication lies in that \"classical music\" is sometimes used to describe non-Western art music exhibiting similar long-lasting and complex characteristics; examples include Indian classical music (i.e. Carnatic Music Hindustani music and Odissi Music), Gamelan music, and various styles of the court of Imperial China (see yayue for instance).[1] Thus in the later 20th century terms such as \"Western classical music\" and \"Western art music\" came in use to address this.[33] The musicologist Ralph P. Locke notes that neither term is ideal, as they create an \"intriguing complication\" when considering \"certain practitioners of Western-art music genres who come from non-Western cultures\".[36][n 10]\n\nComplexity in musical form and harmonic organization are typical traits of classical music.[1] The Oxford English Dictionary (OED) offers three definitions for the word \"classical\" in relation to music:[27]\n\nThe last definition concerns what is now termed the Classical period, a specific stylistic era of European music from the second half of the 18th century to the beginning of the 19th century.[37]\n\nThe Western classical tradition formally begins with music created by and for the early Christian Church.[38] It is probable that the early Church wished to disassociate itself from the predominant music of ancient Greece and Rome, as it was a reminder of the pagan religion it had persecuted and by which it had been persecuted.[38] As such, it remains unclear as to what extent the music of the Christian Church, and thus Western classical music as a whole, was influenced by preceding ancient music.[39] The general attitude towards music was adopted from the Ancient Greek and Roman music theorists and commentators.[40][n 11] Just as in Greco-Roman society, music was considered central to education; along with arithmetic, geometry and astronomy, music was included in the quadrivium, the four subjects of the upper division of a standard liberal arts education in the Middle Ages.[42] This high regard for music was first promoted by the scholars Cassiodorus, Isidore of Seville,[43] and particularly Boethius,[44] whose transmission and expansion on the perspectives of music from Pythagoras, Aristotle and Plato were crucial in the development of medieval musical thought.[45] However, scholars, medieval music theorists and composers regularly misinterpreted or misunderstood the writings of their Greek and Roman predecessors.[46] This was due to the complete absence of surviving Greco-Roman musical works available to medieval musicians,[46][n 12] to the extent that Isidore of Seville (c. 559 – 636) stated \"unless sounds are remembered by man, they perish, for they cannot be written down\", unaware of the systematic notational practices of Ancient Greece centuries before.[47][n 13] The musicologist Gustave Reese notes, however, that many Greco-Roman texts can still be credited as influential to Western classical music, since medieval musicians regularly read their works—regardless of whether they were doing so correctly.[46]\n\nHowever, there are some indisputable musical continuations from the ancient world.[48] Basic aspects such as monophony, improvisation and the dominance of text in musical settings are prominent in both early medieval and music of nearly all ancient civilizations.[49] Greek influences in particular include the church modes (which were descendants of developments by Aristoxenus and Pythagoras),[50] basic acoustical theory from pythagorean tuning,[39] as well as the central function of tetrachords.[51] Ancient Greek instruments such as the aulos (a reed instrument) and the lyre (a stringed instrument similar to a small harp) eventually led to several modern-day instruments of a symphonic orchestra.[52] However, Donald Jay Grout notes that attempting to create a direct evolutionary connection from the ancient music to early medieval is baseless, as it was almost solely influenced by Greco-Roman music theory, not performance or practice.[53]\n\nMedieval music includes Western European music from after the fall of the Western Roman Empire by 476 to about 1400. Monophonic chant, also called plainsong or Gregorian chant, was the dominant form until about 1100.[54] Christian monks developed the first forms of European musical notation in order to standardize liturgy throughout the Church.[55][56] Polyphonic (multi-voiced) music developed from monophonic chant throughout the late Middle Ages and into the Renaissance, including the more complex voicings of motets. During the earlier medieval period, the vocal music from the liturgical genre, predominantly Gregorian chant, was monophonic, using a single, unaccompanied vocal melody line.[57] Polyphonic vocal genres, which used multiple independent vocal melodies, began to develop during the high medieval era, becoming prevalent by the later 13th and early 14th century. Notable Medieval composers include Hildegard of Bingen, Léonin, Pérotin, Philippe de Vitry, Guillaume de Machaut, Francesco Landini, and Johannes Ciconia.\n\nMany medieval musical instruments still exist, but in different forms. Medieval instruments included the flute, the recorder and plucked string instruments like the lute. As well, early versions of the organ and fiddle (or vielle) existed. Medieval instruments in Europe had most commonly been used singly, often self accompanied with a drone note, or occasionally in parts. From at least as early as the 13th century through the 15th century there was a division of instruments into haut (loud, shrill, outdoor instruments) and bas (quieter, more intimate instruments).[58] A number of instrument have roots in Eastern predecessors that were adopted from the medieval Islamic world.[59] For example, the Arabic rebab is the ancestor of all European bowed string instruments, including the lira, rebec and violin.[60][61]\n\nThe musical Renaissance era lasted from 1400 to 1600. It was characterized by greater use of instrumentation, multiple interweaving melodic lines, and the use of earlier forms of bass instruments. Social dancing became more widespread, so musical forms appropriate to accompanying dance began to standardize. It is in this time that the notation of music on a staff and other elements of musical notation began to take shape.[62] This invention made possible the separation of the composition of a piece of music from its transmission; without written music, transmission was oral, and subject to change every time it was transmitted. With a musical score, a work of music could be performed without the composer's presence.[63] The invention of the movable-type printing press in the 15th century had far-reaching consequences on the preservation and transmission of music.[64]\n\nMany instruments originated during the Renaissance; others were variations of, or improvements upon, instruments that had existed previously. Some have survived to the present day; others have disappeared, only to be re-created in order to perform music on period instruments. As in the modern day, instruments may be classified as brass, strings, percussion, and woodwind. Brass instruments in the Renaissance were traditionally played by professionals who were members of Guilds and they included the slide trumpet, the wooden cornet, the valveless trumpet and the sackbut. Stringed instruments included the viol, the rebec, the harp-like lyre, the hurdy-gurdy, the lute, the guitar, the cittern, the bandora, and the orpharion. Keyboard instruments with strings included the harpsichord and the clavichord. Percussion instruments include the triangle, the Jew's harp, the tambourine, the bells, the rumble-pot, and various kinds of drums. Woodwind instruments included the double-reed shawm (an early member of the oboe family), the reed pipe, the bagpipe, the transverse flute, the recorder, the dulcian, and the crumhorn. Simple pipe organs existed, but were largely confined to churches, although there were portable varieties.[65] Printing enabled the standardization of descriptions and specifications of instruments, as well as instruction in their use.[66]\n\nVocal music in the Renaissance is noted for the flourishing of an increasingly elaborate polyphonic style. The principal liturgical forms which endured throughout the entire Renaissance period were masses and motets, with some other developments towards the end, especially as composers of sacred music began to adopt secular forms (such as the madrigal) for their own designs. Towards the end of the period, the early dramatic precursors of opera such as monody, the madrigal comedy, and the intermedio are seen. Around 1597, Italian composer Jacopo Peri wrote Dafne, the first work to be called an opera today. He also composed Euridice, the first opera to have survived to the present day.\n\nNotable Renaissance composers include Josquin des Prez, Giovanni Pierluigi da Palestrina, John Dunstaple, Johannes Ockeghem, Orlande de Lassus, Guillaume Du Fay, Gilles Binchois, Thomas Tallis, William Byrd, Giovanni Gabrieli, Carlo Gesualdo, John Dowland, Jacob Obrecht, Adrian Willaert, Jacques Arcadelt, and Cipriano de Rore.\n\nThe common practice period is typically defined as the era between the formation and the dissolution of common-practice tonality.[citation needed] The term usually spans roughly two-and-a-half centuries, encompassing the Baroque, Classical, and Romantic periods.\n\nBaroque music is characterized by the use of complex tonal counterpoint and the use of a basso continuo, a continuous bass line. Music became more complex in comparison with the simple songs of all previous periods.[67] The beginnings of the sonata form took shape in the canzona, as did a more formalized notion of theme and variations. The tonalities of major and minor as means for managing dissonance and chromaticism in music took full shape.[68]\n\nDuring the Baroque era, keyboard music played on the harpsichord and pipe organ became increasingly popular, and the violin family of stringed instruments took the form generally seen today. Opera as a staged musical drama began to differentiate itself from earlier musical and dramatic forms, and vocal forms like the cantata and oratorio became more common.[69] For the first time, vocalists began adding ornamentals to the music.[67]\n\nThe theories surrounding equal temperament began to be put in wider practice, as it enabled a wider range of chromatic possibilities in hard-to-tune keyboard instruments. Although J.S. Bach did not use equal temperament, changes in the temperaments from the then-common meantone system to various temperaments that made modulation between all keys musically acceptable made possible his Well-Tempered Clavier.[70]\n\nBaroque instruments included some instruments from the earlier periods (e.g., the hurdy-gurdy and recorder) and a number of new instruments (e.g., the oboe, bassoon, cello, contrabass and fortepiano). Some instruments from previous eras fell into disuse, such as the shawm, cittern, rackett, and the wooden cornet. The key Baroque instruments for strings included the violin, viol, viola, viola d'amore, cello, contrabass, lute, theorbo (which often played the basso continuo parts), mandolin, Baroque guitar, harp and hurdy-gurdy. Woodwinds included the Baroque flute, Baroque oboe, recorder and the bassoon. Brass instruments included the cornett, natural horn, natural trumpet, serpent and the trombone. Keyboard instruments included the clavichord, the tangent piano, the harpsichord, the pipe organ, and, later in the period, the fortepiano (an early version of the piano). Percussion instruments included the timpani, snare drum, tambourine and the castanets.\n\nOne major difference between Baroque music and the classical era that followed it is that the types of instruments used in Baroque ensembles were much less standardized. A Baroque ensemble could include one of several different types of keyboard instruments (e.g., pipe organ or harpsichord),[71] additional stringed chordal instruments (e.g., a lute), bowed strings, woodwinds, and brass instruments, and an unspecified number of bass instruments performing the basso continuo,(e.g., a cello, contrabass, viola, bassoon, serpent, etc.).\n\nVocal oeuvres of the Baroque era included suites such as oratorios and cantatas.[72][73] Secular music was less common, and was typically characterized only by instrumental music. Like Baroque art,[74] themes were generally sacred and for the purpose of a catholic setting.\n\nImportant composers of this era include Johann Sebastian Bach, Antonio Vivaldi, George Frideric Handel, Johann Pachelbel, Henry Purcell, Claudio Monteverdi, Barbara Strozzi, Domenico Scarlatti, Georg Philipp Telemann, Arcangelo Corelli, Alessandro Scarlatti, Jean-Philippe Rameau, Jean-Baptiste Lully, and Heinrich Schütz.\n\nThough the term \"classical music\" includes all Western art music from the medieval era to the early 2010s, the Classical era was the period of Western art music from the 1750s to the early 1820s[75]—the era of Wolfgang Amadeus Mozart, Joseph Haydn, and Ludwig van Beethoven.\n\nThe Classical era established many of the norms of composition, presentation, and style, and when the piano became the predominant keyboard instrument. The basic forces required for an orchestra became somewhat standardized (though they would grow as the potential of a wider array of instruments was developed). Chamber music grew to include ensembles with as many as 8-10 performers for serenades. Opera continued to develop, with regional styles in Italy, France, and German-speaking lands. The opera buffa, a form of comic opera, rose in popularity. The symphony came into its own as a musical form, and the concerto was developed as a vehicle for displays of virtuoso playing skill. Orchestras no longer required a harpsichord, and were often led by the lead violinist (now called the concertmaster).[76]\n\nClassical era musicians continued to use many of the instruments from the Baroque era, such as the cello, contrabass, recorder, trombone, timpani, fortepiano (the precursor to the modern piano) and organ. While some Baroque instruments fell into disuse e.g. the theorbo and rackett, many Baroque instruments were changed into the versions still in use today, such as the Baroque violin (which became the violin), Baroque oboe (which became the oboe) and Baroque trumpet, which transitioned to the regular valved trumpet. During the Classical era, the stringed instruments used in orchestra and chamber music such as string quartets were standardized as the four instruments which form the string section of the orchestra: the violin, viola, cello, and double bass. Baroque-era stringed instruments such as fretted, bowed viols were phased out. Woodwinds included the basset clarinet, basset horn, clarinette d'amour, the Classical clarinet, the chalumeau, the flute, oboe and bassoon. Keyboard instruments included the clavichord and the fortepiano. While the harpsichord was still used in basso continuo accompaniment in the 1750s and 1760s, it fell out of use at the end of the century. Brass instruments included the buccin, the ophicleide (a replacement for the bass serpent, which was the precursor of the tuba) and the natural horn.\n\nWind instruments became more refined in the Classical era. While double-reed instruments like the oboe and bassoon became somewhat standardized in the Baroque, the clarinet family of single reeds was not widely used until Mozart expanded its role in orchestral, chamber, and concerto settings.[77]\n\nThe music of the Romantic era, from roughly the first decade of the 19th century to the early 20th century, was characterized by increased attention to an extended melodic line, as well as expressive and emotional elements, paralleling romanticism in other art forms. Musical forms began to break from the Classical era forms (even as those were being codified), with free-form pieces like nocturnes, fantasias, and preludes being written where accepted ideas about the exposition and development of themes were ignored or minimized.[78] The music became more chromatic, dissonant, and tonally colorful, with tensions (with respect to accepted norms of the older forms) about key signatures increasing.[79] The art song (or Lied) came to maturity in this era, as did the epic scales of grand opera, ultimately transcended by Richard Wagner's Ring cycle.[80]\n\nIn the 19th century, musical institutions emerged from the control of wealthy patrons, as composers and musicians could construct lives independent of the nobility. Increasing interest in music by the growing middle classes throughout western Europe spurred the creation of organizations for the teaching, performance, and preservation of music. The piano, which achieved its modern construction in this era (in part due to industrial advances in metallurgy) became widely popular with the middle class, whose demands for the instrument spurred many piano builders. Many symphony orchestras date their founding to this era.[79] Some musicians and composers were the stars of the day; some, like Franz Liszt and Niccolò Paganini, fulfilled both roles.[81]\n\nEuropean cultural ideas and institutions began to follow colonial expansion into other parts of the world. There was also a rise, especially toward the end of the era, of nationalism in music (echoing, in some cases, political sentiments of the time), as composers such as Edvard Grieg, Nikolai Rimsky-Korsakov, and Antonín Dvořák echoed traditional music of their homelands in their compositions.[82]\n\nIn the Romantic era, the modern piano, with a more powerful, sustained tone and a wider range took over from the more delicate-sounding fortepiano. In the orchestra, the existing Classical instruments and sections were retained (string section, woodwinds, brass, and percussion), but these sections were typically expanded to make a fuller, bigger sound. For example, while a Baroque orchestra may have had two double bass players, a Romantic orchestra could have as many as ten. \"As music grew more expressive, the standard orchestral palette just wasn't rich enough for many Romantic composers.\"[83]\n\nThe families of instruments used, especially in orchestras, grew larger; a process that climaxed in the early 20th century with very large orchestras used by late romantic and modernist composers. A wider array of percussion instruments began to appear. Brass instruments took on larger roles, as the introduction of rotary valves made it possible for them to play a wider range of notes. The size of the orchestra (typically around 40 in the Classical era) grew to be over 100.[79] Gustav Mahler's 1906 Symphony No. 8, for example, has been performed with over 150 instrumentalists and choirs of over 400.[84] New woodwind instruments were added, such as the contrabassoon, bass clarinet and piccolo and new percussion instruments were added, including xylophones, snare drums, celestas (a bell-like keyboard instrument), bells, and triangles,[83] large orchestral harps, and even wind machines for sound effects. Saxophones appear in some scores from the late 19th century onwards, usually featured as a solo instrument rather than as in integral part of the orchestra.\n\nThe Wagner tuba, a modified member of the horn family, appears in Richard Wagner's cycle Der Ring des Nibelungen. It also has a prominent role in Anton Bruckner's Symphony No. 7 in E Major and is also used in several late romantic and modernist works by Richard Strauss, Béla Bartók, and others[85] Cornets appear regularly in 19th century scores, alongside trumpets which were regarded as less agile, at least until the end of the century.\n\nProminent composers of this era include Ludwig van Beethoven, Pyotr Ilyich Tchaikovsky, Frédéric Chopin, Hector Berlioz, Franz Schubert, Robert Schumann, Felix Mendelssohn, Franz Liszt, Giuseppe Verdi, Richard Wagner, Johannes Brahms, Alexander Scriabin, Nikolai Medtner, Edvard Grieg, and Johann Strauss II. Gustav Mahler and Richard Strauss are commonly regarded as transitional composers whose music combines both late romantic and early modernist elements.\n\nAt the turn of the century, music was characteristically late romantic in style with its expressive melodies, complex harmonies, and expansive forms. This era was marked by the works of several composers who pushed forward post-romantic symphonic writing. Composers such as Gustav Mahler and Richard Strauss continued to develop the western classical tradition with expansive symphonies and operas, while the likes of Jean Sibelius and Vaughan Williams infused their compositions with nationalistic elements and influences from folk songs. Sergei Prokofiev began in this tradition but soon ventured into modernist territories. At the same time, the impressionist movement, spearheaded by Claude Debussy, was being developed in France, with Maurice Ravel as another notable pioneer.[86]\n\nModernist classical music encompasses many styles of composition that can be characterised as post romantic, impressionist, expressionist, and neoclassical. Modernism marked an era when many composers rejected certain values of the common practice period, such as traditional tonality, melody, instrumentation, and structure.[87] Some music historians regard musical modernism as an era extending from about 1890 to 1930.[88][89] Others consider that modernism ended with one or the other of the two world wars.[90] Still other authorities claim that modernism is not associated with any historical era, but rather is \"an attitude of the composer; a living construct that can evolve with the times\".[91] Despite its decline in the last third of the 20th century, there remained at the end of the century an active core of composers who continued to advance the ideas and forms of modernism, such as Pierre Boulez, Pauline Oliveros, Toru Takemitsu, George Benjamin, Jacob Druckman, Brian Ferneyhough, George Perle, Wolfgang Rihm, Richard Wernick, Richard Wilson, and Ralph Shapey.[92]\n\nTwo musical movements that were dominant during this time were the impressionist beginning around 1890 and the expressionist that started around 1908. It was a period of diverse reactions in challenging and reinterpreting older categories of music, innovations that lead to new ways of organizing and approaching harmonic, melodic, sonic, and rhythmic aspects of music, and changes in aesthetic worldviews in close relation to the larger identifiable period of modernism in the arts of the time. The operative word most associated with it is \"innovation\".[93] Its leading feature is a \"linguistic plurality\", which is to say that no single music genre ever assumed a dominant position.[94]\n\nThe orchestra continued to grow during the early years modernist era, peaking in the first two decades of the 20th century. Saxophones that appeared only rarely during the 19th century became more commonly used as supplementary instruments, but never became core members of the orchestra. While appearing only as featured solo instruments in some works, for example Maurice Ravel's orchestration of Modest Mussorgsky's Pictures at an Exhibition and Sergei Rachmaninoff's Symphonic Dances, the saxophone is included in other works such as Sergei Prokofiev's Romeo and Juliet Suites 1 and 2 and many other works as a member of the orchestral ensemble. In some compositions such as Ravel's Boléro, two or more saxophones of different sizes are used to create an entire section like the other sections of the orchestra. The euphonium is featured in a few late Romantic and 20th century works, usually playing parts marked \"tenor tuba\", including Gustav Holst's The Planets, and Richard Strauss's Ein Heldenleben.\n\nProminent composers of the early 20th century include Igor Stravinsky, Claude Debussy, Sergei Rachmaninoff, Sergei Prokofiev, Arnold Schoenberg, Nikos Skalkottas, Heitor Villa-Lobos, Karol Szymanowski, Anton Webern, Alban Berg, Cécile Chaminade, Paul Hindemith, Aram Khachaturian, George Gershwin, Amy Beach, Béla Bartók, and Dmitri Shostakovich, along with the aforementioned Mahler and Strauss as transitional figures who carried over from the 19th century.\n\nPostmodern music is a period of music that began as early as 1930 according to some authorities.[88][89] It shares characteristics with postmodernist art – that is, art that comes after and reacts against modernism.\n\nSome other authorities have more or less equated postmodern music with the \"contemporary music\" composed well after 1930, from the late 20th century through to the early 21st century.[95][96] Some of the diverse movements of the postmodern/contemporary era include the neoromantic, neomedieval, minimalist, and post minimalist.\n\nContemporary classical music at the beginning of the 21st century was often considered to include all post-1945 musical forms.[97] A generation later, this term now properly refers to the music of today written by composers who are still alive; music that came into prominence in the mid-1970s. It includes different variations of modernist, postmodern, neoromantic, and pluralist music.[92]\n\nPerformers who have studied classical music extensively are said to be \"classically trained\". This training may come from private lessons from instrument or voice teachers or from completion of a formal program offered by a Conservatory, college or university, such as a Bachelor of Music or Master of Music degree (which includes individual lessons from professors). In classical music, \"...extensive formal music education and training, often to postgraduate [Master's degree] level\" is required.[98]\n\nPerformance of classical music repertoire requires a proficiency in sight-reading and ensemble playing, harmonic principles, strong ear training (to correct and adjust pitches by ear), knowledge of performance practice (e.g., Baroque ornamentation), and a familiarity with the style/musical idiom expected for a given composer or musical work (e.g., a Brahms symphony or a Mozart concerto).[citation needed]\n\nThe key characteristic of European classical music that distinguishes it from popular music, folk music, and some other classical music traditions such as Indian classical music, is that the repertoire tends to be written down in musical notation, creating a musical part or score. This score typically determines details of rhythm, pitch, and, where two or more musicians (whether singers or instrumentalists) are involved, how the various parts are coordinated. The written quality of the music has enabled a high level of complexity within them: fugues, for instance, achieve a remarkable marriage of boldly distinctive melodic lines weaving in counterpoint yet creating a coherent harmonic logic. The use of written notation also preserves a record of the works and enables Classical musicians to perform music from many centuries ago.\n\nAlthough Classical music in the 2000s has lost most of its tradition for musical improvisation, from the Baroque era to the Romantic era, there are examples of performers who could improvise in the style of their era. In the Baroque era, organ performers would improvise preludes, keyboard performers playing harpsichord would improvise chords from the figured bass symbols beneath the bass notes of the basso continuo part and both vocal and instrumental performers would improvise musical ornaments.[99] Johann Sebastian Bach was particularly noted for his complex improvisations.[100] During the Classical era, the composer-performer Wolfgang Amadeus Mozart was noted for his ability to improvise melodies in different styles.[101] During the Classical era, some virtuoso soloists would improvise the cadenza sections of a concerto. During the Romantic era, Ludwig van Beethoven would improvise at the piano.[102]\n\nAlmost all of the composers who are described in music textbooks on classical music and whose works are widely performed as part of the standard concert repertoire are male composers, even though there have been a large number of women composers throughout the history of classical music. Musicologist Marcia Citron has asked \"[w]hy is music composed by women so marginal to the standard 'classical' repertoire?\"[103] Citron \"examines the practices and attitudes that have led to the exclusion of women composers from the received 'canon' of performed musical works\". She argues that in the 1800s, women composers typically wrote art songs for performance in small recitals rather than symphonies intended for performance with an orchestra in a large hall, with the latter works being seen as the most important genre for composers; since women composers did not write many symphonies, they were deemed not to be notable as composers.[103] In the \"...Concise Oxford History of Music, Clara S[c]humann is one of the only female composers mentioned.\"[104] Abbey Philips states that \"[d]uring the 20th century the women who were composing/playing gained far less attention than their male counterparts.\"[104]\n\nHistorically, major professional orchestras have been mostly or entirely composed of musicians who are men. Some of the earliest cases of women being hired in professional orchestras was in the position of harpist. The Vienna Philharmonic, for example, did not accept women to permanent membership until 1997, far later than the other orchestras ranked among the world's top five by Gramophone in 2008.[105][n 14] The last major orchestra to appoint a woman to a permanent position was the Berlin Philharmonic.[109] As late as February 1996, the Vienna Philharmonic's principal flute, Dieter Flury, told Westdeutscher Rundfunk that accepting women would be \"gambling with the emotional unity (emotionelle Geschlossenheit) that this organism currently has\".[110] In April 1996, the orchestra's press secretary wrote that \"compensating for the expected leaves of absence\" of maternity leave would be a problem.[111]\n\nIn 2013, an article in Mother Jones stated that while \"[m]any prestigious orchestras have significant female membership—women outnumber men in the New York Philharmonic's violin section—and several renowned ensembles, including the National Symphony Orchestra, the Detroit Symphony, and the Minnesota Orchestra, are led by women violinists\", the double bass, brass, and percussion sections of major orchestras \"...are still predominantly male\".[112] A 2014 BBC article stated that the \"...introduction of 'blind' auditions, where a prospective instrumentalist performs behind a screen so that the judging panel can exercise no gender or racial prejudice, has seen the gender balance of traditionally male-dominated symphony orchestras gradually shift.\"[113]\n\nClassical music has often incorporated elements or material from popular music of the composer's time. Examples include occasional music such as Brahms' use of student drinking songs in his Academic Festival Overture, genres exemplified by Kurt Weill's The Threepenny Opera, and the influence of jazz on early and mid-20th-century composers including Maurice Ravel, exemplified by the movement entitled \"Blues\" in his sonata for violin and piano.[114] Some postmodern, minimalist and postminimalist classical composers acknowledge a debt to popular music.[115][failed verification]\n\nGeorge Gershwin's 1924 orchestral composition Rhapsody in Blue has been described as orchestral jazz or symphonic jazz. The composition combines elements of classical music with jazz-influenced effects.\n\nNumerous examples show influence in the opposite direction, including popular songs based on classical music, the use to which Pachelbel's Canon has been put since the 1970s, and the musical crossover phenomenon, where classical musicians have achieved success in the popular music arena.[116] In heavy metal, a number of lead guitarists (playing electric guitar), including Ritchie Blackmore and Randy Rhoads,[117] modeled their playing styles on Baroque or Classical-era instrumental music.[118]\n\nComposers of classical music have often made use of folk music (music created by musicians who are commonly not classically trained, often from a purely oral tradition). Some composers, like Dvořák and Smetana,[119] have used folk themes to impart a nationalist flavor to their work, while others like Bartók have used specific themes lifted whole from their folk-music origins.[120] Khachaturian widely incorporated into his work the folk music of his native Armenia, but also other ethnic groups of the Middle East and Eastern Europe.[121][122]\n\nCertain staples of classical music are often used commercially (either in advertising or in movie soundtracks). In television commercials, several passages have become clichéd, particularly the opening of Richard Strauss' Also sprach Zarathustra (made famous in the film 2001: A Space Odyssey) and the opening section \"O Fortuna\" of Carl Orff's Carmina Burana; other examples include the \"Dies irae\" from the Verdi Requiem, Edvard Grieg's \"In the Hall of the Mountain King\" from Peer Gynt, the opening bars of Beethoven's Symphony No. 5, Aram Khachaturian's \"Sabre Dance\", Wagner's \"Ride of the Valkyries\" from Die Walküre, Rimsky-Korsakov's \"Flight of the Bumblebee\", and excerpts of Aaron Copland's Rodeo.[citation needed] Several works from the Golden Age of Animation matched the action to classical music. Notable examples are Walt Disney's Fantasia, Tom and Jerry's Johann Mouse, and Warner Bros.' Rabbit of Seville and What's Opera, Doc?\n\nSimilarly, movies and television often use standard, clichéd excerpts of classical music to convey refinement or opulence: some of the most-often heard pieces in this category include Bach's Cello Suite No. 1, Mozart's Eine kleine Nachtmusik, Vivaldi's Four Seasons, Mussorgsky's Night on Bald Mountain (as orchestrated by Rimsky-Korsakov), and Rossini's \"William Tell Overture\". Shawn Vancour argues the commercialization of classical music in the 1920s may have harmed the music industry.[123]\n\nDuring the 1990s, several research papers and popular books wrote on what came to be called the \"Mozart effect\": an observed temporary, small elevation of scores on spatial reasoning tests as a result of listening to Mozart's music. The approach has been popularized in a book by Don Campbell, and is based on an experiment published in Nature suggesting that listening to Mozart temporarily boosted students' IQ by 8 to 9 points.[124] This popularized version of the theory was expressed succinctly by the New York Times music columnist Alex Ross: \"researchers... have determined that listening to Mozart actually makes you smarter.\"[125] Promoters marketed CDs claimed to induce the effect. Florida passed a law requiring toddlers in state-run schools to listen to classical music every day, and in 1998 the governor of Georgia budgeted $105,000 per year to provide every child born in Georgia with a tape or CD of classical music. One of the co-authors of the original studies of the Mozart effect commented \"I don't think it can hurt. I'm all for exposing children to wonderful cultural experiences. But I do think the money could be better spent on music education programs.\"[126]\n"
    },
    {
        "title": "Impressionism",
        "url": "https://en.wikipedia.org/wiki/Impressionism",
        "content": "\n\nImpressionism was a 19th-century art movement characterized by relatively small, thin, yet visible brush strokes, open composition, emphasis on accurate depiction of light in its changing qualities (often accentuating the effects of the passage of time), ordinary subject matter, unusual visual angles, and inclusion of movement as a crucial element of human perception and experience. Impressionism originated with a group of Paris-based artists whose independent exhibitions brought them to prominence during the 1870s and 1880s.\n\nThe Impressionists faced harsh opposition from the conventional art community in France. The name of the style derives from the title of a Claude Monet work, Impression, soleil levant (Impression, Sunrise), which provoked the critic Louis Leroy to coin the term in a satirical 1874 review of the First Impressionist Exhibition published in the Parisian newspaper Le Charivari.[1] The development of Impressionism in the visual arts was soon followed by analogous styles in other media that became known as Impressionist music and Impressionist literature.\n\nRadicals in their time, the early Impressionists violated the rules of academic painting. They constructed their pictures from freely brushed colours that took precedence over lines and contours, following the example of painters such as Eugène Delacroix and J. M. W. Turner. They also painted realistic scenes of everyday life in natural settings, often outdoors, attempting to capture a moment as experienced. \n\nPreviously, paintings were accomplished in studio, whether landscape art, still life or portrait, with an emphasis on verisimilitude.[a] The Impressionists found that they could capture the momentary and transient effects of sunlight by painting outdoors or en plein air. They portrayed overall visual effects instead of details, and used short \"broken\" brush strokes of mixed and pure unmixed colour—not blended smoothly or shaded, as was customary—to achieve an effect of intense colour vibration.[2]\n\nImpressionism emerged in France at the same time that a number of other painters, including the Italian artists known as the Macchiaioli, and Winslow Homer in the United States, were also exploring plein-air painting. The Impressionists, however, developed new techniques specific to the style. Encompassing what its adherents argued was a different way of seeing, it is an art of immediacy and movement, of candid poses and compositions, of the play of light expressed in a bright and varied use of colour.[2] In 1876, the poet and critic Stéphane Mallarmé said of the new style: \"The represented subject, being composed of a harmony of reflected and ever-changing lights, cannot be supposed always to look the same but palpitates with movement, light, and life\".[4]\n\nThe public, at first hostile, gradually came to believe that the Impressionists had captured a fresh and original vision, even if the art critics and art establishment disapproved of the new style. By recreating the sensation in the eye that views the subject, rather than delineating the details of the subject, and by creating a welter of techniques and forms, Impressionism is a precursor of various painting styles, including Post-Impressionism, Fauvism, and Cubism.[5]\n\nIn the middle of the 19th century—a time of rapid industrialization and unsettling social change in France, as Emperor Napoleon III rebuilt Paris and waged war—the Académie des Beaux-Arts dominated French art.[6] The Académie was the preserver of traditional French painting standards of content and style. Historical subjects, religious themes, and portraits were valued; landscape and still life were not. The Académie preferred carefully finished images that looked realistic when examined closely. Paintings in this style were made up of precise brush strokes carefully blended to hide the artist's hand in the work.[7] Colour was restrained and often toned down further by the application of a thick golden varnish.[8]\n\nThe Académie had an annual, juried art show, the Salon de Paris, and artists whose work was displayed in the show won prizes, garnered commissions, and enhanced their prestige. The standards of the juries represented the values of the Académie, represented by the works of such artists as Jean-Léon Gérôme and Alexandre Cabanel. Using an eclectic mix of techniques and formulas established in Western painting since the Renaissance—such as linear perspective and figure types derived from Classical Greek art—these artists produced escapist visions of a reassuringly ordered world.[9] By the 1850s, some artists, notably the Realist painter Gustave Courbet, had gained public attention and critical censure by depicting contemporary realities without the idealization demanded by the Académie.[10]\n\nIn the early 1860s, four young painters—Claude Monet, Pierre-Auguste Renoir, Alfred Sisley, and Frédéric Bazille—met while studying under the academic artist Charles Gleyre. They discovered that they shared an interest in painting landscape and contemporary life rather than historical or mythological scenes. Following a practice—pioneered by artists such as the Englishman John Constable—[11] that had become increasingly popular by mid-century, they often ventured into the countryside together to paint in the open air.[12] Their purpose was not to make sketches to be developed into carefully finished works in the studio, as was the usual custom, but to complete their paintings out-of-doors.[13]\n\nBy painting in sunlight directly from nature, and making bold use of the vivid synthetic pigments that had become available since the beginning of the century, they began to develop a lighter and brighter manner of painting that extended further the Realism of Courbet and the Barbizon school. A favourite meeting place for the artists was the Café Guerbois on Avenue de Clichy in Paris, where the discussions were often led by Édouard Manet, whom the younger artists greatly admired. They were soon joined by Camille Pissarro, Paul Cézanne, and Armand Guillaumin.[14]\n\nDuring the 1860s, the Salon jury routinely rejected about half of the works submitted by Monet and his friends in favour of works by artists faithful to the approved style.[2] In 1863, the Salon jury rejected Manet's The Luncheon on the Grass (Le déjeuner sur l'herbe) primarily because it depicted a nude woman with two clothed men at a picnic. While the Salon jury routinely accepted nudes in historical and allegorical paintings, they condemned Manet for placing a realistic nude in a contemporary setting.[15] The jury's severely worded rejection of Manet's painting appalled his admirers, and the unusually large number of rejected works that year perturbed many French artists.\n\nAfter Emperor Napoleon III saw the rejected works of 1863, he decreed that the public be allowed to judge the work themselves, and the Salon des Refusés (Salon of the Refused) was organized. While many viewers came only to laugh, the Salon des Refusés drew attention to the existence of a new tendency in art and attracted more visitors than the regular Salon.[16]\n\nArtists' petitions requesting a new Salon des Refusés in 1867, and again in 1872, were denied. In December 1873, Monet, Renoir, Pissarro, Sisley, Cézanne, Berthe Morisot, Edgar Degas and several other artists founded the Société anonyme des artistes peintres, sculpteurs, graveurs, etc.[b] to exhibit their artworks independently.[17][18] Members of the association were expected to forswear participation in the Salon.[19] The organizers invited a number of other progressive artists to join them in their inaugural exhibition, including the older Eugène Boudin, whose example had first persuaded Monet to adopt plein air painting years before.[20] Another painter who greatly influenced Monet and his friends, Johan Jongkind, declined to participate, as did Édouard Manet. In total, thirty artists participated in their first exhibition, held in April 1874 at the studio of the photographer Nadar.\n\nThe critical response was mixed. Monet and Cézanne received the harshest attacks. Critic and humorist Louis Leroy wrote a scathing review in the newspaper Le Charivari in which, making wordplay with the title of Claude Monet's Impression, Sunrise (Impression, soleil levant), he gave the artists the name by which they became known. Derisively titling his article \"The Exhibition of the Impressionists\", Leroy declared that Monet's painting was at most, a sketch, and could hardly be termed a finished work.\n\nHe wrote, in the form of a dialogue between viewers,\n\n\"Impression—I was certain of it. I was just telling myself that, since I was impressed, there had to be some impression in it ... and what freedom, what ease of workmanship! Wallpaper in its embryonic state is more finished than that seascape.\"[21]\nThe term Impressionist quickly gained favour with the public. It was also accepted by the artists themselves, even though they were a diverse group in style and temperament, unified primarily by their spirit of independence and rebellion. They exhibited together—albeit with shifting membership—eight times between 1874 and 1886. The Impressionists' style, with its loose, spontaneous brushstrokes, would soon become synonymous with modern life.[8]\n\nMonet, Sisley, Morisot, and Pissarro may be considered the \"purest\" Impressionists, in their consistent pursuit of an art of spontaneity, sunlight, and colour. Degas rejected much of this, as he believed in the primacy of drawing over colour and belittled the practice of painting outdoors.[22] Renoir turned away from Impressionism for a time during the 1880s, and never entirely regained his commitment to its ideas. Édouard Manet, although regarded by the Impressionists as their leader,[23] never abandoned his liberal use of black as a colour (while Impressionists avoided its use and preferred to obtain darker colours by mixing), and never participated in the Impressionist exhibitions. He continued to submit his works to the Salon, where his painting Spanish Singer had won a 2nd class medal in 1861, and he urged the others to do likewise, arguing that \"the Salon is the real field of battle\" where a reputation could be made.[24]\n\nAmong the artists of the core group (minus Bazille, who had died in the Franco-Prussian War in 1870), defections occurred as Cézanne, followed later by Renoir, Sisley, and Monet, abstained from the group exhibitions so they could submit their works to the Salon. Disagreements arose from issues such as Guillaumin's membership in the group, championed by Pissarro and Cézanne against opposition from Monet and Degas, who thought him unworthy.[25] Degas invited Mary Cassatt to display her work in the 1879 exhibition, but also insisted on the inclusion of Jean-François Raffaëlli, Ludovic Lepic, and other realists who did not represent Impressionist practices, causing Monet in 1880 to accuse the Impressionists of \"opening doors to first-come daubers\".[26]\n\nIn this regard, the seventh Paris Impressionist exhibition in 1882 was the most selective of all including the works of only nine \"true\" impressionists, namely Gustave Caillebotte, Paul Gauguin, Armand Guillaumin, Claude Monet, Berthe Morisot, Camille Pissarro, Pierre-Auguste Renoir, Alfred Sisley, and Victor Vignon. The group then divided again over the invitations to Paul Signac and Georges Seurat to exhibit with them at the 8th Impressionist exhibition in 1886. Pissarro was the only artist to show at all eight Paris Impressionist exhibitions.\n\nThe individual artists achieved few financial rewards from the Impressionist exhibitions, but their art gradually won a degree of public acceptance and support. Their dealer, Durand-Ruel, played a major role in this as he kept their work before the public and arranged shows for them in London and New York. Although Sisley died in poverty in 1899, Renoir had a great Salon success in 1879.[27] Monet became secure financially during the early 1880s and so did Pissarro by the early 1890s. By this time the methods of Impressionist painting, in a diluted form, had become commonplace in Salon art.[28]\n\nFrench painters who prepared the way for Impressionism include the Romantic colourist Eugène Delacroix; the leader of the realists, Gustave Courbet; and painters of the Barbizon school such as Théodore Rousseau. The Impressionists learned much from the work of Johan Barthold Jongkind, Jean-Baptiste-Camille Corot and Eugène Boudin, who painted from nature in a direct and spontaneous style that prefigured Impressionism, and who befriended and advised the younger artists.\n\nA number of identifiable techniques and working habits contributed to the innovative style of the Impressionists. Although these methods had been used by previous artists—and are often conspicuous in the work of artists such as Frans Hals, Diego Velázquez, Peter Paul Rubens, John Constable, and J. M. W. Turner—the Impressionists were the first to use them all together, and with such consistency. These techniques include:\n\nNew technology played a role in the development of the style. Impressionists took advantage of the mid-century introduction of premixed paints in tin tubes (resembling modern toothpaste tubes), which allowed artists to work more spontaneously, both outdoors and indoors.[29] Previously, painters made their own paints individually, by grinding and mixing dry pigment powders with linseed oil, which were then stored in animal bladders.[30]\n\nMany vivid synthetic pigments became commercially available to artists for the first time during the 19th century. These included cobalt blue, viridian, cadmium yellow, and synthetic ultramarine blue, all of which were in use by the 1840s, before Impressionism.[31] The Impressionists' manner of painting made bold use of these pigments, and of even newer colours such as cerulean blue,[8] which became commercially available to artists in the 1860s.[31]\n\nThe Impressionists' progress toward a brighter style of painting was gradual. During the 1860s, Monet and Renoir sometimes painted on canvases prepared with the traditional red-brown or grey ground.[32] By the 1870s, Monet, Renoir, and Pissarro usually chose to paint on grounds of a lighter grey or beige colour, which functioned as a middle tone in the finished painting.[32] By the 1880s, some of the Impressionists had come to prefer white or slightly off-white grounds, and no longer allowed the ground colour a significant role in the finished painting.[33]\n\nThe Impressionists reacted to modernity by exploring \"a wide range of non-academic subjects in art\" such as middle-class leisure activities and \"urban themes, including train stations, cafés, brothels, the theater, and dance.\"[34] They found inspiration in the newly widened avenues of Paris, bounded by new tall buildings that offered opportunities to depict bustling crowds, popular entertainments, and nocturnal lighting in artificially closed-off spaces.[35]\n\nA painting such as Caillebotte's Paris Street; Rainy Day (1877) strikes a modern note by emphasizing the isolation of individuals amid the outsized buildings and spaces of the urban environment.[36] When painting landscapes, the Impressionists did not hesitate to include the factories that were proliferating in the countryside. Earlier painters of landscapes had conventionally avoided smokestacks and other signs of industrialization, regarding them as blights on nature's order and unworthy of art.[37]\n\nPrior to the Impressionists, other painters, notably such 17th-century Dutch painters as Jan Steen, had emphasized common subjects, but their methods of composition were traditional. They arranged their compositions so that the main subject commanded the viewer's attention. J. M. W. Turner, while an artist of the Romantic era, anticipated the style of impressionism with his artwork.[38] The Impressionists relaxed the boundary between subject and background so that the effect of an Impressionist painting often resembles a snapshot, a part of a larger reality captured as if by chance.[39] Photography was gaining popularity, and as cameras became more portable, photographs became more candid. Photography inspired Impressionists to represent momentary action, not only in the fleeting lights of a landscape, but in the day-to-day lives of people.[40][41]\n\nThe development of Impressionism can be considered partly as a reaction by artists to the challenge presented by photography, which seemed to devalue the artist's skill in reproducing reality. Both portrait and landscape paintings were deemed somewhat deficient and lacking in truth as photography \"produced lifelike images much more efficiently and reliably\".[42]\n\nIn spite of this, photography actually inspired artists to pursue other means of creative expression, and rather than compete with photography to emulate reality, artists focused \"on the one thing they could inevitably do better than the photograph—by further developing into an art form its very subjectivity in the conception of the image, the very subjectivity that photography eliminated\".[42] The Impressionists sought to express their perceptions of nature, rather than create exact representations. This allowed artists to depict subjectively what they saw with their \"tacit imperatives of taste and conscience\".[43] Photography encouraged painters to exploit aspects of the painting medium, like colour, which photography then lacked: \"The Impressionists were the first to consciously offer a subjective alternative to the photograph\".[42]\n\nAnother major influence was Japanese ukiyo-e art prints (Japonism). The art of these prints contributed significantly to the \"snapshot\" angles and unconventional compositions that became characteristic of Impressionism. An example is Monet's Jardin à Sainte-Adresse, 1867, with its bold blocks of colour and composition on a strong diagonal slant showing the influence of Japanese prints.[45]\n\nEdgar Degas was both an avid photographer and a collector of Japanese prints.[46] His The Dance Class (La classe de danse) of 1874 shows both influences in its asymmetrical composition. The dancers are seemingly caught off guard in various awkward poses, leaving an expanse of empty floor space in the lower right quadrant. He also captured his dancers in sculpture, such as the Little Dancer of Fourteen Years.\n\nImpressionists, in varying degrees, were looking for ways to depict visual experience and contemporary subjects.[47] Female Impressionists were interested in these same ideals but had many social and career limitations compared to male Impressionists.[48] They were particularly excluded from the imagery of the bourgeois social sphere of the boulevard, cafe, and dance hall.[49]\n\nAs well as imagery, women were excluded from the formative discussions that resulted in meetings in those places. That was where male Impressionists were able to form and share ideas about Impressionism.[49] In the academic realm, women were believed to be incapable of handling complex subjects, which led teachers to restrict what they taught female students.[50] It was also considered unladylike to excel in art, since women's true talents were then believed to center on homemaking and mothering.[50]\n\nYet several women were able to find success during their lifetime, even though their careers were affected by personal circumstances – Bracquemond, for example, had a husband who was resentful of her work which caused her to give up painting.[51] The four most well known, namely, Mary Cassatt, Eva Gonzalès, Marie Bracquemond, and Berthe Morisot, are, and were, often referred to as the 'Women Impressionists'. Their participation in the series of eight Impressionist exhibitions that took place in Paris from 1874 to 1886 varied: Morisot participated in seven, Cassatt in four, Bracquemond in three, and Gonzalès did not participate.[51][52]\n\n\nThe critics of the time lumped these four together without regard to their personal styles, techniques, or subject matter.[53] Critics viewing their works at the exhibitions often attempted to acknowledge the women artists' talents but circumscribed them within a limited notion of femininity.[54] Arguing for the suitability of Impressionist technique to women's manner of perception, Parisian critic S.C. de Soissons wrote:\nOne can understand that women have no originality of thought, and that literature and music have no feminine character; but surely women know how to observe, and what they see is quite different from that which men see, and the art which they put in their gestures, in their toilet, in the decoration of their environment is sufficient to give is the idea of an instinctive, of a peculiar genius which resides in each one of them.[55]\nWhile Impressionism legitimized the domestic social life as subject matter, of which women had intimate knowledge, it also tended to limit them to that subject matter. Portrayals of often-identifiable sitters in domestic settings, which could offer commissions, were dominant in the exhibitions.[56] The subjects of the paintings were often women interacting with their environment by either their gaze or movement. Cassatt, in particular, was aware of her placement of subjects: she kept her predominantly female figures from objectification and cliche; when they are not reading, they converse, sew, drink tea, and when they are inactive, they seem lost in thought.[57]\n\nThe women Impressionists, like their male counterparts, were striving for \"truth\", for new ways of seeing and new painting techniques; each artist had an individual painting style.[58] Women Impressionists, particularly Morisot and Cassatt, were conscious of the balance of power between women and objects in their paintings – the bourgeois women depicted are not defined by decorative objects, but instead, interact with and dominate the things with which they live.[59] There are many similarities in their depictions of women who seem both at ease and subtly confined.[60] Gonzalès' Box at the Italian Opera depicts a woman staring into the distance, at ease in a social sphere but confined by the box and the man standing next to her. Cassatt's painting Young Girl at a Window is brighter in color but remains constrained by the canvas edge as she looks out the window.\n\nDespite their success in their ability to have a career and Impressionism's demise attributed to its allegedly feminine characteristics—its sensuality, dependence on sensation, physicality, and fluidity—the four women artists, and other, lesser-known women Impressionists, were largely omitted from art historical textbooks covering Impressionist artists until Tamar Garb's Women Impressionists published in 1986.[61] For example, Impressionism by Jean Leymarie, published in 1955 included no information on any women Impressionists.\n\nPainter Androniqi Zengo Antoniu is co-credited with the introduction of impressionism to Albania.[62]\n\nThe central figures in the development of Impressionism in France,[63][64] listed alphabetically, were:\n\nThe Impressionists\n\nAmong the close associates of the Impressionists, Victor Vignon is the only artist outside the group of prominent names who participated to the most exclusive Seventh Paris Impressionist Exhibition in 1882, which was indeed a rejection to the previous less restricted exhibitions chiefly organized by Degas. Originally from the school of Corot, Vignon was a friend of Camille Pissarro, whose influence is evident in his impressionist style after the late 1870s, and a friend of post-impressionist Vincent van Gogh.\n\nThere were several other close associates of the Impressionists who adopted their methods to some degree. These include Jean-Louis Forain, who participated in Impressionist exhibitions in 1879, 1880, 1881 and 1886,[68] and Giuseppe De Nittis, an Italian artist living in Paris who participated in the first Impressionist exhibit at the invitation of Degas, although the other Impressionists disparaged his work.[69] Federico Zandomeneghi was another Italian friend of Degas who showed with the Impressionists. Eva Gonzalès was a follower of Manet who did not exhibit with the group. \n\nJames Abbott McNeill Whistler was an American-born painter who played a part in Impressionism although he did not join the group and preferred grayed colours. Walter Sickert, an English artist, was initially a follower of Whistler, and later an important disciple of Degas. He did not exhibit with the Impressionists. In 1904, the artist and writer Wynford Dewhurst wrote the first important study of the French painters published in English, Impressionist Painting: its genesis and development, which did much to popularize Impressionism in Great Britain.\n\nBy the early 1880s, Impressionist methods were affecting, at least superficially, the art of the Salon. Fashionable painters such as Jean Béraud and Henri Gervex found critical and financial success by brightening their palettes while retaining the smooth finish expected of Salon art.[70] Works by these artists are sometimes casually referred to as Impressionism, despite their remoteness from Impressionist practice.\n\nThe influence of the French Impressionists lasted long after most of them had died. Artists like J.D. Kirszenbaum were borrowing Impressionist techniques throughout the twentieth century.\n\nAs the influence of Impressionism spread beyond France, artists, too numerous to list, became identified as practitioners of the new style. Some of the more important examples are:\n\nWhile Edgar Degas was primarily known as a painter in his lifetime, he began to pursue the medium of sculpture later in his artistic career in the 1880s. He created as many as 150 sculptures during his lifetime. Degas preferred the medium of wax for his sculptures because it allowed him to make changes, start over, and further explore the modelling process.[74] Only one of Degas's sculptures, Little Dancer of Fourteen Years, was exhibited in his lifetime, which was exhibited at the Sixth Impressionist Exhibition in 1881. Little Dancer proved to be controversial with critics. Some considered Degas to have overthrown sculptural traditions in the same way that Impressionism had overthrown the traditions of painting. Others found it to be ugly. [75] Following the Degas's death in 1917, his heirs authorized bronze castings from 73 of the artist's sculptures.[76]\n\nThe sculptor Auguste Rodin is sometimes called an Impressionist for the way he used roughly modeled surfaces to suggest transient light effects.[77] The sculptor Medardo Rosso has also been called an Impressionist.[78]\n\nSome Russian artists created Impressionistic sculptures of animals in order to break away from old world concepts. Their works have been described as endowing birds and beasts with new spiritual characteristics.[79]\n\nWhile his photographs are less known than his paintings or his sculptures, Edgar Degas also pursued photography later in his life. His photographs were never exhibited during his lifetime, and not much attention was given to them following his death. It was not until the late 20th century that scholars started to take interest in Degas's photographs.[80]\n\nPictorialist photographers, whose work is characterized by soft focus and atmospheric effects, have also been called Impressionists. These Impressionist photographers used various techniques such as photographing subjects out of focus, using soft focus lenses or pinhole lenses, and manipulating the gum bichromate process to create images that resembled Impressionist paintings.[81]\n\nFrench Impressionist Cinema is a term applied to a loosely defined group of films and filmmakers in France from 1919 to 1929, although these years are debatable. French Impressionist filmmakers include Abel Gance, Jean Epstein, Germaine Dulac, Marcel L'Herbier, Louis Delluc, and Dmitry Kirsanoff.\n\nMusical Impressionism is the name given to a movement in European classical music that arose in the late 19th century and continued into the middle of the 20th century. Originating in France, musical Impressionism is characterized by suggestion and atmosphere, and eschews the emotional excesses of the Romantic era. Impressionist composers favoured short forms such as the nocturne, arabesque, and prelude, and often explored uncommon scales such as the whole tone scale. Perhaps the most notable innovations of Impressionist composers were the introduction of major 7th chords and the extension of chord structures in 3rds to five- and six-part harmonies.\n\nThe influence of visual Impressionism on its musical counterpart is debatable. Claude Debussy and Maurice Ravel are generally considered the greatest Impressionist composers, but Debussy disavowed the term, calling it the invention of critics. Erik Satie was also considered in this category, though his approach was regarded as less serious, more musical novelty in nature. \n\nPaul Dukas is another French composer sometimes considered an Impressionist, but his style is perhaps more closely aligned to the late Romanticists. Musical Impressionism beyond France includes the work of such composers as Ottorino Respighi (Italy), Ralph Vaughan Williams, Cyril Scott, and John Ireland (England), Manuel De Falla and Isaac Albeniz (Spain), and Charles Griffes (America).\n\nAmerican Impressionist music differs from European Impressionist music, and these differences are mainly reflected in Charles Griffith's poetry of flute and orchestral music. He is also the most prolific Impressionist composer in the United States.[82]\n\nThe term Impressionism has also been used to describe works of literature in which a few select details suffice to convey the sensory impressions of an incident or scene. Impressionist literature is closely related to Symbolism, with its major exemplars being Baudelaire, Mallarmé, Rimbaud, and Verlaine. Authors such as Virginia Woolf, D.H. Lawrence, Henry James, and Joseph Conrad have written works that are Impressionistic in the way that they describe, rather than interpret, the impressions, sensations and emotions that constitute a character's mental life. Some literary scholars, such as John G. Peters, believe literary Impressionism is better defined by its philosophical stance than by any supposed relationship with Impressionist painting.[83]\n\nDuring the 1880s several artists began to develop different precepts for the use of colour, pattern, form, and line, derived from the Impressionist example: Vincent van Gogh, Paul Gauguin, Georges Seurat, and Henri de Toulouse-Lautrec. These artists were slightly younger than the Impressionists, and their work is known as post-Impressionism. Post-Impressionist artists reacted against the Impressionists' concern with realistically reproducing the optical sensations of light and colour; they turned instead toward symbolic content and the expression of emotion.[84]\n\nPost-Impressionism prefigured the characteristics of Futurism and Cubism, reflecting the change of attitude towards art in European society.[85] Some of the original Impressionist artists also ventured into this new territory; Camille Pissarro briefly painted in a pointillist manner, and even Monet abandoned strict plein air painting. Paul Cézanne, who participated in the first and third Impressionist exhibitions, developed a highly individual vision emphasising pictorial structure, and he is more often called a post-Impressionist. Although these cases illustrate the difficulty of assigning labels, the work of the original Impressionist painters may, by definition, be categorised as Impressionism.\n\nGeneral:\n"
    },
    {
        "title": "Film",
        "url": "https://en.wikipedia.org/wiki/Film",
        "content": "\n\n\n\nA film, also known as a movie or motion picture,[a] is a work of visual art that simulates experiences and otherwise communicates ideas, stories, perceptions, emotions, or atmosphere through the use of moving images that are generally, since the 1930s, synchronized with sound and (less commonly) other sensory stimulations.[1] The word \"cinema\" is orrowed from the French cinéma, an abbreviation of cinématographe (term coined by the Lumière brothers in the 1890s), from Ancient Greek meaning \"recording movement\". The word is today usually used to refer to either a purpose-built venue for screening films, known as a movie theater in the US; the film industry; the overall art form; or .filmmaking.\n\nThe moving images of a film are created by photographing actual scenes with a motion-picture camera, by photographing drawings or miniature models using traditional animation techniques, by means of CGI and computer animation, or by a combination of some or all of these techniques, and other visual effects.\n\nBefore the introduction of digital production, a series of still images were recorded on a strip of chemically sensitized celluloid (photographic film stock), usually at a rate of 24 frames per second. The images are transmitted through a movie projector at the same rate as they were recorded, with a Geneva drive ensuring that each frame remains still during its short projection time. A rotating shutter causes stroboscopic intervals of darkness, but the viewer does not notice the interruptions due to flicker fusion. The apparent motion on the screen is the result of the fact that the visual sense cannot discern the individual images at high speeds, so the impressions of the images blend with the dark intervals and are thus linked together to produce the illusion of one moving image. An analogous optical soundtrack (a graphic recording of the spoken words, music, and other sounds) runs along a portion of the film exclusively reserved for it, and was not projected.\n\nContemporary films are usually fully digital through the entire process of production, distribution, and exhibition.\n\nThe name \"film\" originally referred to the thin layer of photochemical emulsion[2] on the celluloid strip that used to be the actual medium for recording and displaying motion pictures.\n\nMany other terms exist for an individual motion-picture, including \"picture\", \"picture show\", \"moving picture\", \"photoplay\", and \"flick\". The most common term in the United States is \"movie\", while in Europe, \"film\" is preferred. Archaic terms include \"animated pictures\" and \"animated photography\".\n\n\"Flick\" is, in general a slang term, first recorded in 1926. It originates in the verb flicker, owing to the flickering appearance of early films.[3]\n\nCommon terms for the field, in general, include \"the big screen\", \"the movies\", \"the silver screen\", and \"cinema\"; the last of these is commonly used, as an overarching term, in scholarly texts and critical essays. In the early years, the word \"sheet\" was sometimes used instead of \"screen\".\n\nThe art of film has drawn on several earlier traditions in fields such as oral storytelling, literature, theatre and visual arts. Forms of art and entertainment that had already featured moving or projected images include:\n\nThe stroboscopic animation principle was introduced in 1833 with the stroboscopic disc (better known as the phénakisticope) and later applied in the zoetrope (since 1866), the flip book (since 1868), and the praxinoscope (since 1877), before it became the basic principle for cinematography.\n\nExperiments with early phénakisticope-based animation projectors were made at least as early as 1843 and publicly screened in 1847. Jules Duboscq marketed phénakisticope projection systems in France from c. 1853 until the 1890s.\n\nPhotography was introduced in 1839, but initially photographic emulsions needed such long exposures that the recording of moving subjects seemed impossible. At least as early as 1844, photographic series of subjects posed in different positions were created to either suggest a motion sequence or document a range of different viewing angles. The advent of stereoscopic photography, with early experiments in the 1840s and commercial success since the early 1850s, raised interest in completing the photographic medium with the addition of means to capture colour and motion. In 1849, Joseph Plateau published about the idea to combine his invention of the phénakisticope with the stereoscope, as suggested to him by stereoscope inventor Charles Wheatstone, and to use photographs of plaster sculptures in different positions to be animated in the combined device. In 1852, Jules Duboscq patented such an instrument as the \"Stéréoscope-fantascope, ou Bïoscope\", but he only marketed it very briefly, without success. One Bïoscope disc with stereoscopic photographs of a machine is in the Plateau collection of Ghent University, but no instruments or other discs have yet been found.\n\nBy the late 1850s the first examples of instantaneous photography came about and provided hope that motion photography would soon be possible, but it took a few decades before it was successfully combined with a method to record series of sequential images in real-time. In 1878, Eadweard Muybridge eventually managed to take a series of photographs of a running horse with a battery of cameras in a line along the track and published the results as The Horse in Motion on cabinet cards. Muybridge, as well as Étienne-Jules Marey, Ottomar Anschütz and many others, would create many more chronophotography studies. Muybridge had the contours of dozens of his chronophotographic series traced onto glass discs and projected them with his zoopraxiscope in his lectures from 1880 to 1895.\n\nAnschütz made his first instantaneous photographs in 1881. He developed a portable camera that allowed shutter speeds as short as 1/1000 of a second in 1882. The quality of his pictures was generally regarded as much higher than that of the chronophotography works of Muybridge and Étienne-Jules Marey.[4]\nIn 1886, Anschütz developed the Electrotachyscope, an early device that displayed short motion picture loops with 24 glass plate photographs on a 1.5 meter wide rotating wheel that was hand-cranked to a speed of circa 30 frames per second. Different versions were shown at many international exhibitions, fairs, conventions, and arcades from 1887 until at least 1894. Starting in 1891, some 152 examples of a coin-operated peep-box Electrotachyscope model were manufactured by Siemens & Halske in Berlin and sold internationally.[5][4] Nearly 34,000 people paid to see it at the Berlin Exhibition Park in the summer of 1892. Others saw it in London or at the 1893 Chicago World's Fair. On 25 November 1894, Anschütz introduced a Electrotachyscope projector with a 6x8 meter screening in Berlin. Between 22 February and 30 March 1895, a total of circa 7,000 paying customers came to view a 1.5-hour show of some 40 scenes at a 300-seat hall in the old Reichstag building in Berlin.[6]\n\nÉmile Reynaud already mentioned the possibility of projecting the images of the Praxinoscope in his 1877 patent application. He presented a praxinoscope projection device at the Société française de photographie on 4 June 1880, but did not market his praxinoscope a projection before 1882. He then further developed the device into the Théâtre Optique which could project longer sequences with separate backgrounds, patented in 1888. He created several movies for the machine by painting images on hundreds of gelatin plates that were mounted into cardboard frames and attached to a cloth band. From 28 October 1892 to March 1900 Reynaud gave over 12,800 shows to a total of over 500,000 visitors at the Musée Grévin in Paris.\n\nBy the end of the 1880s, the introduction of lengths of celluloid photographic film and the invention of motion picture cameras, which could photograph a rapid sequence of images using only one lens, allowed action to be captured and stored on a single compact reel of film.\n\nMovies were initially shown publicly to one person at a time through \"peep show\" devices such as the Electrotachyscope, Kinetoscope and the Mutoscope. Not much later, exhibitors managed to project films on large screens for theatre audiences.\n\nThe first public screenings of films at which admission was charged were made in 1895 by the American Woodville Latham and his sons, using films produced by their Eidoloscope company,[7] by the Skladanowsky brothers and by the – arguably better known – French brothers Auguste and Louis Lumière with ten of their own productions.[citation needed] Private screenings had preceded these by several months, with Latham's slightly predating the others'.[citation needed]\n\nThe earliest films were simply one static shot that showed an event or action with no editing or other cinematic techniques. Typical films showed employees leaving a factory gate, people walking in the street, and the view from the front of a trolley as it traveled a city's Main Street. According to legend, when a film showed a locomotive at high speed approaching the audience, the audience panicked and ran from the theater. Around the turn of the 20th century, films started stringing several scenes together to tell a story. (The filmmakers who first put several shots or scenes discovered that, when one shot follows another, that act establishes a relationship between the content in the separate shots in the minds of the viewer. It is this relationship that makes all film storytelling possible. In a simple example, if a person is shown looking out a window, whatever the next shot shows, it will be regarded as the view the person was seeing.) Each scene was a single stationary shot with the action occurring before it. The scenes were later broken up into multiple shots photographed from different distances and angles. Other techniques such as camera movement were developed as effective ways to tell a story with film. Until sound film became commercially practical in the late 1920s, motion pictures were a purely visual art, but these innovative silent films had gained a hold on the public imagination. Rather than leave audiences with only the noise of the projector as an accompaniment, theater owners hired a pianist or organist or, in large urban theaters, a full orchestra to play music that fit the mood of the film at any given moment. By the early 1920s, most films came with a prepared list of sheet music to be used for this purpose, and complete film scores were composed for major productions.\n\nThe rise of European cinema was interrupted by the outbreak of World War I, while the film industry in the United States flourished with the rise of Hollywood, typified most prominently by the innovative work of D. W. Griffith in The Birth of a Nation (1915) and Intolerance (1916). However, in the 1920s, European filmmakers such as Eisenstein, F. W. Murnau and Fritz Lang, in many ways inspired by the meteoric wartime progress of film through Griffith, along with the contributions of Charles Chaplin, Buster Keaton and others, quickly caught up with American film-making and continued to further advance the medium.\n\nIn the 1920s, the development of electronic sound recording technologies made it practical to incorporate a soundtrack of speech, music and sound effects synchronized with the action on the screen.[citation needed] The resulting sound films were initially distinguished from the usual silent \"moving pictures\" or \"movies\" by calling them \"talking pictures\" or \"talkies.\"[citation needed] The revolution they wrought was swift. By 1930, silent film was practically extinct in the US and already being referred to as \"the old medium.\"[citation needed]\n\nThe evolution of sound in cinema began with the idea of combining moving images with existing phonograph sound technology. Early sound-film systems, such as Thomas Edison's Kinetoscope and the Vitaphone used by Warner Bros., laid the groundwork for synchronized sound in film. The Vitaphone system, produced alongside Bell Telephone Company and Western Electric, faced initial resistance due to expensive equipping costs, but sound in cinema gained acceptance with movies like Don Juan (1926) and The Jazz Singer (1927).[8]\n\nAmerican film studios, while Europe standardized on Tobis-Klangfilm and Tri-Ergon systems. This new technology allowed for greater fluidity in film, giving rise to more complex and epic movies like King Kong (1933).[9]\n\nAs the television threat emerged in the 1940s and 1950s, the film industry needed to innovate to attract audiences. In terms of sound technology, this meant the development of surround sound and more sophisticated audio systems, such as Cinerama's seven-channel system. However, these advances required a large number of personnel to operate the equipment and maintain the sound experience in theaters.[9]\n\nIn 1966, Dolby Laboratories introduced the Dolby A noise reduction system, which became a standard in the recording industry and eliminated the hissing sound associated with earlier standardization efforts. Dolby Stereo, a revolutionary surround sound system, followed and allowed cinema designers to take acoustics into consideration when designing theaters. This innovation enabled audiences in smaller venues to enjoy comparable audio experiences to those in larger city theaters.[10]\n\nToday, the future of sound in film remains uncertain, with potential influences from artificial intelligence, remastered audio, and personal viewing experiences shaping its development.[11][12] However, it is clear that the evolution of sound in cinema has been marked by continuous innovation and a desire to create more immersive and engaging experiences for audiences.\n\nA significant technological advancement in film was the introduction of \"natural color,\" where color was captured directly from nature through photography, as opposed to being manually added to black-and-white prints using techniques like hand-coloring or stencil-coloring.[13][14] Early color processes often produced colors that appeared far from \"natural\".[15] Unlike the rapid transition from silent films to sound films, color's replacement of black-and-white happened more gradually.[16]\n\nThe crucial innovation was the three-strip version of the Technicolor process, first used in animated cartoons in 1932.[17][18] The process was later applied to live-action short films, specific sequences in feature films, and finally, for an entire feature film, Becky Sharp, in 1935.[19] Although the process was expensive, the positive public response, as evidenced by increased box office revenue, generally justified the additional cost.[13] Consequently, the number of films made in color gradually increased year after year.[20][21]\n\nIn the early 1950s, the proliferation of black-and-white television started seriously depressing North American theater attendance.[citation needed] In an attempt to lure audiences back into theaters, bigger screens were installed, widescreen processes, polarized 3D projection, and stereophonic sound were introduced, and more films were made in color, which soon became the rule rather than the exception. Some important mainstream Hollywood films were still being made in black-and-white as late as the mid-1960s, but they marked the end of an era. Color television receivers had been available in the US since the mid-1950s, but at first, they were very expensive and few broadcasts were in color. During the 1960s, prices gradually came down, color broadcasts became common, and sales boomed. The overwhelming public verdict in favor of color was clear. After the final flurry of black-and-white films had been released in mid-decade, all Hollywood studio productions were filmed in color, with the usual exceptions made only at the insistence of \"star\" filmmakers such as Peter Bogdanovich and Martin Scorsese.[citation needed]\n\nThe decades following the decline of the studio system in the 1960s saw changes in the production and style of film. Various New Wave movements (including the French New Wave, New German Cinema wave, Indian New Wave, Japanese New Wave, New Hollywood, and Egyptian New Wave) and the rise of film-school-educated independent filmmakers contributed to the changes the medium experienced in the latter half of the 20th century. Digital technology has been the driving force for change throughout the 1990s and into the 2000s. Digital 3D projection largely replaced earlier problem-prone 3D film systems and has become popular in the early 2010s.[citation needed]\n\n\"Film theory\" seeks to develop concise and systematic concepts that apply to the study of film as art. The concept of film as an art-form began in 1911 with Ricciotto Canudo's manifest The Birth of the Sixth Art. The Moscow Film School, the oldest film school in the world, was founded in 1919, in order to teach about and research film theory. Formalist film theory, led by Rudolf Arnheim, Béla Balázs, and Siegfried Kracauer, emphasized how film differed from reality and thus could be considered a valid fine art. André Bazin reacted against this theory by arguing that film's artistic essence lay in its ability to mechanically reproduce reality, not in its differences from reality, and this gave rise to realist theory. More recent analysis spurred by Jacques Lacan's psychoanalysis and Ferdinand de Saussure's semiotics among other things has given rise to psychoanalytic film theory, structuralist film theory, feminist film theory, and others. On the other hand, critics from the analytical philosophy tradition, influenced by Wittgenstein, try to clarify misconceptions used in theoretical studies and produce analysis of a film's vocabulary and its link to a form of life.\n\nFilm is considered to have its own language. James Monaco wrote a classic text on film theory, titled \"How to Read a Film,\" that addresses this. Director Ingmar Bergman famously said, \"Andrei Tarkovsky for me is the greatest director, the one who invented a new language, true to the nature of film, as it captures life as a reflection, life as a dream.\" An example of the language is a sequence of back and forth images of one speaking actor's left profile, followed by another speaking actor's right profile, then a repetition of this, which is a language understood by the audience to indicate a conversation. This describes another theory of film, the 180-degree rule, as a visual story-telling device with an ability to place a viewer in a context of being psychologically present through the use of visual composition and editing. The \"Hollywood style\" includes this narrative theory, due to the overwhelming practice of the rule by movie studios based in Hollywood, California, during film's classical era. Another example of cinematic language is having a shot that zooms in on the forehead of an actor with an expression of silent reflection that cuts to a shot of a younger actor who vaguely resembles the first actor, indicating that the first person is remembering a past self, an edit of compositions that causes a time transition.\n\nMontage is a film editing technique in which separate pieces of film are selected, edited, and assembled to create a new section or sequence within a film. This technique can be used to convey a narrative or to create an emotional or intellectual effect by juxtaposing different shots, often for the purpose of condensing time, space, or information. Montage can involve flashbacks, parallel action, or the interplay of various visual elements to enhance the storytelling or create symbolic meaning.[22]\n\nThe concept of montage emerged in the 1920s, with pioneering Soviet filmmakers such as Sergei Eisenstein and Lev Kuleshov developing the theory of montage. Eisenstein's film Battleship Potemkin (1925) is a prime example of the innovative use of montage, where he employed complex juxtapositions of images to create a visceral impact on the audience.[23]\n\nAs the art of montage evolved, filmmakers began incorporating musical and visual counterpoint to create a more dynamic and engaging experience for the viewer. The development of scene construction through mise-en-scène, editing, and special effects led to more sophisticated techniques that can be compared to those utilized in opera and ballet.[24]\n\nThe French New Wave movement of the late 1950s and 1960s also embraced the montage technique, with filmmakers such as Jean-Luc Godard and François Truffaut using montage to create distinctive and innovative films. This approach continues to be influential in contemporary cinema, with directors employing montage to create memorable sequences in their films.[25]\n\nIn contemporary cinema, montage continues to play an essential role in shaping narratives and creating emotional resonance. Filmmakers have adapted the traditional montage technique to suit the evolving aesthetics and storytelling styles of modern cinema.\n\nAs the medium of film continues to evolve, montage remains an integral aspect of visual storytelling, with filmmakers finding new and innovative ways to employ this powerful technique.\n\nIf a movie can illuminate the lives of other people who share this planet with us and show us not only how different they are but, how even so, they share the same dreams and hurts, then it deserves to be called great.\n\nFilm criticism is the analysis and evaluation of films. In general, these works can be divided into two categories: academic criticism by film scholars and journalistic film criticism that appears regularly in newspapers and other media. Film critics working for newspapers, magazines, and broadcast media mainly review new releases. Normally they only see any given film once and have only a day or two to formulate their opinions. Despite this, critics have an important impact on the audience response and attendance at films, especially those of certain genres. Mass marketed action, horror, and comedy films tend not to be greatly affected by a critic's overall judgment of a film. The plot summary and description of a film and the assessment of the director's and screenwriters' work that makes up the majority of most film reviews can still have an important impact on whether people decide to see a film. For prestige films such as most dramas and art films, the influence of reviews is important. Poor reviews from leading critics at major papers and magazines will often reduce audience interest and attendance.\n\nThe impact of a reviewer on a given film's box office performance is a matter of debate. Some observers claim that movie marketing in the 2000s is so intense, well-coordinated and well financed that reviewers cannot prevent a poorly written or filmed blockbuster from attaining market success. However, the cataclysmic failure of some heavily promoted films which were harshly reviewed, as well as the unexpected success of critically praised independent films indicates that extreme critical reactions can have considerable influence. Other observers note that positive film reviews have been shown to spark interest in little-known films. Conversely, there have been several films in which film companies have so little confidence that they refuse to give reviewers an advanced viewing to avoid widespread panning of the film. However, this usually backfires, as reviewers are wise to the tactic and warn the public that the film may not be worth seeing and the films often do poorly as a result. Journalist film critics are sometimes called film reviewers. Critics who take a more academic approach to films, through publishing in film journals and writing books about films using film theory or film studies approaches, study how film and filming techniques work, and what effect they have on people. Rather than having their reviews published in newspapers or appearing on television, their articles are published in scholarly journals or up-market magazines. They also tend to be affiliated with colleges or universities as professors or instructors.\n\nThe making and showing of motion pictures became a source of profit almost as soon as the process was invented. Upon seeing how successful their new invention, and its product, was in their native France, the Lumières quickly set about touring the Continent to exhibit the first films privately to royalty and publicly to the masses. In each country, they would normally add new, local scenes to their catalogue and, quickly enough, found local entrepreneurs in the various countries of Europe to buy their equipment and photograph, export, import, and screen additional product commercially. The Oberammergau Passion Play of 1898[31] was the first commercial motion picture ever produced. Other pictures soon followed, and motion pictures became a separate industry that overshadowed the vaudeville world. Dedicated theaters and companies formed specifically to produce and distribute films, while motion picture actors became major celebrities and commanded huge fees for their performances. By 1917 Charlie Chaplin had a contract that called for an annual salary of one million dollars. From 1931 to 1956, film was also the only image storage and playback system for television programming until the introduction of videotape recorders.\n\nIn the United States, much of the film industry is centered around Hollywood, California. Other regional centers exist in many parts of the world, such as Mumbai-centered Bollywood, the Indian film industry's Hindi cinema which produces the largest number of films in the world.[32] Though the expense involved in making films has led cinema production to concentrate under the auspices of movie studios, recent advances in affordable film making equipment have allowed independent film productions to flourish.\n\nProfit is a key force in the industry, due to the costly and risky nature of filmmaking; many films have large cost overruns, an example being Kevin Costner's Waterworld. Yet many filmmakers strive to create works of lasting social significance. The Academy Awards (also known as \"the Oscars\") are the most prominent film awards in the United States, providing recognition each year to films, based on their artistic merits. There is also a large industry for educational and instructional films made in lieu of or in addition to lectures and texts. Revenue in the industry is sometimes volatile due to the reliance on blockbuster films released in movie theaters. The rise of alternative home entertainment has raised questions about the future of the cinema industry, and Hollywood employment has become less reliable, particularly for medium and low-budget films.[33]\n\nDerivative academic fields of study may both interact with and develop independently of filmmaking, as in film theory and analysis. Fields of academic study have been created that are derivative or dependent on the existence of film, such as film criticism, film history, divisions of film propaganda in authoritarian governments, or psychological on subliminal effects (e.g., of a flashing soda can during a screening). These fields may further create derivative fields, such as a movie review section in a newspaper or a television guide. Sub-industries can spin off from film, such as popcorn makers, and film-related toys (e.g., Star Wars figures). Sub-industries of pre-existing industries may deal specifically with film, such as product placement and other advertising within films.\n\nThe terminology used for describing motion pictures varies considerably between British and American English. In British usage, the name of the medium is film. The word movie is understood but seldom used.[34][35] Additionally, the pictures (plural) is used somewhat frequently to refer to the place where movies are exhibited; in American English this may be called the movies, but that term is becoming outdated. In other countries, the place where movies are exhibited may be called a cinema or movie theatre.\n\nBy contrast, in the United States, movie is the predominant term for the medium. Although the words film and movie are sometimes used interchangeably, film is more often used when considering artistic, theoretical, or technical aspects. The term movies more often refers to entertainment or commercial aspects, as where to go for fun evening on a date. For example, a book titled How to Understand a Film would probably be about the aesthetics or theory of film, while a book entitled Let's Go to the Movies would probably be about the history of entertaining movies and blockbusters.\n\nFurther terminology is used to distinguish various forms and media used in the film industry. Motion pictures and moving pictures are frequently used terms for film and movie productions specifically intended for theatrical exhibition, such as Star Wars. DVD, Blu-ray Disc, and videotape are video formats that can reproduce a photochemical film. A reproduction based on such is called a transfer. After the advent of theatrical film as an industry, the television industry began using videotape as a recording medium. For many decades, tape was solely an analog medium onto which moving images could be either recorded or transferred. Film and filming refer to the photochemical medium that chemically records a visual image and the act of recording respectively. However, the act of shooting images with other visual media, such as with a digital camera, is still called filming, and the resulting works often called films as interchangeable to movies, despite not being shot on film. Silent films need not be utterly silent, but are films and movies without an audible dialogue, including those that have a musical accompaniment. The word talkies refers to the earliest sound films created to have audible dialogue recorded for playback along with the film, regardless of a musical accompaniment. Cinema either broadly encompasses both films and movies, or it is roughly synonymous with film and theatrical exhibition, and both are capitalized when referring to a category of art. The silver screen refers to the projection screen used to exhibit films and, by extension, is also used as a metonym for the entire film industry.\n\nWidescreen refers to a larger width to height in the frame, compared to earlier historic aspect ratios.[36] A feature-length film, or feature film, is of a conventional full length, usually 60 minutes or more, and can commercially stand by itself without other films in a ticketed screening.[37] A short is a film that is not as long as a feature-length film, often screened with other shorts, or preceding a feature-length film. An independent is a film made outside the conventional film industry.\n\nIn US usage, one talks of a screening or projection of a movie or video on a screen at a public or private theater. In British English, a film showing happens at a cinema (never a theatre, which is a different medium and place altogether).[35] Cinema usually refers to an arena designed specifically to exhibit films, where the screen is affixed to a wall, while theatre usually refers to a place where live, non-recorded action or combination thereof occurs from a podium or other type of stage, including the amphitheatre. Theatres can still screen movies in them, though the theatre would be retrofitted to do so. One might propose going to the cinema when referring to the activity, or sometimes to the pictures in British English, whereas the US expression is usually going to the movies. A cinema usually shows a mass-marketed movie using a front-projection screen process with either a film projector or, more recently, with a digital projector. But, cinemas may also show theatrical movies from their home video transfers that include Blu-ray Disc, DVD, and videocassette when they possess sufficient projection quality or based upon need, such as movies that exist only in their transferred state, which may be due to the loss or deterioration of the film master and prints from which the movie originally existed. Due to the advent of digital film production and distribution, physical film might be absent entirely.\n\nA double feature is a screening of two independently marketed, stand-alone feature films. A viewing is a watching of a film. Sales and at the box office refer to tickets sold at a theater, or more currently, rights sold for individual showings. A release is the distribution and often simultaneous screening of a film. A preview is a screening in advance of the main release.\n\nAny film may also have a sequel, which portrays events following those in the film. Bride of Frankenstein is an early example. When there are more films than one with the same characters, story arcs, or subject themes, these movies become a series, such as the James Bond series. Existing outside a specific story timeline usually does not exclude a film from being part of a series. A film that portrays events occurring earlier in a timeline with those in another film, but is released after that film, is sometimes called a prequel, an example being Butch and Sundance: The Early Days.\n\nThe credits, or end credits, are a list that gives credit to the people involved in the production of a film. Films from before the 1970s usually start a film with credits, often ending with only a title card, saying \"The End\" or some equivalent, often an equivalent that depends on the language of the production.[citation needed] From then onward, a film's credits usually appear at the end of most films. However, films with credits that end a film often repeat some credits at or near the start of a film and therefore appear twice, such as that film's acting leads, while less frequently some appearing near or at the beginning only appear there, not at the end, which often happens to the director's credit. The credits appearing at or near the beginning of a film are usually called titles or beginning titles. A post-credits scene is a scene shown after the end of the credits. Ferris Bueller's Day Off has a post-credits scene in which Ferris tells the audience that the film is over and they should go home.\n\nA film's cast refers to a collection of the actors and actresses who appear, or star, in a film. A star is an actor or actress, often a popular one, and in many cases, a celebrity who plays a central character in a film. Occasionally the word can also be used to refer to the fame of other members of the crew, such as a director or other personality, such as Martin Scorsese. A crew is usually interpreted as the people involved in a film's physical construction outside cast participation, and it could include directors, film editors, photographers, grips, gaffers, set decorators, prop masters, and costume designers. A person can both be part of a film's cast and crew, such as Woody Allen, who directed and starred in Take the Money and Run.\n\nA film goer, movie goer, or film buff is a person who likes or often attends films and movies, and any of these, though more often the latter, could also see oneself as a student to films and movies or the filmic process. Intense interest in films, film theory, and film criticism, is known as cinephilia. A film enthusiast is known as a cinephile or cineaste.\n\nPreview performance refers to a showing of a film to a select audience, usually for the purposes of corporate promotions, before the public film premiere itself. Previews are sometimes used to judge audience reaction, which if unexpectedly negative, may result in recutting or even refilming certain sections based on the audience response. One example of a film that was changed after a negative response from the test screening is 1982's First Blood. After the test audience responded very negatively to the death of protagonist John Rambo, a Vietnam veteran, at the end of the film, the company wrote and re-shot a new ending in which the character survives.[38]\n\nTrailers or previews are advertisements for films that will be shown in 1 to 3 months at a cinema. Back in the early days of cinema, with theaters that had only one or two screens, only certain trailers were shown for the films that were going to be shown there. Later, when theaters added more screens or new theaters were built with a lot of screens, all different trailers were shown even if they were not going to play that film in that theater. Film studios realized that the more trailers that were shown (even if it was not going to be shown in that particular theater) the more patrons would go to a different theater to see the film when it came out. The term trailer comes from their having originally been shown at the end of a film program. That practice did not last long because patrons tended to leave the theater after the films ended, but the name has stuck. Trailers are now shown before the film (or the \"A film\" in a double feature program) begins. Film trailers are also common on DVDs and Blu-ray Discs, as well as on the Internet and mobile devices. Trailers are created to be engaging and interesting for viewers. As a result, in the Internet era, viewers often seek out trailers to watch them. Of the ten billion videos watched online annually in 2008, film trailers ranked third, after news and user-created videos.[39] A teaser is a much shorter preview or advertisement that lasts only 10 to 30 seconds. Teasers are used to get patrons excited about a film coming out in the next six to twelve months. Teasers may be produced even before the film production is completed.\n\nFilms are cultural artifacts created by specific cultures, facilitating intercultural dialogue. It is considered to be an important art form that provides entertainment and historical value, often visually documenting a period of time. The visual basis of the medium gives it a universal power of communication, often stretched further through the use of dubbing or subtitles to translate the dialog into other languages.[42] Just seeing a location in a film is linked to higher tourism to that location, demonstrating how powerful the suggestive nature of the medium can be.[43]\n\nFilm is used for a range of goals, including education and propaganda due its ability to effectively intercultural dialogue. When the purpose is primarily educational, a film is called an \"educational film\". Examples are recordings of academic lectures and experiments, or a film based on a classic novel. Film may be propaganda, in whole or in part, such as the films made by Leni Riefenstahl in Nazi Germany, US war film trailers during World War II, or artistic films made under Stalin by Sergei Eisenstein. They may also be works of political protest, as in the films of Andrzej Wajda, or more subtly, the films of Andrei Tarkovsky. The same film may be considered educational by some, and propaganda by others as the categorization of a film can be subjective.\n\nAt its core, the means to produce a film depend on the content the filmmaker wishes to show, and the apparatus for displaying it: the zoetrope merely requires a series of images on a strip of paper. Film production can, therefore, take as little as one person with a camera (or even without a camera, as in Stan Brakhage's 1963 film Mothlight), or thousands of actors, extras, and crew members for a live-action, feature-length epic.\nThe necessary steps for almost any film can be boiled down to conception, planning, execution, revision, and distribution. The more involved the production, the more significant each of the steps becomes. In a typical production cycle of a Hollywood-style film, these main stages are defined as development, pre-production, production, post-production and distribution.\n\nThis production cycle usually takes three years. The first year is taken up with development. The second year comprises preproduction and production. The third year, post-production and distribution. The bigger the production, the more resources it takes, and the more important financing becomes; most feature films are artistic works from the creators' perspective (e.g., film director, cinematographer, screenwriter) and for-profit business entities for the production companies.\n\nA film crew is a group of people hired by a film company, employed during the \"production\" or \"photography\" phase, for the purpose of producing a film or motion picture. Crew is distinguished from cast, who are the actors who appear in front of the camera or provide voices for characters in the film. The crew interacts with but is also distinct from the production staff, consisting of producers, managers, company representatives, their assistants, and those whose primary responsibility falls in pre-production or post-production phases, such as screenwriters and film editors. Communication between production and crew generally passes through the director and his/her staff of assistants. Medium-to-large crews are generally divided into departments with well-defined hierarchies and standards for interaction and cooperation between the departments. Other than acting, the crew handles everything in the photography phase: props and costumes, shooting, sound, electrics (i.e., lights), sets, and production special effects. Caterers (known in the film industry as \"craft services\") are usually not considered part of the crew.\n\nFilm stock consists of transparent celluloid, acetate, or polyester base coated with an emulsion containing light-sensitive chemicals. Cellulose nitrate was the first type of film base used to record motion pictures, but due to its flammability was eventually replaced by safer materials. Stock widths and the film format for images on the reel have had a rich history, though most large commercial films are still shot on (and distributed to theaters) as 35 mm prints.\nOriginally moving picture film was shot and projected at various speeds using hand-cranked cameras and projectors; though 1000 frames per minute (16⁠2/3⁠ frame/s) is generally cited as a standard silent speed, research indicates most films were shot between 16 frame/s and 23 frame/s and projected from 18 frame/s on up (often reels included instructions on how fast each scene should be shown).[44] When synchronized sound film was introduced in the late 1920s, a constant speed was required for the sound head. 24 frames per second were chosen because it was the slowest (and thus cheapest) speed which allowed for sufficient sound quality.[45] The standard was set with Warner Bros.'s The Jazz Singer and their Vitaphone system in 1927.[46][47] Improvements since the late 19th century include the mechanization of cameras – allowing them to record at a consistent speed, quiet camera design – allowing sound recorded on-set to be usable without requiring large \"blimps\" to encase the camera, the invention of more sophisticated filmstocks and lenses, allowing directors to film in increasingly dim conditions, and the development of synchronized sound, allowing sound to be recorded at exactly the same speed as its corresponding action. The soundtrack can be recorded separately from shooting the film, but for live-action pictures, many parts of the soundtrack are usually recorded simultaneously.\n\nAs a medium, film is not limited to motion pictures, since the technology developed as the basis for photography. It can be used to present a progressive sequence of still images in the form of a slideshow. Film has also been incorporated into multimedia presentations and often has importance as primary historical documentation. However, historic films have problems in terms of preservation and storage, and the motion picture industry is exploring many alternatives. Most films on cellulose nitrate base have been copied onto modern safety films. Some studios save color films through the use of separation masters: three B&W negatives each exposed through red, green, or blue filters (essentially a reverse of the Technicolor process). Digital methods have also been used to restore films, although their continued obsolescence cycle makes them (as of 2006) a poor choice for long-term preservation. Film preservation of decaying film stock is a matter of concern to both film historians and archivists and to companies interested in preserving their existing products in order to make them available to future generations (and thereby increase revenue). Preservation is generally a higher concern for nitrate and single-strip color films, due to their high decay rates; black-and-white films on safety bases and color films preserved on Technicolor imbibition prints tend to keep up much better, assuming proper handling and storage.\n\nSome films in recent decades have been recorded using analog video technology similar to that used in television production. Modern digital video cameras and digital projectors are gaining ground as well. These approaches are preferred by some film-makers, especially because footage shot with digital cinema can be evaluated and edited with non-linear editing systems (NLE) without waiting for the film stock to be processed. The migration was gradual, and as of 2005, most major motion pictures were still shot on film.[needs update]\n\nIndependent filmmaking often takes place outside Hollywood, or other major studio systems. An independent film (or indie film) is a film initially produced without financing or distribution from a major film studio. Creative, business and technological reasons have all contributed to the growth of the indie film scene in the late 20th and early 21st century. On the business side, the costs of big-budget studio films also lead to conservative choices in cast and crew. There is a trend in Hollywood towards co-financing (over two-thirds of the films put out by Warner Bros. in 2000 were joint ventures, up from 10% in 1987).[48] A hopeful director is almost never given the opportunity to get a job on a big-budget studio film unless he or she has significant industry experience in film or television. Also, the studios rarely produce films with unknown actors, particularly in lead roles.\n\nBefore the advent of digital alternatives, the cost of professional film equipment and stock was also a hurdle to being able to produce, direct, or star in a traditional studio film. But the advent of consumer camcorders in 1985, and more importantly, the arrival of high-resolution digital video in the early 1990s, have lowered the technology barrier to film production significantly. Both production and post-production costs have been significantly lowered; in the 2000s, the hardware and software for post-production can be installed in a commodity-based personal computer. Technologies such as DVDs, FireWire connections and a wide variety of professional and consumer-grade video editing software make film-making relatively affordable.\n\nSince the introduction of digital video DV technology, the means of production have become more democratized. Filmmakers can conceivably shoot a film with a digital video camera and edit the film, create and edit the sound and music, and mix the final cut on a high-end home computer. However, while the means of production may be democratized, financing, distribution, and marketing remain difficult to accomplish outside the traditional system. Most independent filmmakers rely on film festivals to get their films noticed and sold for distribution. The arrival of internet-based video websites such as YouTube and Veoh has further changed the filmmaking landscape, enabling indie filmmakers to make their films available to the public.\n\nAn open content film is much like an independent film, but it is produced through open collaborations; its source material is available under a license which is permissive enough to allow other parties to create fan fiction or derivative works rather than a traditional copyright. Like independent filmmaking, open source filmmaking takes place outside Hollywood and other major studio systems. For example, the film Balloon was based on the real event during the Cold War.[49]\n\nA fan film is a film or video inspired by a film, television program, comic book or a similar source, created by fans rather than by the source's copyright holders or creators. Fan filmmakers have traditionally been amateurs, but some of the most notable films have actually been produced by professional filmmakers as film school class projects or as demonstration reels. Fan films vary tremendously in length, from short faux-teaser trailers for non-existent motion pictures to rarer full-length motion pictures.\n\nFilm distribution is the process through which a film is made available for viewing by an audience. This is normally the task of a professional film distributor, who would determine the marketing strategy of the film, the media by which a film is to be exhibited or made available for viewing, and may set the release date and other matters. The film may be exhibited directly to the public either through a movie theater (historically the main way films were distributed) or television for personal home viewing (including on DVD-Video or Blu-ray Disc, video-on-demand, online downloading, television programs through broadcast syndication etc.). Other ways of distributing a film include rental or personal purchase of the film in a variety of media and formats, such as VHS tape or DVD, or Internet downloading or streaming using a computer.\n\nAnimation is a technique in which each frame of a film is produced individually, whether generated as a computer graphic, or by photographing a drawn image, or by repeatedly making small changes to a model unit (see claymation and stop motion), and then photographing the result with a special animation camera. When the frames are strung together and the resulting film is viewed at a speed of 16 or more frames per second, there is an illusion of continuous movement (due to the phi phenomenon). Generating such a film is very labor-intensive and tedious, though the development of computer animation has greatly sped up the process. Because animation is very time-consuming and often very expensive to produce, the majority of animation for TV and films comes from professional animation studios. However, the field of independent animation has existed at least since the 1950s, with animation being produced by independent studios (and sometimes by a single person). Several independent animation producers have gone on to enter the professional animation industry.\n\nLimited animation is a way of increasing production and decreasing costs of animation by using \"short cuts\" in the animation process. This method was pioneered by UPA and popularized by Hanna-Barbera in the United States, and by Osamu Tezuka in Japan, and adapted by other studios as cartoons moved from movie theaters to television.[50] Although most animation studios are now using digital technologies in their productions, there is a specific style of animation that depends on film. Camera-less animation, made famous by film-makers like Norman McLaren, Len Lye, and Stan Brakhage, is painted and drawn directly onto pieces of film, and then run through a projector.\n"
    },
    {
        "title": "Homer",
        "url": "https://en.wikipedia.org/wiki/Homer",
        "content": "This is an accepted version of this page\n\n\nHomer (/ˈhoʊmər/; Ancient Greek: Ὅμηρος [hómɛːros], Hómēros; born c. 8th century BCE) was an Ancient Greek poet who is credited as the author of the Iliad and the Odyssey, two epic poems that are foundational works of ancient Greek literature. Homer is considered one of the most revered and influential authors in history.[2]\n\nHomer's Iliad centers on a quarrel between King Agamemnon and the warrior Achilles during the last year of the Trojan War. The Odyssey chronicles the ten-year journey of Odysseus, king of Ithaca, back to his home after the fall of Troy. The epics depict man's struggle, the Odyssey especially so as Odysseus perseveres through punishment of the gods.[3] The poems are in Homeric Greek, also known as Epic Greek, a literary language which shows a mixture of features of the Ionic and Aeolic dialects from different centuries; the predominant influence is Eastern Ionic.[4][5] Most researchers believe that the poems were originally transmitted orally.[6] Despite being predominantly known for its tragic and serious themes, the Homeric poems also contain instances of comedy and laughter.[7]\n\nHomer's epic poems shaped aspects of ancient Greek culture and education, fostering ideals of heroism, glory, and honor.[8] To Plato, Homer was simply the one who \"has taught Greece\" (τὴν Ἑλλάδα πεπαίδευκεν, tēn Helláda pepaídeuken).[9][10]\nIn Dante Alighieri's Divine Comedy, Virgil refers to Homer as \"Poet sovereign\", king of all poets;[11] in the preface to his translation of the Iliad, Alexander Pope acknowledges that Homer has always been considered the \"greatest of poets\".[12] From antiquity to the present day, Homeric epics have inspired many famous works of literature, music, art, and film.[13]\n\nThe question of by whom, when, where and under what circumstances the Iliad and Odyssey were composed continues to be debated. Scholars generally regard the two poems as the works of separate authors. It is thought that the poems were composed at some point around the late eighth or early seventh century BCE.[14] Many accounts of Homer's life circulated in classical antiquity, the most widespread that he was a blind bard from Ionia, a region of central coastal Anatolia in present-day Turkey.[15] Modern scholars consider these accounts legendary.[16]\n\nToday, only the Iliad and the Odyssey are associated with the name \"Homer\". In antiquity, a large number of other works were sometimes attributed to him, including the Homeric Hymns, the Contest of Homer and Hesiod, several epigrams, the Little Iliad, the Nostoi, the Thebaid, the Cypria, the Epigoni, the comic mini-epic Batrachomyomachia (\"The Frog–Mouse War\"), the Margites, the Capture of Oechalia, and the Phocais. These claims are not considered authentic today and were not universally accepted in the ancient world. As with the multitude of legends surrounding Homer's life, they indicate little more than the centrality of Homer to ancient Greek culture.[17][18][19]\n\nSome ancient accounts about Homer were established early and repeated often. They include that Homer was blind (taking as self-referential a passage describing the blind bard Demodocus),[20][21] that he resided at Chios, that he was the son of the river Meles and the nymph Critheïs, that he was a wandering bard, that he composed a varying list of other works (the \"Homerica\"), that he died either in Ios or after failing to solve a riddle set by fishermen,[22] and various explanations for the name \"Homer\" (Ὅμηρος, Hómēros).[20] Another tradition from the days of the Roman emperor Hadrian says Epicaste (daughter of Nestor) and Telemachus (son of Odysseus) were the parents of Homer.[23][24]\n\nThe two best known ancient biographies of Homer are the Life of Homer by the Pseudo-Herodotus and the Contest of Homer and Hesiod.[1][25]\n\nIn the early fourth century BC Alcidamas composed a fictional account of a poetry contest at Chalcis with both Homer and Hesiod. Homer was expected to win, and answered all of Hesiod's questions and puzzles with ease. Then, each of the poets was invited to recite the best passage from their work. Hesiod selected the beginning of Works and Days: \"When the Pleiades born of Atlas ... all in due season\". Homer chose a description of Greek warriors in formation, facing the foe, taken from the Iliad. Though the crowd acclaimed Homer victor, the judge awarded Hesiod the prize; the poet who praised husbandry, he said, was greater than the one who told tales of battles and slaughter.[26]\n\nThe study of Homer is one of the oldest topics in scholarship, dating back to antiquity.[27][28][29] Nonetheless, the aims of Homeric studies have changed over the course of the millennia.[27] The earliest preserved comments on Homer concern his treatment of the gods, which hostile critics such as the poet Xenophanes of Colophon denounced as immoral.[29] The allegorist Theagenes of Rhegium is said to have defended Homer by arguing that the Homeric poems are allegories.[29] The Iliad and the Odyssey were widely used as school texts in ancient Greek and Hellenistic cultures.[27][29][30] They were the first literary works taught to all students.[30] The Iliad, particularly its first few books, was far more intently studied than the Odyssey during the Hellenistic and Roman periods.[30]\n\nAs a result of the poems' prominence in classical Greek education, extensive commentaries on them developed to explain parts that were culturally or linguistically difficult.[27][29] During the Hellenistic and Roman periods, many interpreters, especially the Stoics, who believed that Homeric poems conveyed Stoic doctrines, regarded them as allegories, containing hidden wisdom.[29] Perhaps partially because of the Homeric poems' extensive use in education, many authors believed that Homer's original purpose had been to educate.[29] Homer's wisdom became so widely praised that he began to acquire the image of almost a prototypical philosopher.[29] Byzantine scholars such as Eustathius of Thessalonica and John Tzetzes produced commentaries, extensions and scholia to Homer, especially in the twelfth century.[31][29] Eustathius's commentary on the Iliad alone is massive, sprawling over nearly 4,000 oversized pages in a 21st-century printed version and his commentary on the Odyssey an additional nearly 2,000.[29]\n\nIn 1488, the Greek scholar Demetrios Chalkokondyles published in Florence the editio princeps of the Homeric poems.[29][32]  The earliest modern Homeric scholars started with the same basic approaches towards the Homeric poems as scholars in antiquity.[29][28][27] The allegorical interpretation of the Homeric poems that had been so prevalent in antiquity returned to become the prevailing view of the Renaissance.[29] Renaissance humanists praised Homer as the archetypically wise poet, whose writings contain hidden wisdom, disguised through allegory.[29] In western Europe during the Renaissance, Virgil was more widely read than Homer and Homer was often seen through a Virgilian lens.[33]\n\nIn 1664, contradicting the widespread praise of Homer as the epitome of wisdom, François Hédelin, abbé d'Aubignac wrote a scathing attack on the Homeric poems, declaring that they were incoherent, immoral, tasteless, and without style, that Homer never existed, and that the poems were hastily cobbled together by incompetent editors from unrelated oral songs.[28] Fifty years later, the English scholar Richard Bentley concluded that Homer did exist but that he was an obscure, prehistoric oral poet whose compositions bear little relation to the Iliad and the Odyssey as they have been passed down.[28] According to Bentley, Homer \"wrote a Sequel of Songs and Rhapsodies, to be sung by himself for small Earnings and good Cheer at Festivals and other Days of Merriment; the Ilias he wrote for men, and the Odysseis for the other Sex. These loose songs were not collected together in the Form of an epic Poem till Pisistratus' time, about 500 Years after.\"[28]\n\nFriedrich August Wolf's Prolegomena ad Homerum, published in 1795, argued that much of the material later incorporated into the Iliad and the Odyssey was originally composed in the tenth century BC in the form of short, separate oral songs,[34][35][28] which passed through oral tradition for roughly four hundred years before being assembled into prototypical versions of the Iliad and the Odyssey in the sixth century BC by literate authors.[34][35][28] After being written down, Wolf maintained that the two poems were extensively edited, modernized, and eventually shaped into their present state as artistic unities.[34][35][28] Wolf and the \"Analyst\" school, which led the field in the nineteenth century, sought to recover the original, authentic poems which were thought to be concealed by later excrescences.[34][35][28][36]\n\nWithin the Analyst school were two camps: proponents of the \"lay theory\", which held that the Iliad and the Odyssey were put together from a large number of short, independent songs,[28] and proponents of the \"nucleus theory\", which held that Homer had originally composed shorter versions of the Iliad and the Odyssey, which later poets expanded and revised.[28] A small group of scholars opposed to the Analysts, dubbed \"Unitarians\", saw the later additions as superior, the work of a single inspired poet.[34][35][28] By around 1830, the central preoccupations of Homeric scholars, dealing with whether or not \"Homer\" actually existed, when and how the Homeric poems originated, how they were transmitted, when and how they were finally written down, and their overall unity, had been dubbed \"the Homeric Question\".[28]\n\nFollowing World War I, the Analyst school began to fall out of favor among Homeric scholars.[28] It did not die out entirely, but it came to be increasingly seen as a discredited dead end.[28] Starting in around 1928, Milman Parry and Albert Lord, after their studies of folk bards in the Balkans, developed the \"Oral-Formulaic Theory\" that the Homeric poems were originally composed through improvised oral performances, which relied on traditional epithets and poetic formulas.[37][36][28] This theory found very wide scholarly acceptance[37][36][28] and explained many previously puzzling features of the Homeric poems, including their unusually archaic language, their extensive use of stock epithets, and their other \"repetitive\" features.[36] Many scholars concluded that the \"Homeric Question\" had finally been answered.[28]\n\nMeanwhile, the 'Neoanalysts' sought to bridge the gap between the 'Analysts' and 'Unitarians'.[38][39] The Neoanalysts sought to trace the relationships between the Homeric poems and other epic poems, which have now been lost, but of which modern scholars do possess some patchy knowledge.[28] Neoanalysts hold that knowledge of earlier versions of the epics can be derived from anomalies of structure and detail in the surviving versions of the Iliad and Odyssey. These anomalies point to earlier versions of the Iliad in which Ajax played a more prominent role, in which the Achaean embassy to Achilles comprised different characters, and in which Patroclus was actually mistaken for Achilles by the Trojans. They point to earlier versions of the Odyssey in which Telemachus went in search of news of his father not to Menelaus in Sparta but to Idomeneus in Crete, in which Telemachus met up with his father in Crete and conspired with him to return to Ithaca disguised as the soothsayer Theoclymenus, and in which Penelope recognized Odysseus much earlier in the narrative and conspired with him in the destruction of the suitors.[40]\n\nMost contemporary scholars, although they disagree on other questions about the genesis of the poems, agree that the Iliad and the Odyssey were not produced by the same author, based on \"the many differences of narrative manner, theology, ethics, vocabulary, and geographical perspective, and by the apparently imitative character of certain passages of the Odyssey in relation to the Iliad.\"[41][42][43][28] Nearly all scholars agree that the Iliad and the Odyssey are unified poems, in that each poem shows a clear overall design and that they are not merely strung together from unrelated songs.[28] It is also generally agreed that each poem was composed mostly by a single author, who probably relied heavily on older oral traditions.[28] Nearly all scholars agree that the Doloneia in Book X of the Iliad is not part of the original poem, but rather a later insertion by a different poet.[28]\n\nSome ancient scholars believed Homer to have been an eyewitness to the Trojan War; others thought he had lived up to 500 years afterwards.[44] Contemporary scholars continue to debate the date of the poems.[45][46][28] A long history of oral transmission lies behind the composition of the poems, complicating the search for a precise date.[47] At one extreme, Richard Janko has proposed a date for both poems to the eighth century BC based on linguistic analysis and statistics.[45][46] Barry B. Powell dates the composition of the Iliad and the Odyssey to sometime between 800 and 750 BC, based on the statement from Herodotus, who lived in the late fifth century BC, that Homer lived four hundred years before his own time \"and not more\" (καὶ οὐ πλέοσι) and on the fact that the poems do not mention hoplite battle tactics, inhumation, or literacy.[48]\n\nMartin Litchfield West has argued that the Iliad echoes the poetry of Hesiod and that it must have been composed around 660–650 BC at the earliest, with the Odyssey up to a generation later.[49][50][28] He also interprets passages in the Iliad as showing knowledge of historical events that occurred in the ancient Near East during the middle of the seventh century BC, including the destruction of Babylon by Sennacherib in 689 BC and the Sack of Thebes by Ashurbanipal in 663/4 BC.[28] At the other extreme, a few American scholars such as Gregory Nagy see \"Homer\" as a continually evolving tradition, which grew much more stable as the tradition progressed, but which did not fully cease to continue changing and evolving until as late as the middle of the second century BC.[45][46][28]\n\n\"'Homer\" is a name of unknown etymological origin, around which many theories were erected in antiquity. One such linkage was to the Greek ὅμηρος (hómēros 'hostage' or 'surety'). The explanations suggested by modern scholars tend to mirror their position on the overall Homeric Question. Nagy interprets it as \"he who fits (the song) together\". West has advanced both possible Greek and Phoenician etymologies.[51][52]\n\nScholars continue to debate questions such as whether the Trojan War actually took place – and if so when and where – and to what extent the society depicted by Homer is based on his own or one which was, even at the time of the poems' composition, known only as legends. The Homeric epics are largely set in the east and center of the Mediterranean, with some scattered references to Egypt, Ethiopia and other distant lands, in a warlike society that resembles that of the Greek world slightly before the hypothesized date of the poems' composition.[53][54][55][56]\n\nIn ancient Greek chronology, the sack of Troy was dated to 1184 BC. By the nineteenth century, there was widespread scholarly skepticism that the Trojan War had ever happened and that Troy had even existed, but in 1873 Heinrich Schliemann announced to the world that he had discovered the ruins of Homer's Troy at Hisarlik in modern Turkey. Some contemporary scholars think the destruction of Troy VIIa c. 1220 BC was the origin of the myth of the Trojan War, others that the poem was inspired by multiple similar sieges that took place over the centuries.[57]\n\nMost scholars now agree that the Homeric poems depict customs and elements of the material world that are derived from different periods of Greek history.[36][58][59] For instance, the heroes in the poems use bronze weapons, characteristic of the Bronze Age in which the poems are set, rather than the later Iron Age during which they were composed;[36][58][59] yet the same heroes are cremated (an Iron Age practice) rather than buried (as they were in the Bronze Age).[36][58][59] In some parts of the Homeric poems, heroes are described as carrying large shields like those used by warriors during the Mycenaean period,[36] but, in other places, they are instead described carrying the smaller shields that were commonly used during the time when the poems were written in the early Iron Age.[36]\nIn the Iliad 10.260–265, Odysseus is described as wearing a helmet made of boar's tusks. Such helmets were not worn in Homer's time, but were commonly worn by aristocratic warriors between 1600 and 1150 BC.[60][61][62]\n\nThe decipherment of Linear B in the 1950s by Michael Ventris and continued archaeological investigation has increased modern scholars' understanding of the Bronze Age Aegean civilisation, which in many ways resembles the ancient Near East more than the society described by Homer.[63] Some aspects of the Homeric world are simply made up;[36] for instance, the Iliad 22.145–56 describes there being two springs that run near the city of Troy, one that runs steaming hot and the other that runs icy cold.[36] It is here that Hector takes his final stand against Achilles.[36] Archaeologists, however, have uncovered no evidence that springs of this description ever actually existed.[36]\n\nThe Homeric epics are written in an artificial literary language or 'Kunstsprache' only used in epic hexameter poetry. Homeric Greek shows features of multiple regional Greek dialects and periods, but is fundamentally based on Ionic Greek, in keeping with the tradition that Homer was from Ionia. Linguistic analysis suggests that the Iliad was composed slightly before the Odyssey and that Homeric formulae preserve features older than other parts of the poems.[64][65]\n\nThe poems were composed in unrhymed dactylic hexameter; ancient Greek metre was quantity-based rather than stress-based.[66][67] Homer frequently uses set phrases such as epithets ('crafty Odysseus', 'rosy-fingered Dawn', 'owl-eyed Athena', etc.), Homeric formulae ('and then answered [him/her], Agamemnon, king of men', 'when the early-born rose-fingered Dawn came to light', 'thus he/she spoke'), simile, type scenes, ring composition and repetition. These habits aid the extemporizing bard, and are characteristic of oral poetry. For instance, the main words of a Homeric sentence are generally placed towards the beginning, whereas literate poets like Virgil or Milton use longer and more complicated syntactical structures. Homer then expands on these ideas in subsequent clauses; this technique is called parataxis.[68]\n\nThe so-called 'type scenes' (typische Szenen), were named by Walter Arend in 1933. He noted that Homer often, when describing frequently recurring activities such as eating, praying, fighting and dressing, used blocks of set phrases in sequence that were then elaborated by the poet. The 'Analyst' school had considered these repetitions as un-Homeric, whereas Arend interpreted them philosophically. Parry and Lord noted that these conventions are found in many other cultures.[69][70]\n\n'Ring composition' or chiastic structure (when a phrase or idea is repeated at both the beginning and end of a story, or a series of such ideas first appears in the order A, B, C ... before being reversed as ... C, B, A) has been observed in the Homeric epics. Opinion differs as to whether these occurrences are a conscious artistic device, a mnemonic aid or a spontaneous feature of human storytelling.[71][72]\n\nBoth of the Homeric poems begin with an invocation to the Muse.[73] In the Iliad, the poet beseeches her to sing of \"the anger of Achilles\",[73] and in the Odyssey, he asks her to tell of \"the man of many ways\".[73] A similar opening was later employed by Virgil in his Aeneid.[73]\n\nThe orally transmitted Homeric poems were put into written form at some point between the eighth and sixth centuries BCE. Some scholars believe that they were dictated to a scribe by the poet and that our inherited versions of the Iliad and Odyssey were in origin orally dictated texts.[74] Albert Lord noted that the Balkan bards that he was studying revised and expanded their songs in their process of dictating.[75] Some scholars hypothesize that a similar process of revision and expansion occurred when the Homeric poems were first written down.[76][77]\n\nOther scholars hold that, after the poems were created in the eighth century, they continued to be orally transmitted with considerable revision until they were written down in the sixth century.[78] After textualisation, the poems were each divided into 24 rhapsodes, today referred to as books, and labelled by the letters of the Greek alphabet. Most scholars attribute the book divisions to the Hellenistic scholars of Alexandria, in Egypt.[79] Some trace the divisions back further to the Classical period.[80] Very few credit Homer himself with the divisions.[81]\n\nIn antiquity, it was widely held that the Homeric poems were collected and organised in Athens in the late sixth century BCE by Pisistratus (died 528/7 BCE), in what subsequent scholars have dubbed the \"Peisistratean recension\".[82][29] The idea that the Homeric poems were originally transmitted orally and first written down during the reign of Pisistratus is referenced by the first-century BCE Roman orator Cicero and is also referenced in a number of other surviving sources, including two ancient Lives of Homer.[29] From around 150 BCE, the texts of the Homeric poems found in papyrus fragments exhibit much less variation, and the text seems to have become relatively stable. After the establishment of the Library of Alexandria, Homeric scholars such as Zenodotus of Ephesus, Aristophanes of Byzantium and in particular Aristarchus of Samothrace helped establish a canonical text.[83]\n\nThe first printed edition of Homer was produced in 1488 in Milan, Italy by Demetrios Chalkokondyles. Today scholars use medieval manuscripts, papyri and other sources; some argue for a \"multi-text\" view, rather than seeking a single definitive text. The nineteenth-century edition of Arthur Ludwich mainly follows Aristarchus's work, whereas van Thiel's (1991, 1996) follows the medieval vulgate.[clarification needed] Others, such as Martin West (1998–2000) or T. W. Allen, fall somewhere between these two extremes.[83]\n\nHim with that falchion in his hand behold,\n⁠Who comes before the three, even as their lord.\nThat one is Homer, Poet sovereign;\n\nThis is a partial list of translations into English of Homer's Iliad and Odyssey.\n"
    },
    {
        "title": "Evolution",
        "url": "https://en.wikipedia.org/wiki/Evolution",
        "content": "\n\nEvolution is the change in the heritable characteristics of biological populations over successive generations.[1][2] It occurs when evolutionary processes such as natural selection and genetic drift act on genetic variation, resulting in certain characteristics becoming more or less common within a population over successive generations.[3] The process of evolution has given rise to biodiversity at every level of biological organisation.[4][5]\n\nThe scientific theory of evolution by natural selection was conceived independently by two British naturalists, Charles Darwin and Alfred Russel Wallace, in the mid-19th century as an explanation for why organisms are adapted to their physical and biological environments. The theory was first set out in detail in Darwin's book On the Origin of Species.[6] Evolution by natural selection is established by observable facts about living organisms: (1) more offspring are often produced than can possibly survive; (2) traits vary among individuals with respect to their morphology, physiology, and behaviour; (3) different traits confer different rates of survival and reproduction (differential fitness); and (4) traits can be passed from generation to generation (heritability of fitness).[7] In successive generations, members of a population are therefore more likely to be replaced by the offspring of parents with favourable characteristics for that environment.\n\nIn the early 20th century, competing ideas of evolution were refuted and evolution was combined with Mendelian inheritance and population genetics to give rise to modern evolutionary theory.[8] In this synthesis the basis for heredity is in DNA molecules that pass information from generation to generation. The processes that change DNA in a population include natural selection, genetic drift, mutation, and gene flow.[3]\n\nAll life on Earth—including humanity—shares a last universal common ancestor (LUCA),[9][10][11] which lived approximately 3.5–3.8 billion years ago.[12] The fossil record includes a progression from early biogenic graphite[13] to microbial mat fossils[14][15][16] to fossilised multicellular organisms. Existing patterns of biodiversity have been shaped by repeated formations of new species (speciation), changes within species (anagenesis), and loss of species (extinction) throughout the evolutionary history of life on Earth.[17] Morphological and biochemical traits tend to be more similar among species that share a more recent common ancestor, which historically was used to reconstruct phylogenetic trees, although direct comparison of genetic sequences is a more common method today.[18][19]\n\nEvolutionary biologists have continued to study various aspects of evolution by forming and testing hypotheses as well as constructing theories based on evidence from the field or laboratory and on data generated by the methods of mathematical and theoretical biology. Their discoveries have influenced not just the development of biology but also other fields including agriculture, medicine, and computer science.[20]\n\nEvolution in organisms occurs through changes in heritable characteristics—the inherited characteristics of an organism. In humans, for example, eye colour is an inherited characteristic and an individual might inherit the \"brown-eye trait\" from one of their parents.[21] Inherited traits are controlled by genes and the complete set of genes within an organism's genome (genetic material) is called its genotype.[22]\n\nThe complete set of observable traits that make up the structure and behaviour of an organism is called its phenotype. Some of these traits come from the interaction of its genotype with the environment while others are neutral.[23] Some observable characteristics are not inherited. For example, suntanned skin comes from the interaction between a person's genotype and sunlight; thus, suntans are not passed on to people's children. The phenotype is the ability of the skin to tan when exposed to sunlight. However, some people tan more easily than others, due to differences in genotypic variation; a striking example are people with the inherited trait of albinism, who do not tan at all and are very sensitive to sunburn.[24]\n\nHeritable characteristics are passed from one generation to the next via DNA, a molecule that encodes genetic information.[22] DNA is a long biopolymer composed of four types of bases. The sequence of bases along a particular DNA molecule specifies the genetic information, in a manner similar to a sequence of letters spelling out a sentence. Before a cell divides, the DNA is copied, so that each of the resulting two cells will inherit the DNA sequence. Portions of a DNA molecule that specify a single functional unit are called genes; different genes have different sequences of bases. Within cells, each long strand of DNA is called a chromosome. The specific location of a DNA sequence within a chromosome is known as a locus. If the DNA sequence at a locus varies between individuals, the different forms of this sequence are called alleles. DNA sequences can change through mutations, producing new alleles. If a mutation occurs within a gene, the new allele may affect the trait that the gene controls, altering the phenotype of the organism.[25] However, while this simple correspondence between an allele and a trait works in some cases, most traits are influenced by multiple genes in a quantitative or epistatic manner.[26][27]\n\nEvolution can occur if there is genetic variation within a population. Variation comes from mutations in the genome, reshuffling of genes through sexual reproduction and migration between populations (gene flow). Despite the constant introduction of new variation through mutation and gene flow, most of the genome of a species is very similar among all individuals of that species.[28] However, discoveries in the field of evolutionary developmental biology have demonstrated that even relatively small differences in genotype can lead to dramatic differences in phenotype both within and between species.\n\nAn individual organism's phenotype results from both its genotype and the influence of the environment it has lived in.[27] The modern evolutionary synthesis defines evolution as the change over time in this genetic variation. The frequency of one particular allele will become more or less prevalent relative to other forms of that gene. Variation disappears when a new allele reaches the point of fixation—when it either disappears from the population or replaces the ancestral allele entirely.[29]\n\nMutations are changes in the DNA sequence of a cell's genome and are the ultimate source of genetic variation in all organisms.[30] When mutations occur, they may alter the product of a gene, or prevent the gene from functioning, or have no effect. \n\nAbout half of the mutations in the coding regions of protein-coding genes are deleterious — the other half are neutral. A small percentage of the total mutations in this region confer a fitness benefit.[31] Some of the mutations in other parts of the genome are deleterious but the vast majority are neutral. A few are beneficial.\n\nMutations can involve large sections of a chromosome becoming duplicated (usually by genetic recombination), which can introduce extra copies of a gene into a genome.[32] Extra copies of genes are a major source of the raw material needed for new genes to evolve.[33] This is important because most new genes evolve within gene families from pre-existing genes that share common ancestors.[34] For example, the human eye uses four genes to make structures that sense light: three for colour vision and one for night vision; all four are descended from a single ancestral gene.[35]\n\nNew genes can be generated from an ancestral gene when a duplicate copy mutates and acquires a new function. This process is easier once a gene has been duplicated because it increases the redundancy of the system; one gene in the pair can acquire a new function while the other copy continues to perform its original function.[36][37] Other types of mutations can even generate entirely new genes from previously noncoding DNA, a phenomenon termed de novo gene birth.[38][39]\n\nThe generation of new genes can also involve small parts of several genes being duplicated, with these fragments then recombining to form new combinations with new functions (exon shuffling).[40][41] When new genes are assembled from shuffling pre-existing parts, domains act as modules with simple independent functions, which can be mixed together to produce new combinations with new and complex functions.[42] For example, polyketide synthases are large enzymes that make antibiotics; they contain up to 100 independent domains that each catalyse one step in the overall process, like a step in an assembly line.[43]\n\nOne example of mutation is wild boar piglets. They are camouflage coloured and show a characteristic pattern of dark and light longitudinal stripes. However, mutations in the melanocortin 1 receptor (MC1R) disrupt the pattern. The majority of pig breeds carry MC1R mutations disrupting wild-type colour and different mutations causing dominant black colouring.[44]\n\nIn asexual organisms, genes are inherited together, or linked, as they cannot mix with genes of other organisms during reproduction. In contrast, the offspring of sexual organisms contain random mixtures of their parents' chromosomes that are produced through independent assortment. In a related process called homologous recombination, sexual organisms exchange DNA between two matching chromosomes.[45] Recombination and reassortment do not alter allele frequencies, but instead change which alleles are associated with each other, producing offspring with new combinations of alleles.[46] Sex usually increases genetic variation and may increase the rate of evolution.[47][48]\n\nThe two-fold cost of sex was first described by John Maynard Smith.[49] The first cost is that in sexually dimorphic species only one of the two sexes can bear young. This cost does not apply to hermaphroditic species, like most plants and many invertebrates. The second cost is that any individual who reproduces sexually can only pass on 50% of its genes to any individual offspring, with even less passed on as each new generation passes.[50] Yet sexual reproduction is the more common means of reproduction among eukaryotes and multicellular organisms. The Red Queen hypothesis has been used to explain the significance of sexual reproduction as a means to enable continual evolution and adaptation in response to coevolution with other species in an ever-changing environment.[50][51][52][53] Another hypothesis is that sexual reproduction is primarily an adaptation for promoting accurate recombinational repair of damage in germline DNA, and that increased diversity is a byproduct of this process that may sometimes be adaptively beneficial.[54][55]\n\nGene flow is the exchange of genes between populations and between species.[56] It can therefore be a source of variation that is new to a population or to a species. Gene flow can be caused by the movement of individuals between separate populations of organisms, as might be caused by the movement of mice between inland and coastal populations, or the movement of pollen between heavy-metal-tolerant and heavy-metal-sensitive populations of grasses.\n\nGene transfer between species includes the formation of hybrid organisms and horizontal gene transfer. Horizontal gene transfer is the transfer of genetic material from one organism to another organism that is not its offspring; this is most common among bacteria.[57] In medicine, this contributes to the spread of antibiotic resistance, as when one bacteria acquires resistance genes it can rapidly transfer them to other species.[58] Horizontal transfer of genes from bacteria to eukaryotes such as the yeast Saccharomyces cerevisiae and the adzuki bean weevil Callosobruchus chinensis has occurred.[59][60] An example of larger-scale transfers are the eukaryotic bdelloid rotifers, which have received a range of genes from bacteria, fungi and plants.[61] Viruses can also carry DNA between organisms, allowing transfer of genes even across biological domains.[62]\n\nLarge-scale gene transfer has also occurred between the ancestors of eukaryotic cells and bacteria, during the acquisition of chloroplasts and mitochondria. It is possible that eukaryotes themselves originated from horizontal gene transfers between bacteria and archaea.[63]\n\nSome heritable changes cannot be explained by changes to the sequence of nucleotides in the DNA. These phenomena are classed as epigenetic inheritance systems.[64] DNA methylation marking chromatin, self-sustaining metabolic loops, gene silencing by RNA interference and the three-dimensional conformation of proteins (such as prions) are areas where epigenetic inheritance systems have been discovered at the organismic level.[65] Developmental biologists suggest that complex interactions in genetic networks and communication among cells can lead to heritable variations that may underlay some of the mechanics in developmental plasticity and canalisation.[66] Heritability may also occur at even larger scales. For example, ecological inheritance through the process of niche construction is defined by the regular and repeated activities of organisms in their environment. This generates a legacy of effects that modify and feed back into the selection regime of subsequent generations.[67] Other examples of heritability in evolution that are not under the direct control of genes include the inheritance of cultural traits and symbiogenesis.[68][69]\n\nFrom a neo-Darwinian perspective, evolution occurs when there are changes in the frequencies of alleles within a population of interbreeding organisms,[70] for example, the allele for black colour in a population of moths becoming more common. Mechanisms that can lead to changes in allele frequencies include natural selection, genetic drift, and mutation bias.\n\nEvolution by natural selection is the process by which traits that enhance survival and reproduction become more common in successive generations of a population. It embodies three principles:[7]\n\nMore offspring are produced than can possibly survive, and these conditions produce competition between organisms for survival and reproduction. Consequently, organisms with traits that give them an advantage over their competitors are more likely to pass on their traits to the next generation than those with traits that do not confer an advantage.[71] This teleonomy is the quality whereby the process of natural selection creates and preserves traits that are seemingly fitted for the functional roles they perform.[72] Consequences of selection include nonrandom mating[73] and genetic hitchhiking.\n\nThe central concept of natural selection is the evolutionary fitness of an organism.[74] Fitness is measured by an organism's ability to survive and reproduce, which determines the size of its genetic contribution to the next generation.[74] However, fitness is not the same as the total number of offspring: instead fitness is indicated by the proportion of subsequent generations that carry an organism's genes.[75] For example, if an organism could survive well and reproduce rapidly, but its offspring were all too small and weak to survive, this organism would make little genetic contribution to future generations and would thus have low fitness.[74]\n\nIf an allele increases fitness more than the other alleles of that gene, then with each generation this allele has a higher probability of becoming common within the population. These traits are said to be \"selected for.\" Examples of traits that can increase fitness are enhanced survival and increased fecundity. Conversely, the lower fitness caused by having a less beneficial or deleterious allele results in this allele likely becoming rarer—they are \"selected against.\"[76]\n\nImportantly, the fitness of an allele is not a fixed characteristic; if the environment changes, previously neutral or harmful traits may become beneficial and previously beneficial traits become harmful.[25] However, even if the direction of selection does reverse in this way, traits that were lost in the past may not re-evolve in an identical form.[77][78] However, a re-activation of dormant genes, as long as they have not been eliminated from the genome and were only suppressed perhaps for hundreds of generations, can lead to the re-occurrence of traits thought to be lost like hindlegs in dolphins, teeth in chickens, wings in wingless stick insects, tails and additional nipples in humans etc. \"Throwbacks\" such as these are known as atavisms.[79]\n\nNatural selection within a population for a trait that can vary across a range of values, such as height, can be categorised into three different types. The first is directional selection, which is a shift in the average value of a trait over time—for example, organisms slowly getting taller.[80] Secondly, disruptive selection is selection for extreme trait values and often results in two different values becoming most common, with selection against the average value. This would be when either short or tall organisms had an advantage, but not those of medium height. Finally, in stabilising selection there is selection against extreme trait values on both ends, which causes a decrease in variance around the average value and less diversity.[71][81] This would, for example, cause organisms to eventually have a similar height.\n\nNatural selection most generally makes nature the measure against which individuals and individual traits, are more or less likely to survive. \"Nature\" in this sense refers to an ecosystem, that is, a system in which organisms interact with every other element, physical as well as biological, in their local environment. Eugene Odum, a founder of ecology, defined an ecosystem as: \"Any unit that includes all of the organisms...in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e., exchange of materials between living and nonliving parts) within the system....\"[82] Each population within an ecosystem occupies a distinct niche, or position, with distinct relationships to other parts of the system. These relationships involve the life history of the organism, its position in the food chain and its geographic range. This broad understanding of nature enables scientists to delineate specific forces which, together, comprise natural selection.\n\nNatural selection can act at different levels of organisation, such as genes, cells, individual organisms, groups of organisms and species.[83][84][85] Selection can act at multiple levels simultaneously.[86] An example of selection occurring below the level of the individual organism are genes called transposons, which can replicate and spread throughout a genome.[87] Selection at a level above the individual, such as group selection, may allow the evolution of cooperation.[88]\n\nGenetic drift is the random fluctuation of allele frequencies within a population from one generation to the next.[89] When selective forces are absent or relatively weak, allele frequencies are equally likely to drift upward or downward[clarification needed] in each successive generation because the alleles are subject to sampling error.[90] This drift halts when an allele eventually becomes fixed, either by disappearing from the population or by replacing the other alleles entirely. Genetic drift may therefore eliminate some alleles from a population due to chance alone. Even in the absence of selective forces, genetic drift can cause two separate populations that begin with the same genetic structure to drift apart into two divergent populations with different sets of alleles.[91]\n\nAccording to the neutral theory of molecular evolution most evolutionary changes are the result of the fixation of neutral mutations by genetic drift.[92] In this model, most genetic changes in a population are thus the result of constant mutation pressure and genetic drift.[93] This form of the neutral theory has been debated since it does not seem to fit some genetic variation seen in nature.[94][95] A better-supported version of this model is the nearly neutral theory, according to which a mutation that would be effectively neutral in a small population is not necessarily neutral in a large population.[71] Other theories propose that genetic drift is dwarfed by other stochastic forces in evolution, such as genetic hitchhiking, also known as genetic draft.[90][96][97] Another concept is constructive neutral evolution (CNE), which explains that complex systems can emerge and spread into a population through neutral transitions due to the principles of excess capacity, presuppression, and ratcheting,[98][99][100] and it has been applied in areas ranging from the origins of the spliceosome to the complex interdependence of microbial communities.[101][102][103]\n\nThe time it takes a neutral allele to become fixed by genetic drift depends on population size; fixation is more rapid in smaller populations.[104] The number of individuals in a population is not critical, but instead a measure known as the effective population size.[105] The effective population is usually smaller than the total population since it takes into account factors such as the level of inbreeding and the stage of the lifecycle in which the population is the smallest.[105] The effective population size may not be the same for every gene in the same population.[106]\n\nIt is usually difficult to measure the relative importance of selection and neutral processes, including drift.[107] The comparative importance of adaptive and non-adaptive forces in driving evolutionary change is an area of current research.[108]\n\nMutation bias is usually conceived as a difference in expected rates for two different kinds of mutation, e.g., transition-transversion bias, GC-AT bias, deletion-insertion bias. This is related to the idea of developmental bias. Haldane[109] and Fisher[110] argued that, because mutation is a weak pressure easily overcome by selection, tendencies of mutation would be ineffectual except under conditions of neutral evolution or extraordinarily high mutation rates. This opposing-pressures argument was long used to dismiss the possibility of internal tendencies in evolution,[111] until the molecular era prompted renewed interest in neutral evolution.\n\nNoboru Sueoka[112] and Ernst Freese[113] proposed that systematic biases in mutation might be responsible for systematic differences in genomic GC composition between species. The identification of a GC-biased E. coli mutator strain in 1967,[114] along with the proposal of the neutral theory, established the plausibility of mutational explanations for molecular patterns, which are now common in the molecular evolution literature.\n\nFor instance, mutation biases are frequently invoked in models of codon usage.[115] Such models also include effects of selection, following the mutation-selection-drift model,[116] which allows both for mutation biases and differential selection based on effects on translation. Hypotheses of mutation bias have played an important role in the development of thinking about the evolution of genome composition, including isochores.[117] Different insertion vs. deletion biases in different taxa can lead to the evolution of different genome sizes.[118][119] The hypothesis of Lynch regarding genome size relies on mutational biases toward increase or decrease in genome size.\n\nHowever, mutational hypotheses for the evolution of composition suffered a reduction in scope when it was discovered that (1) GC-biased gene conversion makes an important contribution to composition in diploid organisms such as mammals[120] and (2) bacterial genomes frequently have AT-biased mutation.[121]\n\nContemporary thinking about the role of mutation biases reflects a different theory from that of Haldane and Fisher. More recent work[111] showed that the original \"pressures\" theory assumes that evolution is based on standing variation: when evolution depends on events of mutation that introduce new alleles, mutational and developmental biases in the introduction of variation (arrival biases) can impose biases on evolution without requiring neutral evolution or high mutation rates.[111][122]\nSeveral studies report that the mutations implicated in adaptation reflect common mutation biases[123][124][125] though others dispute this interpretation.[126]\n\nRecombination allows alleles on the same strand of DNA to become separated. However, the rate of recombination is low (approximately two events per chromosome per generation). As a result, genes close together on a chromosome may not always be shuffled away from each other and genes that are close together tend to be inherited together, a phenomenon known as linkage.[127] This tendency is measured by finding how often two alleles occur together on a single chromosome compared to expectations, which is called their linkage disequilibrium. A set of alleles that is usually inherited in a group is called a haplotype. This can be important when one allele in a particular haplotype is strongly beneficial: natural selection can drive a selective sweep that will also cause the other alleles in the haplotype to become more common in the population; this effect is called genetic hitchhiking or genetic draft.[128] Genetic draft caused by the fact that some neutral genes are genetically linked to others that are under selection can be partially captured by an appropriate effective population size.[96]\n\nA special case of natural selection is sexual selection, which is selection for any trait that increases mating success by increasing the attractiveness of an organism to potential mates.[130] Traits that evolved through sexual selection are particularly prominent among males of several animal species. Although sexually favoured, traits such as cumbersome antlers, mating calls, large body size and bright colours often attract predation, which compromises the survival of individual males.[131][132] This survival disadvantage is balanced by higher reproductive success in males that show these hard-to-fake, sexually selected traits.[133]\n\nEvolution influences every aspect of the form and behaviour of organisms. Most prominent are the specific behavioural and physical adaptations that are the outcome of natural selection. These adaptations increase fitness by aiding activities such as finding food, avoiding predators or attracting mates. Organisms can also respond to selection by cooperating with each other, usually by aiding their relatives or engaging in mutually beneficial symbiosis. In the longer term, evolution produces new species through splitting ancestral populations of organisms into new groups that cannot or will not interbreed. These outcomes of evolution are distinguished based on time scale as macroevolution versus microevolution. Macroevolution refers to evolution that occurs at or above the level of species, in particular speciation and extinction, whereas microevolution refers to smaller evolutionary changes within a species or population, in particular shifts in allele frequency and adaptation.[135] Macroevolution is the outcome of long periods of microevolution.[136] Thus, the distinction between micro- and macroevolution is not a fundamental one—the difference is simply the time involved.[137] However, in macroevolution, the traits of the entire species may be important. For instance, a large amount of variation among individuals allows a species to rapidly adapt to new habitats, lessening the chance of it going extinct, while a wide geographic range increases the chance of speciation, by making it more likely that part of the population will become isolated. In this sense, microevolution and macroevolution might involve selection at different levels—with microevolution acting on genes and organisms, versus macroevolutionary processes such as species selection acting on entire species and affecting their rates of speciation and extinction.[138][139][140]\n\nA common misconception is that evolution has goals, long-term plans, or an innate tendency for \"progress\", as expressed in beliefs such as orthogenesis and evolutionism; realistically, however, evolution has no long-term goal and does not necessarily produce greater complexity.[141][142][143] Although complex species have evolved, they occur as a side effect of the overall number of organisms increasing, and simple forms of life still remain more common in the biosphere.[144] For example, the overwhelming majority of species are microscopic prokaryotes, which form about half the world's biomass despite their small size[145] and constitute the vast majority of Earth's biodiversity.[146] Simple organisms have therefore been the dominant form of life on Earth throughout its history and continue to be the main form of life up to the present day, with complex life only appearing more diverse because it is more noticeable.[147] Indeed, the evolution of microorganisms is particularly important to evolutionary research since their rapid reproduction allows the study of experimental evolution and the observation of evolution and adaptation in real time.[148][149]\n\nAdaptation is the process that makes organisms better suited to their habitat.[150][151] Also, the term adaptation may refer to a trait that is important for an organism's survival. For example, the adaptation of horses' teeth to the grinding of grass. By using the term adaptation for the evolutionary process and adaptive trait for the product (the bodily part or function), the two senses of the word may be distinguished. Adaptations are produced by natural selection.[152] The following definitions are due to Theodosius Dobzhansky:\n\nAdaptation may cause either the gain of a new feature, or the loss of an ancestral feature. An example that shows both types of change is bacterial adaptation to antibiotic selection, with genetic changes causing antibiotic resistance by both modifying the target of the drug, or increasing the activity of transporters that pump the drug out of the cell.[156] Other striking examples are the bacteria Escherichia coli evolving the ability to use citric acid as a nutrient in a long-term laboratory experiment,[157] Flavobacterium evolving a novel enzyme that allows these bacteria to grow on the by-products of nylon manufacturing,[158][159] and the soil bacterium Sphingobium evolving an entirely new metabolic pathway that degrades the synthetic pesticide pentachlorophenol.[160][161] An interesting but still controversial idea is that some adaptations might increase the ability of organisms to generate genetic diversity and adapt by natural selection (increasing organisms' evolvability).[162][163][164][165]\n\nAdaptation occurs through the gradual modification of existing structures. Consequently, structures with similar internal organisation may have different functions in related organisms. This is the result of a single ancestral structure being adapted to function in different ways. The bones within bat wings, for example, are very similar to those in mice feet and primate hands, due to the descent of all these structures from a common mammalian ancestor.[167] However, since all living organisms are related to some extent,[168] even organs that appear to have little or no structural similarity, such as arthropod, squid and vertebrate eyes, or the limbs and wings of arthropods and vertebrates, can depend on a common set of homologous genes that control their assembly and function; this is called deep homology.[169][170]\n\nDuring evolution, some structures may lose their original function and become vestigial structures.[171] Such structures may have little or no function in a current species, yet have a clear function in ancestral species, or other closely related species. Examples include pseudogenes,[172] the non-functional remains of eyes in blind cave-dwelling fish,[173] wings in flightless birds,[174] the presence of hip bones in whales and snakes,[166] and sexual traits in organisms that reproduce via asexual reproduction.[175] Examples of vestigial structures in humans include wisdom teeth,[176] the coccyx,[171] the vermiform appendix,[171] and other behavioural vestiges such as goose bumps[177][178] and primitive reflexes.[179][180][181]\n\nHowever, many traits that appear to be simple adaptations are in fact exaptations: structures originally adapted for one function, but which coincidentally became somewhat useful for some other function in the process.[182] One example is the African lizard Holaspis guentheri, which developed an extremely flat head for hiding in crevices, as can be seen by looking at its near relatives. However, in this species, the head has become so flattened that it assists in gliding from tree to tree—an exaptation.[182] Within cells, molecular machines such as the bacterial flagella[183] and protein sorting machinery[184] evolved by the recruitment of several pre-existing proteins that previously had different functions.[135] Another example is the recruitment of enzymes from glycolysis and xenobiotic metabolism to serve as structural proteins called crystallins within the lenses of organisms' eyes.[185][186]\n\nAn area of current investigation in evolutionary developmental biology is the developmental basis of adaptations and exaptations.[187] This research addresses the origin and evolution of embryonic development and how modifications of development and developmental processes produce novel features.[188] These studies have shown that evolution can alter development to produce new structures, such as embryonic bone structures that develop into the jaw in other animals instead forming part of the middle ear in mammals.[189] It is also possible for structures that have been lost in evolution to reappear due to changes in developmental genes, such as a mutation in chickens causing embryos to grow teeth similar to those of crocodiles.[190] It is now becoming clear that most alterations in the form of organisms are due to changes in a small set of conserved genes.[191]\n\nInteractions between organisms can produce both conflict and cooperation. When the interaction is between pairs of species, such as a pathogen and a host, or a predator and its prey, these species can develop matched sets of adaptations. Here, the evolution of one species causes adaptations in a second species. These changes in the second species then, in turn, cause new adaptations in the first species. This cycle of selection and response is called coevolution.[192] An example is the production of tetrodotoxin in the rough-skinned newt and the evolution of tetrodotoxin resistance in its predator, the common garter snake. In this predator-prey pair, an evolutionary arms race has produced high levels of toxin in the newt and correspondingly high levels of toxin resistance in the snake.[193]\n\nNot all co-evolved interactions between species involve conflict.[194] Many cases of mutually beneficial interactions have evolved. For instance, an extreme cooperation exists between plants and the mycorrhizal fungi that grow on their roots and aid the plant in absorbing nutrients from the soil.[195] This is a reciprocal relationship as the plants provide the fungi with sugars from photosynthesis. Here, the fungi actually grow inside plant cells, allowing them to exchange nutrients with their hosts, while sending signals that suppress the plant immune system.[196]\n\nCoalitions between organisms of the same species have also evolved. An extreme case is the eusociality found in social insects, such as bees, termites and ants, where sterile insects feed and guard the small number of organisms in a colony that are able to reproduce. On an even smaller scale, the somatic cells that make up the body of an animal limit their reproduction so they can maintain a stable organism, which then supports a small number of the animal's germ cells to produce offspring. Here, somatic cells respond to specific signals that instruct them whether to grow, remain as they are, or die. If cells ignore these signals and multiply inappropriately, their uncontrolled growth causes cancer.[197]\n\nSuch cooperation within species may have evolved through the process of kin selection, which is where one organism acts to help raise a relative's offspring.[198] This activity is selected for because if the helping individual contains alleles which promote the helping activity, it is likely that its kin will also contain these alleles and thus those alleles will be passed on.[199] Other processes that may promote cooperation include group selection, where cooperation provides benefits to a group of organisms.[200]\n\nSpeciation is the process where a species diverges into two or more descendant species.[201]\n\nThere are multiple ways to define the concept of \"species\". The choice of definition is dependent on the particularities of the species concerned.[202] For example, some species concepts apply more readily toward sexually reproducing organisms while others lend themselves better toward asexual organisms. Despite the diversity of various species concepts, these various concepts can be placed into one of three broad philosophical approaches: interbreeding, ecological and phylogenetic.[203] The Biological Species Concept (BSC) is a classic example of the interbreeding approach. Defined by evolutionary biologist Ernst Mayr in 1942, the BSC states that \"species are groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups.\"[204] Despite its wide and long-term use, the BSC like other species concepts is not without controversy, for example, because genetic recombination among prokaryotes is not an intrinsic aspect of reproduction;[205] this is called the species problem.[202] Some researchers have attempted a unifying monistic definition of species, while others adopt a pluralistic approach and suggest that there may be different ways to logically interpret the definition of a species.[202][203]\n\nBarriers to reproduction between two diverging sexual populations are required for the populations to become new species. Gene flow may slow this process by spreading the new genetic variants also to the other populations. Depending on how far two species have diverged since their most recent common ancestor, it may still be possible for them to produce offspring, as with horses and donkeys mating to produce mules.[206] Such hybrids are generally infertile. In this case, closely related species may regularly interbreed, but hybrids will be selected against and the species will remain distinct. However, viable hybrids are occasionally formed and these new species can either have properties intermediate between their parent species, or possess a totally new phenotype.[207] The importance of hybridisation in producing new species of animals is unclear, although cases have been seen in many types of animals,[208] with the gray tree frog being a particularly well-studied example.[209]\n\nSpeciation has been observed multiple times under both controlled laboratory conditions and in nature.[210] In sexually reproducing organisms, speciation results from reproductive isolation followed by genealogical divergence. There are four primary geographic modes of speciation. The most common in animals is allopatric speciation, which occurs in populations initially isolated geographically, such as by habitat fragmentation or migration. Selection under these conditions can produce very rapid changes in the appearance and behaviour of organisms.[211][212] As selection and drift act independently on populations isolated from the rest of their species, separation may eventually produce organisms that cannot interbreed.[213]\n\nThe second mode of speciation is peripatric speciation, which occurs when small populations of organisms become isolated in a new environment. This differs from allopatric speciation in that the isolated populations are numerically much smaller than the parental population. Here, the founder effect causes rapid speciation after an increase in inbreeding increases selection on homozygotes, leading to rapid genetic change.[214]\n\nThe third mode is parapatric speciation. This is similar to peripatric speciation in that a small population enters a new habitat, but differs in that there is no physical separation between these two populations. Instead, speciation results from the evolution of mechanisms that reduce gene flow between the two populations.[201] Generally this occurs when there has been a drastic change in the environment within the parental species' habitat. One example is the grass Anthoxanthum odoratum, which can undergo parapatric speciation in response to localised metal pollution from mines.[215] Here, plants evolve that have resistance to high levels of metals in the soil. Selection against interbreeding with the metal-sensitive parental population produced a gradual change in the flowering time of the metal-resistant plants, which eventually produced complete reproductive isolation. Selection against hybrids between the two populations may cause reinforcement, which is the evolution of traits that promote mating within a species, as well as character displacement, which is when two species become more distinct in appearance.[216]\n\nFinally, in sympatric speciation species diverge without geographic isolation or changes in habitat. This form is rare since even a small amount of gene flow may remove genetic differences between parts of a population.[217] Generally, sympatric speciation in animals requires the evolution of both genetic differences and nonrandom mating, to allow reproductive isolation to evolve.[218]\n\nOne type of sympatric speciation involves crossbreeding of two related species to produce a new hybrid species. This is not common in animals as animal hybrids are usually sterile. This is because during meiosis the homologous chromosomes from each parent are from different species and cannot successfully pair. However, it is more common in plants because plants often double their number of chromosomes, to form polyploids.[219] This allows the chromosomes from each parental species to form matching pairs during meiosis, since each parent's chromosomes are represented by a pair already.[220] An example of such a speciation event is when the plant species Arabidopsis thaliana and Arabidopsis arenosa crossbred to give the new species Arabidopsis suecica.[221] This happened about 20,000 years ago,[222] and the speciation process has been repeated in the laboratory, which allows the study of the genetic mechanisms involved in this process.[223] Indeed, chromosome doubling within a species may be a common cause of reproductive isolation, as half the doubled chromosomes will be unmatched when breeding with undoubled organisms.[224]\n\nSpeciation events are important in the theory of punctuated equilibrium, which accounts for the pattern in the fossil record of short \"bursts\" of evolution interspersed with relatively long periods of stasis, where species remain relatively unchanged.[225] In this theory, speciation and rapid evolution are linked, with natural selection and genetic drift acting most strongly on organisms undergoing speciation in novel habitats or small populations. As a result, the periods of stasis in the fossil record correspond to the parental population and the organisms undergoing speciation and rapid evolution are found in small populations or geographically restricted habitats and therefore rarely being preserved as fossils.[139]\n\nExtinction is the disappearance of an entire species. Extinction is not an unusual event, as species regularly appear through speciation and disappear through extinction.[226] Nearly all animal and plant species that have lived on Earth are now extinct,[227] and extinction appears to be the ultimate fate of all species.[228] These extinctions have happened continuously throughout the history of life, although the rate of extinction spikes in occasional mass extinction events.[229] The Cretaceous–Paleogene extinction event, during which the non-avian dinosaurs became extinct, is the most well-known, but the earlier Permian–Triassic extinction event was even more severe, with approximately 96% of all marine species driven to extinction.[229] The Holocene extinction event is an ongoing mass extinction associated with humanity's expansion across the globe over the past few thousand years. Present-day extinction rates are 100–1000 times greater than the background rate and up to 30% of current species may be extinct by the mid 21st century.[230] Human activities are now the primary cause of the ongoing extinction event;[231][232] global warming may further accelerate it in the future.[233] Despite the estimated extinction of more than 99% of all species that ever lived on Earth,[234][235] about 1 trillion species are estimated to be on Earth currently with only one-thousandth of 1% described.[236]\n\nThe role of extinction in evolution is not very well understood and may depend on which type of extinction is considered.[229] The causes of the continuous \"low-level\" extinction events, which form the majority of extinctions, may be the result of competition between species for limited resources (the competitive exclusion principle).[237] If one species can out-compete another, this could produce species selection, with the fitter species surviving and the other species being driven to extinction.[84] The intermittent mass extinctions are also important, but instead of acting as a selective force, they drastically reduce diversity in a nonspecific manner and promote bursts of rapid evolution and speciation in survivors.[238]\n\nConcepts and models used in evolutionary biology, such as natural selection, have many applications.[239]\n\nArtificial selection is the intentional selection of traits in a population of organisms. This has been used for thousands of years in the domestication of plants and animals.[240] More recently, such selection has become a vital part of genetic engineering, with selectable markers such as antibiotic resistance genes being used to manipulate DNA. Proteins with valuable properties have evolved by repeated rounds of mutation and selection (for example modified enzymes and new antibodies) in a process called directed evolution.[241]\n\nUnderstanding the changes that have occurred during an organism's evolution can reveal the genes needed to construct parts of the body, genes which may be involved in human genetic disorders.[242] For example, the Mexican tetra is an albino cavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves.[243] This helped identify genes required for vision and pigmentation.[244]\n\nEvolutionary theory has many applications in medicine. Many human diseases are not static phenomena, but capable of evolution. Viruses, bacteria, fungi and cancers evolve to be resistant to host immune defences, as well as to pharmaceutical drugs.[245][246][247] These same problems occur in agriculture with pesticide[248] and herbicide[249] resistance. It is possible that we are facing the end of the effective life of most of available antibiotics[250] and predicting the evolution and evolvability[251] of our pathogens and devising strategies to slow or circumvent it is requiring deeper knowledge of the complex forces driving evolution at the molecular level.[252]\n\nIn computer science, simulations of evolution using evolutionary algorithms and artificial life started in the 1960s and were extended with simulation of artificial selection.[253] Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s. He used evolution strategies to solve complex engineering problems.[254] Genetic algorithms in particular became popular through the writing of John Henry Holland.[255] Practical applications also include automatic evolution of computer programmes.[256] Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers and also to optimise the design of systems.[257]\n\nThe Earth is about 4.54 billion years old.[258][259][260] The earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago,[12][261] during the Eoarchean Era after a geological crust started to solidify following the earlier molten Hadean Eon. Microbial mat fossils have been found in 3.48 billion-year-old sandstone in Western Australia.[14][15][16] Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old metasedimentary rocks discovered in Western Greenland[13] as well as \"remains of biotic life\" found in 4.1 billion-year-old rocks in Western Australia.[262][263] Commenting on the Australian findings, Stephen Blair Hedges wrote: \"If life arose relatively quickly on Earth, then it could be common in the universe.\"[262][264]  In July 2016, scientists reported identifying a set of 355 genes from the last universal common ancestor (LUCA) of all organisms living on Earth.[265]\n\nMore than 99% of all species, amounting to over five billion species,[266] that ever lived on Earth are estimated to be extinct.[234][235] Estimates on the number of Earth's current species range from 10 million to 14 million,[267][268] of which about 1.9 million are estimated to have been named[269] and 1.6 million documented in a central database to date,[270] leaving at least 80% not yet described.\n\nHighly energetic chemistry is thought to have produced a self-replicating molecule around 4 billion years ago, and half a billion years later the last common ancestor of all life existed.[10] The current scientific consensus is that the complex biochemistry that makes up life came from simpler chemical reactions.[271][272] The beginning of life may have included self-replicating molecules such as RNA[273] and the assembly of simple cells.[274]\n\nAll organisms on Earth are descended from a common ancestor or ancestral gene pool.[168][275] Current species are a stage in the process of evolution, with their diversity the product of a long series of speciation and extinction events.[276] The common descent of organisms was first deduced from four simple facts about organisms: First, they have geographic distributions that cannot be explained by local adaptation. Second, the diversity of life is not a set of completely unique organisms, but organisms that share morphological similarities. Third, vestigial traits with no clear purpose resemble functional ancestral traits. Fourth, organisms can be classified using these similarities into a hierarchy of nested groups, similar to a family tree.[277]\n\nDue to horizontal gene transfer, this \"tree of life\" may be more complicated than a simple branching tree, since some genes have spread independently between distantly related species.[278][279] To solve this problem and others, some authors prefer to use the \"Coral of life\" as a metaphor or a mathematical model to illustrate the evolution of life. This view dates back to an idea briefly mentioned by Darwin but later abandoned.[280]\n\nPast species have also left records of their evolutionary history. Fossils, along with the comparative anatomy of present-day organisms, constitute the morphological, or anatomical, record.[281] By comparing the anatomies of both modern and extinct species, palaeontologists can infer the lineages of those species. However, this approach is most successful for organisms that had hard body parts, such as shells, bones or teeth. Further, as prokaryotes such as bacteria and archaea share a limited set of common morphologies, their fossils do not provide information on their ancestry.\n\nMore recently, evidence for common descent has come from the study of biochemical similarities between organisms. For example, all living cells use the same basic set of nucleotides and amino acids.[282] The development of molecular genetics has revealed the record of evolution left in organisms' genomes: dating when species diverged through the molecular clock produced by mutations.[283] For example, these DNA sequence comparisons have revealed that humans and chimpanzees share 98% of their genomes and analysing the few areas where they differ helps shed light on when the common ancestor of these species existed.[284]\n\nProkaryotes inhabited the Earth from approximately 3–4 billion years ago.[286][287] No obvious changes in morphology or cellular organisation occurred in these organisms over the next few billion years.[288] The eukaryotic cells emerged between 1.6 and 2.7 billion years ago. The next major change in cell structure came when bacteria were engulfed by eukaryotic cells, in a cooperative association called endosymbiosis.[289][290] The engulfed bacteria and the host cell then underwent coevolution, with the bacteria evolving into either mitochondria or hydrogenosomes.[291] Another engulfment of cyanobacterial-like organisms led to the formation of chloroplasts in algae and plants.[292]\n\nThe history of life was that of the unicellular eukaryotes, prokaryotes and archaea until about 610 million years ago when multicellular organisms began to appear in the oceans in the Ediacaran period.[286][293] The evolution of multicellularity occurred in multiple independent events, in organisms as diverse as sponges, brown algae, cyanobacteria, slime moulds and myxobacteria.[294] In January 2016, scientists reported that, about 800 million years ago, a minor genetic change in a single molecule called GK-PID may have allowed organisms to go from a single cell organism to one of many cells.[295]\n\nSoon after the emergence of these first multicellular organisms, a remarkable amount of biological diversity appeared over approximately 10 million years, in an event called the Cambrian explosion. Here, the majority of types of modern animals appeared in the fossil record, as well as unique lineages that subsequently became extinct.[296] Various triggers for the Cambrian explosion have been proposed, including the accumulation of oxygen in the atmosphere from photosynthesis.[297]\n\nAbout 500 million years ago, plants and fungi colonised the land and were soon followed by arthropods and other animals.[298] Insects were particularly successful and even today make up the majority of animal species.[299] Amphibians first appeared around 364 million years ago, followed by early amniotes and birds around 155 million years ago (both from \"reptile\"-like lineages), mammals around 129 million years ago, Homininae around 10 million years ago and modern humans around 250,000 years ago.[300][301][302] However, despite the evolution of these large animals, smaller organisms similar to the types that evolved early in this process continue to be highly successful and dominate the Earth, with the majority of both biomass and species being prokaryotes.[146]\n\nThe proposal that one type of organism could descend from another type goes back to some of the first pre-Socratic Greek philosophers, such as Anaximander and Empedocles.[304] Such proposals survived into Roman times. The poet and philosopher Lucretius followed Empedocles in his masterwork De rerum natura (lit. 'On the Nature of Things').[305][306]\n\nIn contrast to these materialistic views, Aristotelianism had considered all natural things as actualisations of fixed natural possibilities, known as forms.[307][308] This became part of a medieval teleological understanding of nature in which all things have an intended role to play in a divine cosmic order. Variations of this idea became the standard understanding of the Middle Ages and were integrated into Christian learning, but Aristotle did not demand that real types of organisms always correspond one-for-one with exact metaphysical forms and specifically gave examples of how new types of living things could come to be.[309]\n\nA number of Arab Muslim scholars wrote about evolution, most notably Ibn Khaldun, who wrote the book Muqaddimah in 1377 AD, in which he asserted that humans developed from \"the world of the monkeys\", in a process by which \"species become more numerous\".[310]\n\nThe \"New Science\" of the 17th century rejected the Aristotelian approach. It sought to explain natural phenomena in terms of physical laws that were the same for all visible things and that did not require the existence of any fixed natural categories or divine cosmic order. However, this new approach was slow to take root in the biological sciences: the last bastion of the concept of fixed natural types. John Ray applied one of the previously more general terms for fixed natural types, \"species\", to plant and animal types, but he strictly identified each type of living thing as a species and proposed that each species could be defined by the features that perpetuated themselves generation after generation.[311] The biological classification introduced by Carl Linnaeus in 1735 explicitly recognised the hierarchical nature of species relationships, but still viewed species as fixed according to a divine plan.[312]\n\nOther naturalists of this time speculated on the evolutionary change of species over time according to natural laws. In 1751, Pierre Louis Maupertuis wrote of natural modifications occurring during reproduction and accumulating over many generations to produce new species.[313] Georges-Louis Leclerc, Comte de Buffon, suggested that species could degenerate into different organisms, and Erasmus Darwin proposed that all warm-blooded animals could have descended from a single microorganism (or \"filament\").[314] The first full-fledged evolutionary scheme was Jean-Baptiste Lamarck's \"transmutation\" theory of 1809,[315] which envisaged spontaneous generation continually producing simple forms of life that developed greater complexity in parallel lineages with an inherent progressive tendency, and postulated that on a local level, these lineages adapted to the environment by inheriting changes caused by their use or disuse in parents.[316] (The latter process was later called Lamarckism.)[316][317][318] These ideas were condemned by established naturalists as speculation lacking empirical support. In particular, Georges Cuvier insisted that species were unrelated and fixed, their similarities reflecting divine design for functional needs. In the meantime, Ray's ideas of benevolent design had been developed by William Paley into the Natural Theology or Evidences of the Existence and Attributes of the Deity (1802), which proposed complex adaptations as evidence of divine design and which was admired by Charles Darwin.[319][320]\n\nThe crucial break from the concept of constant typological classes or types in biology came with the theory of evolution through natural selection, which was formulated by Charles Darwin and Alfred Wallace in terms of variable populations. Darwin used the expression \"descent with modification\" rather than \"evolution\".[321] Partly influenced by An Essay on the Principle of Population (1798) by Thomas Robert Malthus, Darwin noted that population growth would lead to a \"struggle for existence\" in which favourable variations prevailed as others perished. In each generation, many offspring fail to survive to an age of reproduction because of limited resources. This could explain the diversity of plants and animals from a common ancestry through the working of natural laws in the same way for all types of organism.[322][323][324][325] Darwin developed his theory of \"natural selection\" from 1838 onwards and was writing up his \"big book\" on the subject when Alfred Russel Wallace sent him a version of virtually the same theory in 1858. Their separate papers were presented together at an 1858 meeting of the Linnean Society of London.[326] At the end of 1859, Darwin's publication of his \"abstract\" as On the Origin of Species explained natural selection in detail and in a way that led to an increasingly wide acceptance of Darwin's concepts of evolution at the expense of alternative theories. Thomas Henry Huxley applied Darwin's ideas to humans, using paleontology and comparative anatomy to provide strong evidence that humans and apes shared a common ancestry. Some were disturbed by this since it implied that humans did not have a special place in the universe.[327]\n\nOthniel C. Marsh, America’s first paleontologist, was the first to provide solid fossil evidence to support Darwin’s theory of evolution by unearthing the ancestors of the modern horse.[328]  In 1877, Marsh delivered a very influential speech before the annual meeting of the American Association for the Advancement of Science, providing a demonstrative argument for evolution.  For the first time, Marsh traced the evolution of vertebrates from fish all the way through humans.  Sparing no detail, he listed a wealth of fossil examples of past life forms.  The significance of this speech was immediately recognized by the scientific community, and it was printed in its entirety in several scientific journals.[329][330]\n\nIn 1880, Marsh caught the attention of the scientific world with the publication of Odontornithes: a Monograph on Extinct Birds of North America, which included his discoveries of birds with teeth.  These skeletons helped bridge the gap between dinosaurs and birds, and provided invaluable support for Darwin's theory of evolution.[331]  Darwin wrote to Marsh saying, \"Your work on these old birds & on the many fossil animals of N. America has afforded the best support to the theory of evolution, which has appeared within the last 20 years\" (since Darwin's publication of Origin of Species).[332][333]\n\nThe mechanisms of reproductive heritability and the origin of new traits remained a mystery. Towards this end, Darwin developed his provisional theory of pangenesis.[334] In 1865, Gregor Mendel reported that traits were inherited in a predictable manner through the independent assortment and segregation of elements (later known as genes). Mendel's laws of inheritance eventually supplanted most of Darwin's pangenesis theory.[335] August Weismann made the important distinction between germ cells that give rise to gametes (such as sperm and egg cells) and the somatic cells of the body, demonstrating that heredity passes through the germ line only. Hugo de Vries connected Darwin's pangenesis theory to Weismann's germ/soma cell distinction and proposed that Darwin's pangenes were concentrated in the cell nucleus and when expressed they could move into the cytoplasm to change the cell's structure. De Vries was also one of the researchers who made Mendel's work well known, believing that Mendelian traits corresponded to the transfer of heritable variations along the germline.[336] To explain how new variants originate, de Vries developed a mutation theory that led to a temporary rift between those who accepted Darwinian evolution and biometricians who allied with de Vries.[337][338] In the 1930s, pioneers in the field of population genetics, such as Ronald Fisher, Sewall Wright and J. B. S. Haldane set the foundations of evolution onto a robust statistical philosophy. The false contradiction between Darwin's theory, genetic mutations, and Mendelian inheritance was thus reconciled.[339]\n\nIn the 1920s and 1930s, the modern synthesis connected natural selection and population genetics, based on Mendelian inheritance, into a unified theory that included random genetic drift, mutation, and gene flow. This new version of evolutionary theory focused on changes in allele frequencies in population. It explained patterns observed across species in populations, through fossil transitions in palaeontology.[339]\n\nSince then, further syntheses have extended evolution's explanatory power in the light of numerous discoveries, to cover biological phenomena across the whole of the biological hierarchy from genes to populations.[340]\n\nThe publication of the structure of DNA by James Watson and Francis Crick with contribution of Rosalind Franklin in 1953 demonstrated a physical mechanism for inheritance.[341] Molecular biology improved understanding of the relationship between genotype and phenotype. Advances were also made in phylogenetic systematics, mapping the transition of traits into a comparative and testable framework through the publication and use of evolutionary trees.[342] In 1973, evolutionary biologist Theodosius Dobzhansky penned that \"nothing in biology makes sense except in the light of evolution\", because it has brought to light the relations of what first seemed disjointed facts in natural history into a coherent explanatory body of knowledge that describes and predicts many observable facts about life on this planet.[343]\n\nOne extension, known as evolutionary developmental biology and informally called \"evo-devo\", emphasises how changes between generations (evolution) act on patterns of change within individual organisms (development).[237][344] Since the beginning of the 21st century, some biologists have argued for an extended evolutionary synthesis, which would account for the effects of non-genetic inheritance modes, such as epigenetics, parental effects, ecological inheritance and cultural inheritance, and evolvability.[345][346]\n\nIn the 19th century, particularly after the publication of On the Origin of Species in 1859, the idea that life had evolved was an active source of academic debate centred on the philosophical, social and religious implications of evolution. Today, the modern evolutionary synthesis is accepted by a vast majority of scientists.[237] However, evolution remains a contentious concept for some theists.[348]\n\nWhile various religions and denominations have reconciled their beliefs with evolution through concepts such as theistic evolution, there are creationists who believe that evolution is contradicted by the creation myths found in their religions and who raise various objections to evolution.[135][349][350] As had been demonstrated by responses to the publication of Vestiges of the Natural History of Creation in 1844, the most controversial aspect of evolutionary biology is the implication of human evolution that humans share common ancestry with apes and that the mental and moral faculties of humanity have the same types of natural causes as other inherited traits in animals.[351] In some countries, notably the United States, these tensions between science and religion have fuelled the current creation–evolution controversy, a religious conflict focusing on politics and public education.[352] While other scientific fields such as cosmology[353] and Earth science[354] also conflict with literal interpretations of many religious texts, evolutionary biology experiences significantly more opposition from religious literalists.\n\nThe teaching of evolution in American secondary school biology classes was uncommon in most of the first half of the 20th century. The Scopes Trial decision of 1925 caused the subject to become very rare in American secondary biology textbooks for a generation, but it was gradually re-introduced later and became legally protected with the 1968 Epperson v. Arkansas decision. Since then, the competing religious belief of creationism was legally disallowed in secondary school curricula in various decisions in the 1970s and 1980s, but it returned in pseudoscientific form as intelligent design (ID), to be excluded once again in the 2005 Kitzmiller v. Dover Area School District case.[355] The debate over Darwin's ideas did not generate significant controversy in China.[356]\n\n\n"
    },
    {
        "title": "Human anatomy",
        "url": "https://en.wikipedia.org/wiki/Human_anatomy",
        "content": "\n\nHuman anatomy (gr. ἀνατομία, \"dissection\", from ἀνά, \"up\", and τέμνειν, \"cut\") is primarily the scientific study of the morphology of the human body.[1] Anatomy is subdivided into gross anatomy and microscopic anatomy.[1] Gross anatomy (also called topographical anatomy, regional anatomy, or anthropotomy) is the study of anatomical structures that can be seen by the naked eye.[1] Microscopic anatomy is the study of minute anatomical structures assisted with microscopes, which includes histology (the study of the organization of tissues),[1] and cytology (the study of cells). Anatomy, human physiology (the study of function), and biochemistry (the study of the chemistry of living structures) are complementary basic medical sciences that are generally together (or in tandem) to students studying medical sciences.\n\nIn some of its facets human anatomy is closely related to embryology, comparative anatomy and comparative embryology,[1] through common roots in evolution; for example, much of the human body maintains the ancient segmental pattern that is present in all vertebrates with basic units being repeated, which is particularly obvious in the vertebral column and in the ribcage, and can be traced from very early embryos.\n\nThe human body consists of biological systems, that consist of organs, that consist of tissues, that consist of cells and connective tissue.\n\nThe history of anatomy has been characterized, over a long period of time, by a continually developing understanding of the functions of organs and structures in the body. Methods have also advanced dramatically, advancing from examination of animals through dissection of fresh and preserved cadavers (dead human bodies) to technologically complex techniques developed in the 20th century.\n\nGenerally, physicians, dentists, physiotherapists, nurses, paramedics, radiographers, and students of certain biological sciences, learn gross anatomy and microscopic anatomy from anatomical models, skeletons, textbooks, diagrams, photographs, lectures, and tutorials. The study of microscopic anatomy (or histology) can be aided by practical experience examining histological preparations (or slides) under a microscope; and in addition, medical and dental students generally also learn anatomy with practical experience of dissection and inspection of cadavers (dead human bodies). A thorough working knowledge of anatomy is required for all medical doctors, especially surgeons, and doctors working in some diagnostic specialities, such as histopathology and radiology.\n\nHuman anatomy, physiology, and biochemistry are basic medical sciences, which are generally taught to medical students in their first year at medical school. Human anatomy can be taught regionally or systemically;[1] that is, respectively, studying anatomy by bodily regions such as the head and chest, or studying by specific systems, such as the nervous or respiratory systems. The major anatomy textbook, Gray's Anatomy, has recently been reorganized from a systems format to a regional format, in line with modern teaching.[2][3]\n\nGross anatomy has become a key part of visual arts. Basic concepts of how muscles and bones function and deform with movement is key to drawing, painting or animating a human figure. Many books such as \"Human Anatomy for Artists: The Elements of Form\", are written as a guide to drawing the human body anatomically correctly.[4] Leonardo da Vinci sought to improve his art through a better understanding of human anatomy. In the process he advanced both human anatomy and its representation in art.\n\nBecause the structure of living organism is complex, anatomy is organized by levels, from the smallest components of cells to the largest organs and their relationship to other organs.\n\nHead and neck\n\nThorax\n\nAbdomen and pelvis (both sexes)\n\nMale pelvis\n\nFemale pelvis\n\nSuperficial anatomy or surface anatomy is important in human anatomy being the study of anatomical landmarks that can be readily identified from the contours or other reference points on the surface of the body.[1] With knowledge of superficial anatomy, physicians gauge the position and anatomy of deeper structures.\n\nCommon names of well known parts of the human body, from top to bottom:\n"
    },
    {
        "title": "Climate",
        "url": "https://en.wikipedia.org/wiki/Climate",
        "content": "This is an accepted version of this page\n\n\n\n\nClimate is the long-term weather pattern in a region, typically averaged over 30 years.[1][2] More rigorously, it is the mean and variability of meteorological variables over a time spanning from months to millions of years. Some of the meteorological variables that are commonly measured are temperature, humidity, atmospheric pressure, wind, and precipitation. In a broader sense, climate is the state of the components of the climate system, including the atmosphere, hydrosphere, cryosphere, lithosphere and biosphere and the interactions between them.[1] The climate of a location is affected by its latitude, longitude, terrain, altitude, land use and nearby water bodies and their currents.[3]\n\nClimates can be classified according to the average and typical variables, most commonly temperature and precipitation. The most widely used classification scheme is the Köppen climate classification. The Thornthwaite system,[4] in use since 1948, incorporates evapotranspiration along with temperature and precipitation information and is used in studying biological diversity and how climate change affects it. The major classifications in Thornthwaite's climate classification are microthermal, mesothermal, and megathermal.[5] Finally, the Bergeron and Spatial Synoptic Classification systems focus on the origin of air masses that define the climate of a region.\n\nPaleoclimatology is the study of ancient climates. Paleoclimatologists seek to explain climate variations for all parts of the Earth during any given geologic period, beginning with the time of the Earth's formation.[6] Since very few direct observations of climate were available before the 19th century, paleoclimates are inferred from proxy variables. They include non-biotic evidence—such as sediments found in lake beds and ice cores—and biotic evidence—such as tree rings and coral. Climate models are mathematical models of past, present, and future climates. Climate change may occur over long and short timescales due to various factors. Recent warming is discussed in terms of global warming, which results in redistributions of biota. For example, as climate scientist Lesley Ann Hughes has written: \"a 3 °C [5 °F] change in mean annual temperature corresponds to a shift in isotherms of approximately 300–400 km [190–250 mi] in latitude (in the temperate zone) or 500 m [1,600 ft] in elevation. Therefore, species are expected to move upwards in elevation or towards the poles in latitude in response to shifting climate zones.\"[7][8]\n\nClimate (from Ancient Greek  κλίμα 'inclination') is commonly defined as the weather averaged over a long period.[9] The standard averaging period is 30 years,[10] but other periods may be used depending on the purpose. Climate also includes statistics other than the average, such as the magnitudes of day-to-day or year-to-year variations. The Intergovernmental Panel on Climate Change (IPCC) 2001 glossary definition is as follows:\n\n\"Climate in a narrow sense is usually defined as the \"average weather\", or more rigorously, as the statistical description in terms of the mean and variability of relevant quantities over a period ranging from months to thousands or millions of years. The classical period is 30 years, as defined by the World Meteorological Organization (WMO). These quantities are most often surface variables such as temperature, precipitation, and wind. Climate in a wider sense is the state, including a statistical description, of the climate system.\"[11]\nThe World Meteorological Organization (WMO) describes \"climate normals\" as \"reference points used by climatologists to compare current climatological trends to that of the past or what is considered typical. A climate normal is defined as the arithmetic average of a climate element (e.g. temperature) over a 30-year period. A 30-year period is used as it is long enough to filter out any interannual variation or anomalies such as El Niño–Southern Oscillation, but also short enough to be able to show longer climatic trends.\"[12]\n\nThe WMO originated from the International Meteorological Organization which set up a technical commission for climatology in 1929. At its 1934 Wiesbaden meeting, the technical commission designated the thirty-year period from 1901 to 1930 as the reference time frame for climatological standard normals. In 1982, the WMO agreed to update climate normals, and these were subsequently completed on the basis of climate data from 1 January 1961 to 31 December 1990.[13] The 1961–1990 climate normals serve as the baseline reference period. The next set of climate normals to be published by WMO is from 1991 to 2010.[14] Aside from collecting from the most common atmospheric variables (air temperature, pressure, precipitation and wind), other variables such as humidity, visibility, cloud amount, solar radiation, soil temperature, pan evaporation rate, days with thunder and days with hail are also collected to measure change in climate conditions.[15]\n\nThe difference between climate and weather is usefully summarized by the popular phrase \"Climate is what you expect, weather is what you get.\"[16] Over historical time spans, there are a number of nearly constant variables that determine climate, including latitude, altitude, proportion of land to water, and proximity to oceans and mountains. All of these variables change only over periods of millions of years due to processes such as plate tectonics. Other climate determinants are more dynamic: the thermohaline circulation of the ocean leads to a 5 °C (9 °F) warming of the northern Atlantic Ocean compared to other ocean basins.[17] Other ocean currents redistribute heat between land and water on a more regional scale. The density and type of vegetation coverage affects solar heat absorption,[18] water retention, and rainfall on a regional level. Alterations in the quantity of atmospheric greenhouse gases (particularly carbon dioxide and methane) determines the amount of solar energy retained by the planet, leading to global warming or global cooling. The variables which determine climate are numerous and the interactions complex, but there is general agreement that the broad outlines are understood, at least insofar as the determinants of historical climate change are concerned.[19][20]\n\nClimate classifications are systems that categorize the world's climates. A climate classification may correlate closely with a biome classification, as climate is a major influence on life in a region. One of the most used is the Köppen climate classification scheme first developed in 1899.[21]\n\nThere are several ways to classify climates into similar regimes. Originally, climes were defined in Ancient Greece to describe the weather depending upon a location's latitude. Modern climate classification methods can be broadly divided into genetic methods, which focus on the causes of climate, and empiric methods, which focus on the effects of climate. Examples of genetic classification include methods based on the relative frequency of different air mass types or locations within synoptic weather disturbances. Examples of empiric classifications include climate zones defined by plant hardiness,[22] evapotranspiration,[23] or more generally the Köppen climate classification which was originally designed to identify the climates associated with certain biomes. A common shortcoming of these classification schemes is that they produce distinct boundaries between the zones they define, rather than the gradual transition of climate properties more common in nature.\n\nPaleoclimatology is the study of past climate over a great period of the Earth's history. It uses evidence with different time scales (from decades to millennia) from ice sheets, tree rings, sediments, pollen, coral, and rocks to determine the past state of the climate. It demonstrates periods of stability and periods of change and can indicate whether changes follow patterns such as regular cycles.[24]\n\nDetails of the modern climate record are known through the taking of measurements from such weather instruments as thermometers, barometers, and anemometers during the past few centuries. The instruments used to study weather over the modern time scale, their observation frequency, their known error, their immediate environment, and their exposure have changed over the years, which must be considered when studying the climate of centuries past.[25] Long-term modern climate records skew towards population centres and affluent countries.[26] Since the 1960s, the launch of satellites allow records to be gathered on a global scale, including areas with little to no human presence, such as the Arctic region and oceans.\n\nClimate variability is the term to describe variations in the mean state and other characteristics of climate (such as chances or possibility of extreme weather, etc.) \"on all spatial and temporal scales beyond that of individual weather events.\"[27] Some of the variability does not appear to be caused systematically and occurs at random times. Such variability is called random variability or noise. On the other hand, periodic variability occurs relatively regularly and in distinct modes of variability or climate patterns.[28]\n\nThere are close correlations between Earth's climate oscillations and astronomical factors (barycenter changes, solar variation, cosmic ray flux, cloud albedo feedback, Milankovic cycles), and modes of heat distribution between the ocean-atmosphere climate system. In some cases, current, historical and paleoclimatological natural oscillations may be masked by significant volcanic eruptions, impact events, irregularities in climate proxy data, positive feedback processes or anthropogenic emissions of substances such as greenhouse gases.[29]\n\nOver the years, the definitions of climate variability and the related term climate change have shifted. While the term climate change now implies change that is both long-term and of human causation, in the 1960s the word climate change was used for what we now describe as climate variability, that is, climatic inconsistencies and anomalies.[28]\n\nClimate change is the variation in global or regional climates over time.[34] It reflects changes in the variability or average state of the atmosphere over time scales ranging from decades to millions of years. These changes can be caused by processes internal to the Earth, external forces (e.g. variations in sunlight intensity) or human activities, as found recently.[35][36]  Scientists have identified Earth's Energy Imbalance (EEI) to be a fundamental metric of the status of global change.[37]\n\nIn recent usage, especially in the context of environmental policy, the term \"climate change\" often refers only to changes in modern climate, including the rise in average surface temperature known as global warming. In some cases, the term is also used with a presumption of human causation, as in the United Nations Framework Convention on Climate Change (UNFCCC). The UNFCCC uses \"climate variability\" for non-human caused variations.[38]\n\nEarth has undergone periodic climate shifts in the past, including four major ice ages. These consist of glacial periods where conditions are colder than normal, separated by interglacial periods. The accumulation of snow and ice during a glacial period increases the surface albedo, reflecting more of the Sun's energy into space and maintaining a lower atmospheric temperature. Increases in greenhouse gases, such as by volcanic activity, can increase the global temperature and produce an interglacial period. Suggested causes of ice age periods include the positions of the continents, variations in the Earth's orbit, changes in the solar output, and volcanism.[39] However, these naturally caused changes in climate occur on a much slower time scale than the present rate of change which is caused by the emission of greenhouse gases by human activities.[40]\n\nAccording to the EU's Copernicus Climate Change Service, average global air temperature has passed 1.5C of warming the period from February 2023 to January 2024.[41]\n\nClimate models use quantitative methods to simulate the interactions and transfer of radiative energy between the atmosphere,[42] oceans, land surface and ice through a series of physics equations. They are used for a variety of purposes, from the study of the dynamics of the weather and climate system to projections of future climate. All climate models balance, or very nearly balance, incoming energy as short wave (including visible) electromagnetic radiation to the Earth with outgoing energy as long wave (infrared) electromagnetic radiation from the Earth. Any imbalance results in a change in the average temperature of the Earth.\n\nClimate models are available on different resolutions ranging from >100 km to 1 km. High resolutions in global climate models require significant computational resources, and so only a few global datasets exist. Global climate models can be dynamically or statistically downscaled to regional climate models to analyze impacts of climate change on a local scale. Examples are ICON[43] or mechanistically downscaled data such as CHELSA (Climatologies at high resolution for the earth's land surface areas).[44][45]\n\nThe most talked-about applications of these models in recent years have been their use to infer the consequences of increasing greenhouse gases in the atmosphere, primarily carbon dioxide (see greenhouse gas). These models predict an upward trend in the global mean surface temperature, with the most rapid increase in temperature being projected for the higher latitudes of the Northern Hemisphere.\n\nModels can range from relatively simple to quite complex. Simple radiant heat transfer models treat the Earth as a single point and average outgoing energy. This can be expanded vertically (as in radiative-convective models), or horizontally. Finally, more complex (coupled) atmosphere–ocean–sea ice global climate models discretise and solve the full equations for mass and energy transfer and radiant exchange.[46]\n"
    },
    {
        "title": "Biodiversity",
        "url": "https://en.wikipedia.org/wiki/Biodiversity",
        "content": "\n\nBiodiversity is the variability of life on Earth. It can be measured on various levels. There is for example genetic variability, species diversity, ecosystem diversity and phylogenetic diversity.[1] Diversity is not distributed evenly on Earth. It is greater in the tropics as a result of the warm climate and high primary productivity in the region near the equator. Tropical forest ecosystems cover less than one-fifth of Earth's terrestrial area and contain about 50% of the world's species.[2] There are latitudinal gradients in species diversity for both marine and terrestrial taxa.[3]\n\nSince life began on Earth, six  major mass extinctions and several minor events have led to large and sudden drops in biodiversity. The Phanerozoic aeon (the last 540 million years) marked a rapid growth in biodiversity via the Cambrian explosion. In this period, the majority of multicellular phyla first appeared. The next 400 million years included repeated, massive biodiversity losses. Those events have been classified as mass extinction events. In the Carboniferous, rainforest collapse may have led to a great loss of plant and animal life. The Permian–Triassic extinction event, 251 million years ago, was the worst; vertebrate recovery took 30 million years. \n\nHuman activities have led to an ongoing biodiversity loss and an accompanying loss of genetic diversity. This process is often referred to as Holocene extinction, or sixth mass extinction. For example, it was estimated in 2007 that up to 30% of all species will be extinct by 2050.[4] Destroying habitats for farming is a key reason why biodiversity is decreasing today. Climate change also plays a role.[5][6] This can be seen for example in the effects of climate change on biomes. This anthropogenic extinction may have started toward the end of the Pleistocene, as some studies suggest that the megafaunal extinction event that took place around the end of the last ice age partly resulted from overhunting.[7]\n\nBiologists most often define biodiversity as the \"totality of genes, species and ecosystems of a region\".[8][9] An advantage of this definition is that it presents a unified view of the traditional types of biological variety previously identified:\n\nBiodiversity is most commonly used to replace the more clearly-defined and long-established terms, species diversity and species richness.[13] However, there is no concrete definition for biodiversity, as its definition continues to be defined. Other definitions include (in chronological order):\n\nAccording to estimates by Mora et al. (2011), there are approximately 8.7 million terrestrial species and 2.2 million oceanic species. The authors note that these estimates are strongest for eukaryotic organisms and likely represent the lower bound of prokaryote diversity.[18] Other estimates include:\n\nSince the rate of extinction has increased, many extant species may become extinct before they are described.[30] Not surprisingly, in the animalia the most studied groups are birds and mammals, whereas fishes and arthropods are the least studied animals groups.[31]\n\nDuring the last century, decreases in biodiversity have been increasingly observed. It was estimated in 2007 that up to 30% of all species will be extinct by 2050.[4] Of these, about one eighth of known plant species are threatened with extinction.[35] Estimates reach as high as 140,000 species per year (based on Species-area theory).[36] This figure indicates unsustainable ecological practices, because few species emerge each year.[37] The rate of species loss is greater now than at any time in human history, with extinctions occurring at rates hundreds of times higher than background extinction rates.[35][38][39] and expected to still grow in the upcoming years.[39][40][41] As of 2012, some studies suggest that 25% of all mammal species could be extinct in 20 years.[42]\n\nIn absolute terms, the planet has lost 58% of its biodiversity since 1970 according to a 2016 study by the World Wildlife Fund.[43] The Living Planet Report 2014 claims that \"the number of mammals, birds, reptiles, amphibians, and fish across the globe is, on average, about half the size it was 40 years ago\". Of that number, 39% accounts for the terrestrial wildlife gone, 39% for the marine wildlife gone and 76% for the freshwater wildlife gone. Biodiversity took the biggest hit in Latin America, plummeting 83 percent. High-income countries showed a 10% increase in biodiversity, which was canceled out by a loss in low-income countries. This is despite the fact that high-income countries use five times the ecological resources of low-income countries, which was explained as a result of a process whereby wealthy nations are outsourcing resource depletion to poorer nations, which are suffering the greatest ecosystem losses.[44]\n\nA 2017 study published in PLOS One found that the biomass of insect life in Germany had declined by three-quarters in the last 25 years.[45] Dave Goulson of Sussex University stated that their study suggested that humans \"appear to be making vast tracts of land inhospitable to most forms of life, and are currently on course for ecological Armageddon. If we lose the insects then everything is going to collapse.\"[46]\n\nIn 2020 the World Wildlife Foundation published a report saying that \"biodiversity is being destroyed at a rate unprecedented in human history\". The report claims that 68% of the population of the examined species were destroyed in the years 1970 – 2016.[47]\n\nOf 70,000 monitored species, around 48% are experiencing population declines from human activity (in 2023), whereas only 3% have increasing populations.[48][49][50]\n\nRates of decline in biodiversity in the current sixth mass extinction match or exceed rates of loss in the five previous mass extinction events in the fossil record.[60] Biodiversity loss is in fact \"one of the most critical manifestations of the Anthropocene\" (since around the 1950s); the continued decline of biodiversity constitutes \"an unprecedented threat\" to the continued existence of human civilization.[61] The reduction is caused primarily by human impacts, particularly habitat destruction.\n\nSince the Stone Age, species loss has accelerated above the average basal rate, driven by human activity. Estimates of species losses are at a rate 100–10,000 times as fast as is typical in the fossil record.[62]\n\nLoss of biodiversity results in the loss of natural capital that supplies ecosystem goods and services. Species today are being wiped out at a rate 100 to 1,000 times higher than baseline, and the rate of extinctions is increasing. This process destroys the resilience and adaptability of life on Earth.[63]\n\nIn 2006, many species were formally classified as rare or endangered or threatened; moreover, scientists have estimated that millions more species are at risk which have not been formally recognized. About 40 percent of the 40,177 species assessed using the IUCN Red List criteria are now listed as threatened with extinction—a total of 16,119.[64] As of late 2022 9251 species were considered part of the IUCN's critically endangered.[65]\n\nNumerous scientists and the IPBES Global Assessment Report on Biodiversity and Ecosystem Services assert that human population growth and overconsumption are the primary factors in this decline.[66][67][68][69][70] However, other scientists have criticized this finding and say that loss of habitat caused by \"the growth of commodities for export\" is the main driver.[71]\n\nSome studies have however pointed out that habitat destruction for the expansion of agriculture and the overexploitation of wildlife are the more significant drivers of contemporary biodiversity loss, not climate change.[5][6]\n\nBiodiversity is not evenly distributed, rather it varies greatly across the globe as well as within regions and seasons. Among other factors, the diversity of all living things (biota) depends on temperature, precipitation, altitude, soils, geography and the interactions between other species.[72] The study of the spatial distribution of organisms, species and ecosystems, is the science of biogeography.[73][74]\n\nDiversity consistently measures higher in the tropics and in other localized regions such as the Cape Floristic Region and lower in polar regions generally. Rain forests that have had wet climates for a long time, such as Yasuní National Park in Ecuador, have particularly high biodiversity.[75][76]\n\nThere is local biodiversity, which directly impacts daily life, affecting the availability of fresh water, food choices, and fuel sources for humans. Regional biodiversity includes habitats and ecosystems that synergizes and either overlaps or differs on a regional scale. National biodiversity within a country determines the ability for a country to thrive according to its habitats and ecosystems on a national scale. Also, within a country, endangered species are initially supported on a national level then internationally. Ecotourism may be utilized to support the economy and encourages tourists to continue to visit and support species and ecosystems they visit, while they enjoy the available amenities provided. International biodiversity impacts global livelihood, food systems, and health. Problematic pollution, over consumption, and climate change can devastate international biodiversity. Nature-based solutions are a critical tool for a global resolution. Many species are in danger of becoming extinct and need world leaders to be proactive with the Kunming-Montreal Global Biodiversity Framework. \n\nTerrestrial biodiversity is thought to be up to 25 times greater than ocean biodiversity.[77] Forests harbour most of Earth's terrestrial biodiversity. The conservation of the world's biodiversity is thus utterly dependent on the way in which we interact with and use the world's forests.[78] A new method used in 2011, put the total number of species on Earth at 8.7 million, of which 2.1 million were estimated to live in the ocean.[79] However, this estimate seems to under-represent the diversity of microorganisms.[80] Forests provide habitats for 80 percent of amphibian species, 75 percent of bird species and 68 percent of mammal species. About 60 percent of all vascular plants are found in tropical forests. Mangroves provide breeding grounds and nurseries for numerous species of fish and shellfish and help trap sediments that might otherwise adversely affect seagrass beds and coral reefs, which are habitats for many more marine species.[78] Forests span around 4 billion acres (nearly a third of the Earth's land mass) and are home to approximately 80% of the world's biodiversity. About 1 billion hectares are covered by primary forests. Over 700 million hectares of the world's woods are officially protected.[81][82]\n\nThe biodiversity of forests varies considerably according to factors such as forest type, geography, climate and soils – in addition to human use.[78] Most forest habitats in temperate regions support relatively few animal and plant species and species that tend to have large geographical distributions, while the montane forests of Africa, South America and Southeast Asia and lowland forests of Australia, coastal Brazil, the Caribbean islands, Central America and insular Southeast Asia have many species with small geographical distributions.[78] Areas with dense human populations and intense agricultural land use, such as Europe, parts of Bangladesh, China, India and North America, are less intact in terms of their biodiversity. Northern Africa, southern Australia, coastal Brazil, Madagascar and South Africa, are also identified as areas with striking losses in biodiversity intactness.[78] European forests in EU and non-EU nations comprise more than 30% of Europe's land mass (around 227 million hectares), representing an almost 10% growth since 1990.[83][84]\n\nGenerally, there is an increase in biodiversity from the poles to the tropics. Thus localities at lower latitudes have more species than localities at higher latitudes. This is often referred to as the latitudinal gradient in species diversity. Several ecological factors may contribute to the gradient, but the ultimate factor behind many of them is the greater mean temperature at the equator compared to that at the poles.[85]\n\nEven though terrestrial biodiversity declines from the equator to the poles,[86] some studies claim that this characteristic is unverified in aquatic ecosystems, especially in marine ecosystems.[87] The latitudinal distribution of parasites does not appear to follow this rule.[73] Also, in terrestrial ecosystems the soil bacterial diversity has been shown to be highest in temperate climatic zones,[88] and has been attributed to carbon inputs and habitat connectivity.[89]\n\nIn 2016, an alternative hypothesis (\"the fractal biodiversity\") was proposed to explain the biodiversity latitudinal gradient.[90] In this study, the species pool size and the fractal nature of ecosystems were combined to clarify some general patterns of this gradient. This hypothesis considers temperature, moisture, and net primary production (NPP) as the main variables of an ecosystem niche and as the axis of the ecological hypervolume. In this way, it is possible to build fractal hyper volumes, whose fractal dimension rises to three moving towards the equator.[91]\n\nA biodiversity hotspot is a region with a high level of endemic species that have experienced great habitat loss.[92] The term hotspot was introduced in 1988 by Norman Myers.[93][94][95][96] While hotspots are spread all over the world, the majority are forest areas and most are located in the tropics.[97]\n\nBrazil's Atlantic Forest is considered one such hotspot, containing roughly 20,000 plant species, 1,350 vertebrates and millions of insects, about half of which occur nowhere else.[98][99] The island of Madagascar and India are also particularly notable. Colombia is characterized by high biodiversity, with the highest rate of species by area unit worldwide and it has the largest number of endemics (species that are not found naturally anywhere else) of any country. About 10% of the species of the Earth can be found in Colombia, including over 1,900 species of bird, more than in Europe and North America combined, Colombia has 10% of the world's mammals species, 14% of the amphibian species and 18% of the bird species of the world.[100] Madagascar dry deciduous forests and lowland rainforests possess a high ratio of endemism.[101][102] Since the island separated from mainland Africa 66 million years ago, many species and ecosystems have evolved independently.[103] Indonesia's 17,000 islands cover 735,355 square miles (1,904,560 km2) and contain 10% of the world's flowering plants, 12% of mammals and 17% of reptiles, amphibians and birds—along with nearly 240 million people.[104] Many regions of high biodiversity and/or endemism arise from specialized habitats which require unusual adaptations, for example, alpine environments in high mountains, or Northern European peat bogs.[102]\n\nAccurately measuring differences in biodiversity can be difficult. Selection bias amongst researchers may contribute to biased empirical research for modern estimates of biodiversity. In 1768, Rev. Gilbert White succinctly observed of his Selborne, Hampshire \"all nature is so full, that that district produces the most variety which is the most examined.\"[105]\n\nBiodiversity is the result of 3.5 billion years of evolution.[106] The origin of life has not been established by science, however, some evidence suggests that life may already have been well-established only a few hundred million years after the formation of the Earth. Until approximately 2.5 billion years ago, all life consisted of microorganisms – archaea, bacteria, and single-celled protozoans and protists.[80]\n\nBiodiversity grew fast during the Phanerozoic (the last 540 million years), especially during the so-called Cambrian explosion—a period during which nearly every phylum of multicellular organisms first appeared.[108] However, recent studies suggest that this diversification had started earlier, at least in the Ediacaran, and that it continued in the Ordovician.[109] Over the next 400 million years or so, invertebrate diversity showed little overall trend and vertebrate diversity shows an overall exponential trend.[10] This dramatic rise in diversity was marked by periodic, massive losses of diversity classified as mass extinction events.[10] A significant loss occurred in anamniotic limbed vertebrates when rainforests collapsed in the Carboniferous,[110] but amniotes seem to have been little affected by this event; their diversification slowed down later, around the Asselian/Sakmarian boundary, in the early Cisuralian (Early Permian), about 293 Ma ago.[111] The worst was the Permian-Triassic extinction event, 251 million years ago.[112][113] Vertebrates took 30 million years to recover from this event.[114]\n\nThe most recent major mass extinction event, the Cretaceous–Paleogene extinction event, occurred 66 million years ago. This period has attracted more attention than others because it resulted in the extinction of the non-avian dinosaurs, which were represented by many lineages at the end of the Maastrichtian, just before that extinction event. However, many other taxa were affected by this crisis, which affected even marine taxa, such as ammonites, which also became extinct around that time.[115]\n\nThe biodiversity of the past is called Paleobiodiversity. The fossil record suggests that the last few million years featured the greatest biodiversity in history.[10] However, not all scientists support this view, since there is uncertainty as to how strongly the fossil record is biased by the greater availability and preservation of recent geologic sections.[116] Some scientists believe that corrected for sampling artifacts, modern biodiversity may not be much different from biodiversity 300 million years ago,[108] whereas others consider the fossil record reasonably reflective of the diversification of life.[117][10] Estimates of the present global macroscopic species diversity vary from 2 million to 100 million, with a best estimate of somewhere near 9 million,[79] the vast majority arthropods.[118] Diversity appears to increase continually in the absence of natural selection.[119]\n\nThe existence of a global carrying capacity, limiting the amount of life that can live at once, is debated, as is the question of whether such a limit would also cap the number of species. While records of life in the sea show a logistic pattern of growth, life on land (insects, plants and tetrapods) shows an exponential rise in diversity.[10] As one author states, \"Tetrapods have not yet invaded 64 percent of potentially habitable modes and it could be that without human influence the ecological and taxonomic diversity of tetrapods would continue to increase exponentially until most or all of the available eco-space is filled.\"[10]\n\nIt also appears that the diversity continues to increase over time, especially after mass extinctions.[120]\n\nOn the other hand, changes through the Phanerozoic correlate much better with the hyperbolic model (widely used in population biology, demography and macrosociology, as well as fossil biodiversity) than with exponential and logistic models. The latter models imply that changes in diversity are guided by a first-order positive feedback (more ancestors, more descendants) and/or a negative feedback arising from resource limitation. Hyperbolic model implies a second-order positive feedback.[121] Differences in the strength of the second-order feedback due to different intensities of interspecific competition might explain the faster rediversification of ammonoids in comparison to bivalves after the end-Permian extinction.[121] The hyperbolic pattern of the world population growth arises from a second-order positive feedback between the population size and the rate of technological growth.[122] The hyperbolic character of biodiversity growth can be similarly accounted for by a feedback between diversity and community structure complexity.[122][123] The similarity between the curves of biodiversity and human population probably comes from the fact that both are derived from the interference of the hyperbolic trend with cyclical and stochastic dynamics.[122][123]\n\nMost biologists agree however that the period since human emergence is part of a new mass extinction, named the Holocene extinction event, caused primarily by the impact humans are having on the environment.[124] It has been argued that the present rate of extinction is sufficient to eliminate most species on the planet Earth within 100 years.[125]\n\nNew species are regularly discovered (on average between 5–10,000 new species each year, most of them insects) and many, though discovered, are not yet classified (estimates are that nearly 90% of all arthropods are not yet classified).[118] Most of the terrestrial diversity is found in tropical forests and in general, the land has more species than the ocean; some 8.7 million species may exist on Earth, of which some 2.1 million live in the ocean.[79]\n\nIt is estimated that 5 to 50 billion species have existed on the planet.[126] Assuming that there may be a maximum of about 50 million species currently alive,[127] it stands to reason that greater than 99% of the planet's species went extinct prior to the evolution of humans.[128] Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.2 million have been documented and over 86% have not yet been described.[129] However, a May 2016 scientific report estimates that 1 trillion species are currently on Earth, with only one-thousandth of one percent described.[130] The total amount of related DNA base pairs on Earth is estimated at 5.0 x 1037 and weighs 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as four trillion tons of carbon.[131] In July 2016, scientists reported identifying a set of 355 genes from the last universal common ancestor (LUCA) of all organisms living on Earth.[132]\n\nThe age of Earth is about 4.54 billion years.[133][134][135] The earliest undisputed evidence of life dates at least from 3.7 billion years ago, during the Eoarchean era after a geological crust started to solidify following the earlier molten Hadean eon.[136][137][138] There are microbial mat fossils found in 3.48 billion-year-old sandstone discovered in Western Australia. Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old meta-sedimentary rocks discovered in Western Greenland..[139][140] More recently, in 2015, \"remains of biotic life\" were found in 4.1 billion-year-old rocks in Western Australia. According to one of the researchers, \"If life arose relatively quickly on Earth...then it could be common in the universe.\"[141]\n\nThere have been many claims about biodiversity's effect on the ecosystem services, especially provisioning and regulating services.[142] Some of those claims have been validated, some are incorrect and some lack enough evidence to draw definitive conclusions.[142]\n\nEcosystem services have been grouped in three types:[142]\n\nExperiments with controlled environments have shown that humans cannot easily build ecosystems to support human needs;[144] for example insect pollination cannot be mimicked, though there have been attempts to create artificial pollinators using unmanned aerial vehicles.[145] The economic activity of pollination alone represented between $2.1–14.6 billion in 2003.[146] Other sources have reported somewhat conflicting results and in 1997 Robert Costanza and his colleagues reported the estimated global value of ecosystem services (not captured in traditional markets) at an average of $33 trillion annually.[147]\n\nWith regards to provisioning services, greater species diversity has the following benefits:\n\nWith regards to regulating services, greater species diversity has the following benefits:\n\nGreater species diversity\n\nAgricultural diversity can be divided into two categories: intraspecific diversity, which includes the genetic variation within a single species, like the potato (Solanum tuberosum) that is composed of many different forms and types (e.g. in the U.S. they might compare russet potatoes with new potatoes or purple potatoes, all different, but all part of the same species, S. tuberosum). The other category of agricultural diversity is called interspecific diversity and refers to the number and types of different species.\n\nAgricultural diversity can also be divided by whether it is 'planned' diversity or 'associated' diversity. This is a functional classification that we impose and not an intrinsic feature of life or diversity. Planned diversity includes the crops which a farmer has encouraged, planted or raised (e.g. crops, covers, symbionts, and livestock, among others), which can be contrasted with the associated diversity that arrives among the crops, uninvited (e.g. herbivores, weed species and pathogens, among others).[156]\n\nAssociated biodiversity can be damaging or beneficial. The beneficial associated biodiversity include for instance wild pollinators such as wild bees and syrphid flies that pollinate crops[157] and natural enemies and antagonists to pests and pathogens. Beneficial associated biodiversity occurs abundantly in crop fields and provide multiple ecosystem services such as pest control, nutrient cycling and pollination that support crop production.[158]\n\nAlthough about 80 percent of humans' food supply comes from just 20 kinds of plants,[159] humans use at least 40,000 species.[160] Earth's surviving biodiversity provides resources for increasing the range of food and other products suitable for human use, although the present extinction rate shrinks that potential.[125]\n\nBiodiversity's relevance to human health is becoming an international political issue, as scientific evidence builds on the global health implications of biodiversity loss.[161][162][163] This issue is closely linked with the issue of climate change,[164] as many of the anticipated health risks of climate change are associated with changes in biodiversity (e.g. changes in populations and distribution of disease vectors, scarcity of fresh water, impacts on agricultural biodiversity and food resources etc.). This is because the species most likely to disappear are those that buffer against infectious disease transmission, while surviving species tend to be the ones that increase disease transmission, such as that of West Nile Virus, Lyme disease and Hantavirus, according to a study done co-authored by Felicia Keesing, an ecologist at Bard College and Drew Harvell, associate director for Environment of the Atkinson Center for a Sustainable Future (ACSF) at Cornell University.[165]\n\nSome of the health issues influenced by biodiversity include dietary health and nutrition security, infectious disease, medical science and medicinal resources, social and psychological health.[166] Biodiversity is also known to have an important role in reducing disaster risk and in post-disaster relief and recovery efforts.[167][168]\n\nBiodiversity provides critical support for drug discovery and the availability of medicinal resources.[169][170] A significant proportion of drugs are derived, directly or indirectly, from biological sources: at least 50% of the pharmaceutical compounds on the US market are derived from plants, animals and microorganisms, while about 80% of the world population depends on medicines from nature (used in either modern or traditional medical practice) for primary healthcare.[162] Only a tiny fraction of wild species has been investigated for medical potential.\n\nMarine ecosystems are particularly important,[171] although inappropriate bioprospecting can increase biodiversity loss, as well as violating the laws of the communities and states from which the resources are taken.[172][173][174]\n\nMany industrial materials derive directly from biological sources. These include building materials, fibers, dyes, rubber, and oil. Biodiversity is also important to the security of resources such as water, timber, paper, fiber, and food.[175][176][177] As a result, biodiversity loss is a significant risk factor in business development and a threat to long-term economic sustainability.[178][179]\n\nPhilosophically it could be argued that biodiversity has intrinsic aesthetic and spiritual value to mankind in and of itself. This idea can be used as a counterweight to the notion that tropical forests and other ecological realms are only worthy of conservation because of the services they provide.[180]\n\nBiodiversity also affords many non-material benefits including spiritual and aesthetic values, knowledge systems and education.[62]\n\nLess than 1% of all species that have been described have been studied beyond noting their existence.[187] The vast majority of Earth's species are microbial. Contemporary biodiversity physics is \"firmly fixated on the visible [macroscopic] world\".[188] For example, microbial life is metabolically and environmentally more diverse than multicellular life (see e.g., extremophile). \"On the tree of life, based on analyses of small-subunit ribosomal RNA, visible life consists of barely noticeable twigs. The inverse relationship of size and population recurs higher on the evolutionary ladder—to a first approximation, all multicellular species on Earth are insects\".[189] Insect extinction rates are high—supporting the Holocene extinction hypothesis.[190][58]\n\nBiodiversity naturally varies due to seasonal shifts. Spring's arrival enhances biodiversity as numerous species breed and feed, while winter's onset temporarily reduces it as some insects perish and migrating animals leave. Additionally, the seasonal fluctuation in plant and invertebrate populations influences biodiversity.[191]\n\nBarriers such as large rivers, seas, oceans, mountains and deserts encourage diversity by enabling independent evolution on either side of the barrier, via the process of allopatric speciation. The term invasive species is applied to species that breach the natural barriers that would normally keep them constrained. Without barriers, such species occupy new territory, often supplanting native species by occupying their niches, or by using resources that would normally sustain native species.\n\nSpecies are increasingly being moved by humans (on purpose and accidentally). Some studies say that diverse ecosystems are more resilient and resist invasive plants and animals.[192] Many studies cite effects of invasive species on natives,[193] but not extinctions. \n\nInvasive species seem to increase local (alpha diversity) diversity, which decreases turnover of diversity (beta diversity). Overall gamma diversity may be lowered because species are going extinct because of other causes,[194] but even some of the most insidious invaders (e.g.: Dutch elm disease, emerald ash borer, chestnut blight in North America) have not caused their host species to become extinct. Extirpation, population decline and homogenization of regional biodiversity are much more common. Human activities have frequently been the cause of invasive species circumventing their barriers,[195] by introducing them for food and other purposes. Human activities therefore allow species to migrate to new areas (and thus become invasive) occurred on time scales much shorter than historically have been required for a species to extend its range.\n\nAt present, several countries have already imported so many exotic species, particularly agricultural and ornamental plants, that their indigenous fauna/flora may be outnumbered. For example, the introduction of kudzu from Southeast Asia to Canada and the United States has threatened biodiversity in certain areas.[196] Another example are pines, which have invaded forests, shrublands and grasslands in the southern hemisphere.[197]\n\nEndemic species can be threatened with extinction[198] through the process of genetic pollution, i.e. uncontrolled hybridization, introgression and genetic swamping. Genetic pollution leads to homogenization or replacement of local genomes as a result of either a numerical and/or fitness advantage of an introduced species.[199]\n\nHybridization and introgression are side-effects of introduction and invasion. These phenomena can be especially detrimental to rare species that come into contact with more abundant ones. The abundant species can interbreed with the rare species, swamping its gene pool. This problem is not always apparent from morphological (outward appearance) observations alone. Some degree of gene flow is normal adaptation and not all gene and genotype constellations can be preserved. However, hybridization with or without introgression may, nevertheless, threaten a rare species' existence.[200][201]\n\nConservation biology matured in the mid-20th century as ecologists, naturalists and other scientists began to research and address issues pertaining to global biodiversity declines.[203][204][205]\n\nThe conservation ethic advocates management of natural resources for the purpose of sustaining biodiversity in species, ecosystems, the evolutionary process and human culture and society.[54][203][205][206][207]\n\nConservation biology is reforming around strategic plans to protect biodiversity.[203][208][209][210] Preserving global biodiversity is a priority in strategic conservation plans that are designed to engage public policy and concerns affecting local, regional and global scales of communities, ecosystems and cultures.[211] Action plans identify ways of sustaining human well-being, employing natural capital, macroeconomic policies including economic incentives, and ecosystem services.[212][213]\n\nIn the EU Directive 1999/22/EC zoos are described as having a role in the preservation of the biodiversity of wildlife animals by conducting research or participation in breeding programs.[214]\n\nRemoval of exotic species will allow the species that they have negatively impacted to recover their ecological niches. Exotic species that have become pests can be identified taxonomically (e.g., with Digital Automated Identification SYstem (DAISY), using the barcode of life).[215][216] Removal is practical only given large groups of individuals due to the economic cost.\n\nAs sustainable populations of the remaining native species in an area become assured, \"missing\" species that are candidates for reintroduction can be identified using databases such as the Encyclopedia of Life and the Global Biodiversity Information Facility.\n\nProtected areas, including forest reserves and biosphere reserves, serve many functions including for affording protection to wild animals and their habitat.[219] Protected areas have been set up all over the world with the specific aim of protecting and conserving plants and animals. Some scientists have called on the global community to designate as protected areas of 30 percent of the planet by 2030, and 50 percent by 2050, in order to mitigate biodiversity loss from anthropogenic causes.[220][221] The target of protecting 30% of the area of the planet by the year 2030 (30 by 30) was adopted by almost 200 countries in the 2022 United Nations Biodiversity Conference. At the moment of adoption (December 2022) 17% of land territory and 10% of ocean territory were protected.[222] In a study published 4 September 2020 in Science Advances researchers mapped out regions that can help meet critical conservation and climate goals.[223]\n\nProtected areas safeguard nature and cultural resources and contribute to livelihoods, particularly at local level. There are over 238 563 designated protected areas worldwide, equivalent to 14.9 percent of the earth's land surface, varying in their extension, level of protection, and type of management (IUCN, 2018).[224]\n\nThe benefits of protected areas extend beyond their immediate environment and time. In addition to conserving nature, protected areas are crucial for securing the long-term delivery of ecosystem services. They provide numerous benefits including the conservation of genetic resources for food and agriculture, the provision of medicine and health benefits, the provision of water, recreation and tourism, and for acting as a buffer against disaster. Increasingly, there is acknowledgement of the wider socioeconomic values of these natural ecosystems and of the ecosystem services they can provide.[225]\n\nA national park is a large natural or near natural area set aside to protect large-scale ecological processes, which also provide a foundation for environmentally and culturally compatible, spiritual, scientific, educational, recreational and visitor opportunities. These areas are selected by governments or private organizations to protect natural biodiversity along with its underlying ecological structure and supporting environmental processes, and to promote education and recreation. The International Union for Conservation of Nature (IUCN), and its World Commission on Protected Areas (WCPA), has defined \"National Park\" as its Category II type of protected areas.[226] Wildlife sanctuaries aim only at the conservation of species \n\nForest protected areas are a subset of all protected areas in which a significant portion of the area is forest.[78] This may be the whole or only a part of the protected area.[78] Globally, 18 percent of the world's forest area, or more than 700 million hectares, fall within legally established protected areas such as national parks, conservation areas and game reserves.[78]\n\nThere is an estimated 726 million ha of forest in protected areas worldwide. Of the six major world regions, South America has the highest share of forests in protected areas, 31 percent.[227] The forests play a vital role in harboring more than 45,000 floral and 81,000 faunal species of which 5150 floral and 1837 faunal species are endemic.[228] In addition, there are 60,065 different tree species in the world.[229] Plant and animal species confined to a specific geographical area are called endemic species. \n\nIn forest reserves, rights to activities like hunting and grazing are sometimes given to communities living on the fringes of the forest, who sustain their livelihood partially or wholly from forest resources or products. \n\nApproximately 50 million hectares (or 24%) of European forest land is protected for biodiversity and landscape protection. Forests allocated for soil, water, and other ecosystem services encompass around 72 million hectares (32% of European forest area).[230][231]\n\nIn 2019, a summary for policymakers of the largest, most comprehensive study to date of biodiversity and ecosystem services, the Global Assessment Report on Biodiversity and Ecosystem Services, was published by the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES). It stated that \"the state of nature has deteriorated at an unprecedented and accelerating rate\". To fix the problem, humanity will need a transformative change, including sustainable agriculture, reductions in consumption and waste, fishing quotas and collaborative water management.[232][233]\n\nThe concept of nature-positive is playing a role in mainstreaming the goals of the Global Biodiversity Framework (GBF) for biodiversity.[234] The aim of mainstreaming is to embed biodiversity considerations into public and private practice to conserve and sustainably use biodiversity on global and local levels.[235] The concept of nature-positive refers to the societal goal to halt and reverse biodiversity loss, measured from a baseline of 2020 levels, and to achieve full so-called \"nature recovery\" by 2050.[236]\n\nCitizen science, also known as public participation in scientific research, has been widely used in environmental sciences and is particularly popular in a biodiversity-related context. It has been used to enable scientists to involve the general public in biodiversity research, thereby enabling the scientists to collect data that they would otherwise not have been able to obtain.[237]\n\nVolunteer observers have made significant contributions to on-the-ground knowledge about biodiversity, and recent improvements in technology have helped increase the flow and quality of occurrences from citizen sources. A 2016 study published in Biological Conservation[238] registers the massive contributions that citizen scientists already make to data mediated by the Global Biodiversity Information Facility (GBIF). Despite some limitations of the dataset-level analysis, it is clear that nearly half of all occurrence records shared through the GBIF network come from datasets with significant volunteer contributions. Recording and sharing observations are enabled by several global-scale platforms, including iNaturalist and eBird.[239][240]\n\nGlobal agreements such as the Convention on Biological Diversity, give \"sovereign national rights over biological resources\" (not property). The agreements commit countries to \"conserve biodiversity\", \"develop resources for sustainability\" and \"share the benefits\" resulting from their use. Biodiverse countries that allow bioprospecting or collection of natural products, expect a share of the benefits rather than allowing the individual or institution that discovers/exploits the resource to capture them privately. Bioprospecting can become a type of biopiracy when such principles are not respected.[241]\n\nSovereignty principles can rely upon what is better known as Access and Benefit Sharing Agreements (ABAs). The Convention on Biodiversity implies informed consent between the source country and the collector, to establish which resource will be used and for what and to settle on a fair agreement on benefit sharing.\n\nOn the 19 of December 2022, during the 2022 United Nations Biodiversity Conference every country on earth, with the exception of the United States and the Holy See, signed onto the agreement which includes protecting 30% of land and oceans by 2030 (30 by 30) and 22 other targets intended to reduce biodiversity loss.[222][242][243] The agreement includes also recovering 30% of earth degraded ecosystems and increasing funding for biodiversity issues.[244]\n\nIn May 2020, the European Union published its Biodiversity Strategy for 2030. The biodiversity strategy is an essential part of the climate change mitigation strategy of the European Union. From the 25% of the European budget that will go to fight climate change, large part will go to restore biodiversity[210] and nature based solutions.\n\nThe EU Biodiversity Strategy for 2030 include the next targets:\n\nApproximately half of the global GDP depend on nature. In Europe many parts of the economy that generate trillions of euros per year depend on nature. The benefits of Natura 2000 alone in Europe are €200 – €300 billion per year.[246]\n\nBiodiversity is taken into account in some political and judicial decisions:\n\nUniform approval for use of biodiversity as a legal standard has not been achieved, however. Bosselman argues that biodiversity should not be used as a legal standard, claiming that the remaining areas of scientific uncertainty cause unacceptable administrative waste and increase litigation without promoting preservation goals.[249]\n\nIndia passed the Biological Diversity Act in 2002 for the conservation of biological diversity in India. The Act also provides mechanisms for equitable sharing of benefits from the use of traditional biological resources and knowledge.\n"
    },
    {
        "title": "Sustainable development",
        "url": "https://en.wikipedia.org/wiki/Sustainable_development",
        "content": "\n\nSustainable development is an approach to growth and human development that aims to meet the needs of the present without compromising the ability of future generations to meet their own needs.[1][2] The aim is to have a society where living conditions and resources meet human needs without undermining planetary integrity.[3][4] Sustainable development aims to balance the needs of the economy, environment, and social well-being. The Brundtland Report in 1987 helped to make the concept of sustainable development better known. \n\nSustainable development overlaps with the idea of sustainability which is a normative concept.[5] UNESCO formulated a distinction between the two concepts as follows: \"Sustainability is often thought of as a long-term goal (i.e. a more sustainable world), while sustainable development refers to the many processes and pathways to achieve it.\"[6]\n\nThe Rio Process that began at the 1992 Earth Summit in Rio de Janeiro has placed the concept of sustainable development on the international agenda. Sustainable development is the foundational concept of the Sustainable Development Goals (SDGs).[7] These global goals for the year 2030 were adopted in 2015 by the United Nations General Assembly (UNGA). They address the global challenges, including for example poverty, climate change, biodiversity loss, and peace. \n\n\nThere are some problems with the concept of sustainable development. Some scholars say it is an oxymoron because according to them, development is inherently unsustainable. Other commentators are disappointed in the lack of progress that has been achieved so far.[8][9] Scholars have stated that sustainable development is open-ended, much critiqued as ambiguous, incoherent, and therefore easily appropriated.[7] \nIn 1987, the United Nations World Commission on Environment and Development released the report Our Common Future, commonly called the Brundtland Report.[1] The report included a definition of \"sustainable development\" which is now widely used:[1][10]\n\nSustainable development is a development that meets the needs of the present without compromising the ability of future generations to meet their own needs. It contains two key concepts within it:\n\nSustainable development thus tries to find a balance between economic development, environmental protection, and social well-being.\n\nHowever, scholars have pointed out that there are manifold understandings of sustainable development. Also there are incoherencies in the dominant market-based socio-economic-political organisation. Attempts towards universal sustainable development need to account for the extremely varied challenges, circumstances, and choices that shape prospects and prosperity for all, everywhere.[7]\n\nThe discourse of sustainable development is highly influential in global and national governance frameworks, though its meaning and operationalization are context-dependent and have evolved over time. The evolution of this discourse can for example be seen in the transition from the Millennium Development Goals (MDGs, years 2000 to 2015) to the Sustainable Development Goals (SDGs, years 2015 to 2030).[11]\n\nSustainable development has its roots in ideas regarding sustainable forest management, which were developed in Europe during the 17th and 18th centuries.[12][13][14] In response to a growing awareness of the depletion of timber resources in England, John Evelyn argued, in his 1662 essay Sylva, that \"sowing and planting of trees had to be regarded as a national duty of every landowner, in order to stop the destructive over- exploitation of natural resources.\" In 1713, Hans Carl von Carlowitz, a senior mining administrator in the service of Elector Frederick Augustus I of Saxony published Sylvicultura economics, a 400-page work on forestry. Building upon the ideas of Evelyn and French minister Jean-Baptiste Colbert, von Carlowitz developed the concept of managing forests for sustained yield.[12] His work influenced others, including Alexander von Humboldt and Georg Ludwig Hartig, eventually leading to the development of the science of forestry. This, in turn, influenced people like Gifford Pinchot, the first head of the US Forest Service, whose approach to forest management was driven by the idea of wise use of resources, and Aldo Leopold whose land ethic was influential in the development of the environmental movement in the 1960s.[12][13]\n\nFollowing the publication of Rachel Carson's Silent Spring in 1962, the developing environmental movement drew attention to the relationship between economic growth and environmental degradation. Kenneth E. Boulding, in his influential 1966 essay The Economics of the Coming Spaceship Earth, identified the need for the economic system to fit itself to the ecological system with its limited pools of resources.[13] Another milestone was the 1968 article by Garrett Hardin that popularized the term \"tragedy of the commons\".[15]\n\nThe direct linking of sustainability and development in a contemporary sense can be traced to the early 1970s. \"Strategy of Progress\", a 1972 book (in German) by Ernst Basler, explained how the long-acknowledged sustainability concept of preserving forests for future wood production can be directly transferred to the broader importance of preserving environmental resources to sustain the world for future generations.[16] That same year, the interrelationship of environment and development was formally demonstrated in a systems dynamic simulation model reported in the classic report on Limits to Growth. This was commissioned by the Club of Rome and written by a group of scientists led by Dennis and Donella Meadows of the Massachusetts Institute of Technology. Describing the desirable \"state of global equilibrium\", the authors wrote: \"We are searching for a model output that represents a world system that is sustainable without sudden and uncontrolled collapse and capable of satisfying the basic material requirements of all of its people.\"[17] The year 1972 also saw the publication of the influential book, A Blueprint for Survival.[18][19]\n\nIn 1975, an MIT research group prepared ten days of hearings on \"Growth and Its Implication for the Future\" for the US Congress, the first hearings ever held on sustainable development.[20]\n\nIn 1980, the International Union for Conservation of Nature published a world conservation strategy that included one of the first references to sustainable development as a global priority[21] and introduced the term \"sustainable development\".[22]: 4  Two years later, the United Nations World Charter for Nature raised five principles of conservation by which human conduct affecting nature is to be guided and judged.[23]\n\nSince the Brundtland Report, the concept of sustainable development has developed beyond the initial intergenerational framework to focus more on the goal of \"socially inclusive and environmentally sustainable economic growth\".[22]: 5  In 1992, the UN Conference on Environment and Development published the Earth Charter, which outlines the building of a just, sustainable, and peaceful global society in the 21st century. The action plan Agenda 21 for sustainable development identified information, integration, and participation as key building blocks to help countries achieve development that recognizes these interdependent pillars. Furthermore, Agenda 21 emphasizes that broad public participation in decision-making is a fundamental prerequisite for achieving sustainable development.[24]\n\nThe Rio Protocol was a huge leap forward: for the first time, the world agreed on a sustainability agenda. In fact, a global consensus was facilitated by neglecting concrete goals and operational details. \n\nThe most comprehensive global governance framework for sustainable development is the 2030 Agenda for Sustainable Development with its 17 Sustainable Development Goals (SDGs).[11] This agenda was a follow-up to the Millennium Declaration from the year 2000 with its eight Millennium Development Goals (MDGs), the first comprehensive global governance framework for the achievement of sustainable development.[11] The SDGs have concrete targets (unlike the results from the Rio Process) but no methods for sanctions.[25][26]: 137  They contain goals, targets and indicators for example in the areas of poverty reduction, environmental protection, human prosperity and peace.[11]\n\nSustainability means different things to different people, and the concept of sustainable development has led to a diversity of discourses that legitimize competing sociopolitical projects. Global environmental governance scholars have identified a comprehensive set of discourses within the public space that mostly convey four sustainability frames: mainstream sustainability, progressive sustainability, a limits discourse, and radical sustainability.[27]\n\nFirst, mainstream sustainability is a conservative approach on both economic and political terms. Second, progressive sustainability is an economically conservative, yet politically reformist approach. Under this framing, sustainable development is still centered on economic growth, which is deemed compatible with environmental sustainability. However, human well-being and development can only be achieved through a redistribution of power to even out inequalities between developed and developing countries. Third, a limits discourse is an economically reformist, yet politically conservative approach to sustainability. Fourth, radical sustainability is a transformative approach seeking to break with existing global economic and political structures.[27]\n\n\n\nSustainable development, like sustainability, is regarded to have three dimensions: the environment, economy and society. The idea is that a good balance between the three dimensions should be achieved. Instead of calling them dimensions, other terms commonly used are pillars, domains, aspects, spheres.\n\nScholars usually distinguish three different areas of sustainability. These are the environmental, the social, and the economic. Several terms are in use for this concept. Authors may speak of three pillars, dimensions, components, aspects,[35] perspectives, factors, or goals. All mean the same thing in this context.[28] The three dimensions paradigm has few theoretical foundations.[28]\n\nCountries could develop systems for monitoring and evaluation of progress towards achieving sustainable development by adopting indicators that measure changes across economic, social and environmental dimensions.\nSix interdependent capacities are deemed to be necessary for the successful pursuit of sustainable development.[37] These are the capacities to measure progress towards sustainable development; promote equity within and between generations; adapt to shocks and surprises; transform the system onto more sustainable development pathways; link knowledge with action for sustainability; and to devise governance arrangements that allow people to work together.\n\nDuring the MDG era (year 2000 to 2015), the key objective of sustainable development was poverty reduction to be reached through economic growth and participation in the global trade system.[11] The SDGs take a much more comprehensive approach to sustainable development than the MDGs did. They offer a more people-centred development agenda. Out of the 17 SDGs, for example, 11 goals contain targets related to equity, equality or inclusion, and SDG 10 is solely devoted to addressing inequality within and among countries.[11]\n\n An unsustainable situation occurs when natural capital (the total of nature's resources) is used up faster than it can be replenished.[38]: 58  Sustainability requires that human activity only uses nature's resources at a rate at which they can be replenished naturally. The concept of sustainable development is intertwined with the concept of carrying capacity. Theoretically, the long-term result of environmental degradation is the inability to sustain human life.[38]\n\nImportant operational principles of sustainable development were published by Herman Daly in 1990: renewable resources should provide a sustainable yield (the rate of harvest should not exceed the rate of regeneration); for non-renewable resources there should be equivalent development of renewable substitutes; waste generation should not exceed the assimilative capacity of the environment.[39]\n\nIn 2019, a summary for policymakers of the largest, most comprehensive study to date of biodiversity and ecosystem services was published by the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services. It recommended that human civilization will need a transformative change, including sustainable agriculture, reductions in consumption and waste, fishing quotas and collaborative water management.[40][41]\n\nEnvironmental problems associated with industrial agriculture and agribusiness are now being addressed through approaches such as sustainable agriculture, organic farming and more sustainable business practices.[42] At the local level there are various movements working towards sustainable food systems which may include less meat consumption, local food production, slow food, sustainable gardening, and organic gardening.[43] The environmental effects of different dietary patterns depend on many factors, including the proportion of animal and plant foods consumed and the method of food production.[44][45]\nAs global population and affluence have increased, so has the use of various materials increased in volume, diversity, and distance transported. By 2050, humanity could consume an estimated 140 billion tons of minerals, ores, fossil fuels and biomass per year (three times its current amount) unless the economic growth rate is decoupled from the rate of natural resource consumption.[46]\n\nSustainable use of materials has targeted the idea of dematerialization, converting the linear path of materials (extraction, use, disposal in landfill) to a circular material flow that reuses materials as much as possible, much like the cycling and reuse of waste in nature.[47] This way of thinking is expressed in the concept of circular economy, which employs reuse, sharing, repair, refurbishment, remanufacturing and recycling to create a closed-loop system, minimizing the use of resource inputs and the creation of waste, pollution and carbon emissions.[48] The European Commission has adopted an ambitious Circular Economy Action Plan in 2020, which aims at making sustainable products the norm in the EU.[49][50]\n\nIt has been suggested that because of the rural poverty and overexploitation, environmental resources should be treated as important economic assets, called natural capital.[51] Economic development has traditionally required a growth in the gross domestic product. This model of unlimited personal and GDP growth may be over. Sustainable development may involve improvements in the quality of life for many but may necessitate a decrease in resource consumption.[52] \"Growth\" generally ignores the direct effect that the environment may have on social welfare, whereas \"development\" takes it into account.[53]\n\nAs early as the 1970s, the concept of sustainability was used to describe an economy \"in equilibrium with basic ecological support systems\".[54] Scientists in many fields have highlighted The Limits to Growth,[55][56] and economists have presented alternatives, for example a 'steady-state economy', to address concerns over the impacts of expanding human development on the planet.[57] In 1987, the economist Edward Barbier published the study The Concept of Sustainable Economic Development, where he recognized that goals of environmental conservation and economic development are not conflicting and can be reinforcing each other.[58]\n\nA World Bank study from 1999 concluded that based on the theory of genuine savings (defined as \"traditional net savings less the value of resource depletion and environmental degradation plus the value of investment in human capital\"), policymakers have many possible interventions to increase sustainability, in macroeconomics or purely environmental.[59] Several studies have noted that efficient policies for renewable energy and pollution are compatible with increasing human welfare, eventually reaching a golden-rule[clarification needed] steady state.[60][61][62][63]\n\nA meta review in 2002 looked at environmental and economic valuations and found a \"lack of concrete understanding of what \"sustainability policies\" might entail in practice\".[64] A study concluded in 2007 that knowledge, manufactured and human capital (health and education) has not compensated for the degradation of natural capital in many parts of the world.[65] It has been suggested that intergenerational equity can be incorporated into a sustainable development and decision making, as has become common in economic valuations of climate economics.[66]\n\nThe World Business Council for Sustainable Development published a Vision 2050 document in 2021 to show \"How business can lead the transformations the world needs\". The vision states that \"we envision a world in which 9+billion people can live well, within planetary boundaries, by 2050.\"[67] This report was highlighted by The Guardian as \"the largest concerted corporate sustainability action plan to date – include reversing the damage done to ecosystems, addressing rising greenhouse gas emissions and ensuring societies move to sustainable agriculture.\"[68]\n\nThere are many reasons why sustainability is so difficult to achieve. These reasons have the name sustainability barriers.[32][69] Before addressing these barriers it is important to analyze and understand them.[32]: 34  Some barriers arise from nature and its complexity (\"everything is related\").[70] Others arise from the human condition. One example is the value-action gap. This reflects the fact that people often do not act according to their convictions. Experts describe these barriers as intrinsic to the concept of sustainability.[71]: 81 \n\nOther barriers are extrinsic to the concept of sustainability. This means it is possible to overcome them. One way would be to put a price tag on the consumption of public goods.[71]: 84  Some extrinsic barriers relate to the nature of dominant institutional frameworks. Examples would be where market mechanisms fail for public goods. Existing societies, economies, and cultures encourage increased consumption. There is a structural imperative for growth in competitive market economies. This inhibits necessary societal change.[72]\n\nFurthermore, there are several barriers related to the difficulties of implementing sustainability policies. There are trade-offs between the goals of environmental policies and economic development. Environmental goals include nature conservation. Development may focus on poverty reduction.[69][32]: 65  There are also trade-offs between short-term profit and long-term viability.[71]: 65  Political pressures generally favor the short term over the long term. So they form a barrier to actions oriented toward improving sustainability.[71]: 86 \n\nThe concept of sustainable development has been and still is, subject to criticism, including the question of what is to be sustained in sustainable development. It has been argued that there is no such thing as sustainable use of a non-renewable resource, since any positive rate of exploitation will eventually lead to the exhaustion of earth's finite stock;[73]: 13  this perspective renders the Industrial Revolution as a whole unsustainable.[74]: 20f [75]: 61–67 [57]: 22f \n\nThe sustainable development debate is based on the assumption that societies need to manage three types of capital (economic, social, and natural), which may be non-substitutable and whose consumption might be irreversible.[76] Natural capital can not necessarily be substituted by economic capital.[57] While it is possible that we can find ways to replace some natural resources, it is much less likely that they will ever be able to replace ecosystem services, such as the protection provided by the ozone layer, or the climate stabilizing function of the Amazonian forest.\n\nThe concept of sustainable development has been criticized from different angles. While some see it as paradoxical (or an oxymoron) and regard development as inherently unsustainable, others are disappointed in the lack of progress that has been achieved so far.[8][9] Part of the problem is that \"development\" itself is not consistently defined.[26]: 16 [77]\n\nThe vagueness of the Brundtland definition of sustainable development has been criticized as follows:[26]: 17  The definition has \"opened up the possibility of downplaying sustainability. Hence, governments spread the message that we can have it all at the same time, i.e. economic growth, prospering societies and a healthy environment. No new ethic is required. This so-called weak version of sustainability is popular among governments, and businesses, but profoundly wrong and not even weak, as there is no alternative to preserving the earth's ecological integrity.\"[78]: 2 \n\nScholars have stated that sustainable development is open-ended, much critiqued as ambiguous, incoherent, and therefore easily appropriated.[7]\n\nSustainable development is the foundational concept of the Sustainable Development Goals (SDGs). Policies to achieve the SDGs are meant to cohere around this concept.[7]\nThe Sustainable Development Goals (SDGs) are a collection of 17 global objectives established by the United Nations in 2015 as part of the 2030 Agenda for Sustainable Development. These goals aim to address a broad range of interconnected global challenges, including poverty eradication, environmental sustainability, social equity, and economic growth, under the guiding principle of \"leaving no one behind.\" Designed to replace the Millennium Development Goals, the SDGs apply universally to all countries, irrespective of their development status, and seek to promote peace, prosperity, and the health of the planet.\n\nEach goal is further divided into specific targets—totaling 169—and measured by 232 unique indicators to track progress. Key goals include ending poverty (SDG 1), achieving gender equality (SDG 5), combating climate change (SDG 13), and fostering global partnerships (SDG 17). However, progress has been uneven and faces significant obstacles, such as rising inequality, climate change, biodiversity loss, and the impact of the COVID-19 pandemic.\n\nEducation for sustainable development (ESD) is a term officially used by the United Nations. It is defined as education practices that encourage changes in knowledge, skills, values, and attitudes to enable a more sustainable and just society for humanity. ESD aims to empower and equip current and future generations to meet their needs using a balanced and integrated approach to sustainable development's economic, social, and environmental dimensions.[79][80]\n\nAgenda 21 was the first international document that identified education as an essential tool for achieving sustainable development and highlighted areas of action for education.[81][82] ESD is a component of measurement in an indicator for Sustainable Development Goal 12 (SDG) for \"responsible consumption and production\". SDG 12 has 11 targets, and target 12.8 is \"By 2030, ensure that people everywhere have the relevant information and awareness for sustainable development and lifestyles in harmony with nature.\"[83] 20 years after the Agenda 21 document was declared, the 'Future we want' document was proclaimed in the Rio+20 UN Conference on Sustainable Development, stating that \"We resolve to promote education for sustainable development and to integrate sustainable development more actively into education beyond the Decade of Education for Sustainable Development.\"[84]\n\nOne version of education for Sustainable Development recognizes modern-day environmental challenges. It seeks to define new ways to adjust to a changing biosphere, as well as engage individuals to address societal issues that come with them [85] In the International Encyclopedia of Education, this approach to education is seen as an attempt to \"shift consciousness toward an ethics of life-giving relationships that respects the interconnectedness of man to his natural world\" to equip future members of society with environmental awareness and a sense of responsibility to sustainability.[86]\n\nFor UNESCO, education for sustainable development involves:\n\nintegrating key sustainable development issues into teaching and learning. This may include, for example, instruction about climate change, disaster risk reduction, biodiversity, and poverty reduction and sustainable consumption. It also requires participatory teaching and learning methods that motivate and empower learners to change their behaviours and take action for sustainable development. ESD consequently promotes competencies like critical thinking, imagining future scenarios and making decisions in a collaborative way.[87][88]\nThe Thessaloniki Declaration, presented at the \"International Conference on Environment and Society: Education and Public Awareness for Sustainability\" by UNESCO and the Government of Greece (December 1997), highlights the importance of sustainability not only with regards to the natural environment, but also with \"poverty, health, food security, democracy, human rights, and peace\".[89]\n"
    },
    {
        "title": "Globalization",
        "url": "https://en.wikipedia.org/wiki/Globalization",
        "content": "\n\n1800s: Martineau · Tocqueville · Marx · Spencer · Le Bon  · Ward · Pareto · Tönnies · Veblen · Simmel · Durkheim · Addams · Mead · Weber · Du Bois · Mannheim · Elias\n\nGlobalization (North American spelling; also Oxford spelling [UK]) or globalisation (non-Oxford British spelling; see spelling differences) is the process of increasing interdependence and integration among the economies, markets, societies, and cultures of different countries worldwide. This is made possible by the reduction of barriers to international trade, the liberalization of capital movements, the development of transportation, and the advancement of information and communication technologies.[1] The term globalization first appeared in the early 20th century (supplanting an earlier French term mondialisation), developed its current meaning sometime in the second half of the 20th century, and came into popular use in the 1990s to describe the unprecedented international connectivity of the post–Cold War world.[2] The origins of globalization can be traced back to the 18th and 19th centuries, driven by advances in transportation and communication technologies. These developments increased global interactions, fostering the growth of international trade and the exchange of ideas, beliefs, and cultures. While globalization is primarily an economic process of interaction and integration, it is also closely linked to social and cultural dynamics. Additionally, disputes and international diplomacy have played significant roles in the history and evolution of globalization, continuing to shape its modern form.\n\nEconomically, globalization involves goods, services, data, technology, and the economic resources of capital.[3] The expansion of global markets liberalizes the economic activities of the exchange of goods and funds. Removal of cross-border trade barriers has made the formation of global markets more feasible.[4] Advances in transportation, like the steam locomotive, steamship, jet engine, and container ships, and developments in telecommunication infrastructure such as the telegraph, the Internet, mobile phones, and smartphones, have been major factors in globalization and have generated further interdependence of economic and cultural activities around the globe.[5][6][7]\n\nThough many scholars place the origins of globalization in modern times, others trace its history to long before the European Age of Discovery and voyages to the New World, and some even to the third millennium BCE.[8] Large-scale globalization began in the 1820s, and in the late 19th century and early 20th century drove a rapid expansion in the connectivity of the world's economies and cultures.[9] The term global city was subsequently popularized by sociologist Saskia Sassen in her work The Global City: New York, London, Tokyo (1991).[10]\n\nIn 2000, the International Monetary Fund (IMF) identified four basic aspects of globalization: trade and transactions, capital and investment movements, migration and movement of people, and the dissemination of knowledge.[11] Globalizing processes affect and are affected by business and work organization, economics, sociocultural resources, and the natural environment. Academic literature commonly divides globalization into three major areas: economic globalization, cultural globalization, and political globalization.[12]\n\nProponents of globalization point to economic growth and broader societal development as benefits, while opponents claim globalizing processes are detrimental to social well-being due to ethnocentrism, environmental consequences, and other potential drawbacks.[13][14]\n\nBetween 1990 and 2010, globalisation progressed rapidly, driven by the information and communication technology revolution that lowered communication costs, along with trade liberalisation and the shift of manufacturing operations to emerging economies (particularly China). [15][16][17]\n\nThe word globalization was used in the English language as early as the 1930s, but only in the context of education, and the term failed to gain traction. Over the next few decades, the term was occasionally used by other scholars and media, but it was not clearly defined.[2] One of the first usages of the term in the meaning resembling the later, was by French economist François Perroux in his essays from the early 1960s (in his French works he used the term \"mondialisation\" (literarly worldization in French), also translated as mundialization).[2] Theodore Levitt is often credited with popularizing the term and bringing it into the mainstream business audience in the later in the middle of 1980s.[2]\n\nThough often treated as synonyms, in French, globalization is seen as a stage following mondialisation, a stage that implies the dissolution of national identities and the abolishment of borders inside the world network of economic exchanges.[18]\n\nSince its inception, the concept of globalization has inspired competing definitions and interpretations. Its antecedents date back to the great movements of trade and empire across Asia and the Indian Ocean from the 15th century onward.[19][20]\n\nIn 1848, Karl Marx noticed the increasing level of national inter-dependence brought on by capitalism, and predicted the universal character of the modern world society. He states:\n\nThe bourgeoisie has through its exploitation of the world market given a cosmopolitan character to production and consumption in every country. To the great chagrin of Reactionists, it has drawn from under the feet of industry the national ground on which it stood. All old-established national industries have been destroyed or are daily being destroyed. . . . In place of the old local and national seclusion and self-sufficiency, we have intercourse in every direction, universal inter-dependence of nations.[21]\nSociologists Martin Albrow and Elizabeth King define globalization as \"all those processes by which the people of the world are incorporated into a single world society.\"[3] In The Consequences of Modernity, Anthony Giddens writes: \"Globalization can thus be defined as the intensification of worldwide social relations which link distant localities in such a way that local happenings are shaped by events occurring many miles away and vice versa.\"[22] In 1992, Roland Robertson, professor of sociology at the University of Aberdeen and an early writer in the field, described globalization as \"the compression of the world and the intensification of the consciousness of the world as a whole.\"[23]\n\nIn Global Transformations, David Held and his co-writers state:\n\nAlthough in its simplistic sense globalization refers to the widening, deepening and speeding up of global interconnection, such a definition begs further elaboration. ... Globalization can be on a continuum with the local, national and regional. At one end of the continuum lie social and economic relations and networks which are organized on a local and/or national basis; at the other end lie social and economic relations and networks which crystallize on the wider scale of regional and global interactions. Globalization can refer to those spatial-temporal processes of change which underpin a transformation in the organization of human affairs by linking together and expanding human activity across regions and continents. Without reference to such expansive spatial connections, there can be no clear or coherent formulation of this term. ... A satisfactory definition of globalization must capture each of these elements: extensity (stretching), intensity, velocity and impact.[24]\nHeld and his co-writers' definition of globalization in that same book as \"transformation in the spatial organization of social relations and transactions—assessed in terms of their extensity, intensity, velocity and impact—generating transcontinental or inter-regional flows\" was called \"probably the most widely-cited definition\" in the 2014 DHL Global Connectiveness Index.[25]\n\nSwedish journalist Thomas Larsson, in his book The Race to the Top: The Real Story of Globalization, states that globalization:\n\n...is the process of world shrinkage, of distances getting shorter, things moving closer. It pertains to the increasing ease with which somebody on one side of the world can interact, to mutual benefit, with somebody on the other side of the world.[26]\nPaul James defines globalization with a more direct and historically contextualized emphasis:\n\nGlobalization is the extension of social relations across world-space, defining that world-space in terms of the historically variable ways that it has been practiced and socially understood through changing world-time.[27]\nManfred Steger, professor of global studies and research leader in the Global Cities Institute at RMIT University, identifies four main empirical dimensions of globalization: economic, political, cultural, and ecological. A fifth dimension—the ideological—cutting across the other four. The ideological dimension, according to Steger, is filled with a range of norms, claims, beliefs, and narratives about the phenomenon itself.[28]\n\nJames and Steger stated that the concept of globalization \"emerged from the intersection of four interrelated sets of 'communities of practice' (Wenger, 1998): academics, journalists, publishers/editors, and librarians.\"[2]: 424  They note the term was used \"in education to describe the global life of the mind\"; in international relations to describe the extension of the European Common Market, and in journalism to describe how the \"American Negro and his problem are taking on a global significance\".[2] They have also argued that four forms of globalization can be distinguished that complement and cut across the solely empirical dimensions.[27][29] According to James, the oldest dominant form of globalization is embodied globalization, the movement of people. A second form is agency-extended globalization, the circulation of agents of different institutions, organizations, and polities, including imperial agents. Object-extended globalization, a third form, is the movement of commodities and other objects of exchange. He calls the transmission of ideas, images, knowledge, and information across world-space disembodied globalization, maintaining that it is currently the dominant form of globalization. James holds that this series of distinctions allows for an understanding of how, today, the most embodied forms of globalization such as the movement of refugees and migrants are increasingly restricted, while the most disembodied forms such as the circulation of financial instruments and codes are the most deregulated.[30]\n\nThe journalist Thomas L. Friedman popularized the term \"flat world\", arguing that globalized trade, outsourcing, supply-chaining, and political forces had permanently changed the world, for better and worse. He asserted that the pace of globalization was quickening and that its impact on business organization and practice would continue to grow.[31]\n\nEconomist Takis Fotopoulos defined \"economic globalization\" as the opening and deregulation of commodity, capital, and labor markets that led toward present neoliberal globalization. He used \"political globalization\" to refer to the emergence of a transnational élite and a phasing out of the nation-state. Meanwhile, he used \"cultural globalization\" to reference the worldwide homogenization of culture. Other of his usages included \"ideological globalization\", \"technological globalization\", and \"social globalization\".[32]\n\nLechner and Boli (2012) define globalization as more people across large distances becoming connected in more and different ways.[33]\n\n\"Globophobia\" is used to refer to the fear of globalization, though it can also mean the fear of balloons.[34][35][36]\n\nThere are both distal and proximate causes which can be traced in the historical factors affecting globalization. Large-scale globalization began in the 19th century.[37]\n\nArchaic globalization conventionally refers to a phase in the history of globalization including globalizing events and developments from the time of the earliest civilizations until roughly the 1600s. This term is used to describe the relationships between communities and states and how they were created by the geographical spread of ideas and social norms at both local and regional levels.[38]\n\nIn this schema, three main prerequisites are posited for globalization to occur. The first is the idea of Eastern Origins, which shows how Western states have adapted and implemented learned principles from the East.[38] Without the spread of traditional ideas from the East, Western globalization would not have emerged the way it did. The interactions of states were not on a global scale and most often were confined to Asia, North Africa, the Middle East, and certain parts of Europe.[38] With early globalization, it was difficult for states to interact with others that were not close. Eventually, technological advances allowed states to learn of others' existence and thus another phase of globalization can occur. The third has to do with inter-dependency, stability, and regularity. If a state is not dependent on another, then there is no way for either state to be mutually affected by the other. This is one of the driving forces behind global connections and trade; without either, globalization would not have emerged the way it did and states would still be dependent on their own production and resources to work. This is one of the arguments surrounding the idea of early globalization. It is argued that archaic globalization did not function in a similar manner to modern globalization because states were not as interdependent on others as they are today.[38]\n\nAlso posited is a \"multi-polar\" nature to archaic globalization, which involved the active participation of non-Europeans. Because it predated the Great Divergence in the nineteenth century, where Western Europe pulled ahead of the rest of the world in terms of industrial production and economic output, archaic globalization was a phenomenon that was driven not only by Europe but also by other economically developed Old World centers such as Gujarat, Bengal, coastal China, and Japan.[39]\n\nThe German historical economist and sociologist Andre Gunder Frank argues that a form of globalization began with the rise of trade links between Sumer and the Indus Valley civilization in the third millennium BCE. This archaic globalization existed during the Hellenistic Age, when commercialized urban centers enveloped the axis of Greek culture that reached from India to Spain, including Alexandria and the other Alexandrine cities. Early on, the geographic position of Greece and the necessity of importing wheat forced the Greeks to engage in maritime trade. Trade in ancient Greece was largely unrestricted: the state controlled only the supply of grain.[8]\n\nTrade on the Silk Road was a significant factor in the development of civilizations from China, the Indian subcontinent, Persia, Europe, and Arabia, opening long-distance political and economic interactions between them.[40] Though silk was certainly the major trade item from China, common goods such as salt and sugar were traded as well; and religions, syncretic philosophies, and various technologies, as well as diseases, also traveled along the Silk Routes. In addition to economic trade, the Silk Road served as a means of carrying out cultural trade among the civilizations along its network.[41] The movement of people, such as refugees, artists, craftsmen, missionaries, robbers, and envoys, resulted in the exchange of religions, art, languages, and new technologies.[42] From around 3000 BCE to 1000 CE, connectivity within Afro-Eurasia was centered upon the Indo-Mediterranean region, with the Silk Road later rising in importance with the Mongol Empire's consolidation of Asia in the 13th century.[43][44]\n\n\"Early modern\" or \"proto-globalization\" covers a period of the history of globalization roughly spanning the years between 1600 and 1800. The concept of \"proto-globalization\" was first introduced by historians A. G. Hopkins and Christopher Bayly. The term describes the phase of increasing trade links and cultural exchange that characterized the period immediately preceding the advent of high \"modern globalization\" in the late 19th century.[45] This phase of globalization was characterized by the rise of maritime European empires, in the 15th and 17th centuries, first the Portuguese Empire (1415) followed by the Spanish Empire (1492), and later the Dutch and British Empires. In the 17th century, world trade developed further when chartered companies like the British East India Company (founded in 1600) and the Dutch East India Company (founded in 1602, often described as the first multinational corporation in which stock was offered) were established.[46]\n\nAn alternative view from historians Dennis Flynn and Arturo Giraldez, postulated that: globalization began with the first circumnavigation of the globe under the Magellan-Elcano expedition which preluded the rise of global silver trade.[47][48]\n\nEarly modern globalization is distinguished from modern globalization on the basis of expansionism, the method of managing global trade, and the level of information exchange. The period is marked by the shift of hegemony to Western Europe, the rise of larger-scale conflicts between powerful nations such as the Thirty Years' War, and demand for commodities, most particularly slaves. The triangular trade made it possible for Europe to take advantage of resources within the Western Hemisphere. The transfer of animal stocks, plant crops, and epidemic diseases associated with Alfred W. Crosby's concept of the Columbian exchange also played a central role in this process. European, Middle Eastern, Indian, Southeast Asian, and Chinese merchants were all involved in early modern trade and communications, particularly in the Indian Ocean region.\n\nAccording to economic historians Kevin H. O'Rourke, Leandro Prados de la Escosura, and Guillaume Daudin, several factors promoted globalization in the period 1815–1870:[49]\n\nDuring the 19th century, globalization approached its form as a direct result of the Industrial Revolution. Industrialization allowed standardized production of household items using economies of scale while rapid population growth created sustained demand for commodities. In the 19th century, steamships reduced the cost of international transportation significantly and railroads made inland transportation cheaper. The transportation revolution occurred some time between 1820 and 1850.[37] More nations embraced international trade.[37] Globalization in this period was decisively shaped by nineteenth-century imperialism such as in Africa and Asia.[50][51]\n\nAfter World War II, work by politicians led to the agreements of the Bretton Woods Conference, in which major governments laid down the framework for international monetary policy, commerce, and finance, and the founding of several international institutions intended to facilitate economic growth by lowering trade barriers. Initially, the General Agreement on Tariffs and Trade (GATT) led to a series of agreements to remove trade restrictions. GATT's successor was the World Trade Organization (WTO), which provided a framework for negotiating and formalizing trade agreements and a dispute resolution process. Exports nearly doubled from 8.5% of total gross world product in 1970 to 16.2% in 2001.[52] The approach of using global agreements to advance trade stumbled with the failure of the Doha Development Round of trade negotiation. Many countries then shifted to bilateral or smaller multilateral agreements, such as the 2011 United States–Korea Free Trade Agreement.\n\nThe invention of shipping containers in 1956 helped advance the globalization of commerce.[50][51] Since the 1970s, aviation has become increasingly affordable to middle classes in developed countries. Open skies policies and low-cost carriers have helped to bring competition to the market. In the 1990s, the growth of low-cost communication networks cut the cost of communicating between countries. More work can be performed using a computer without regard to location. This included accounting, software development, and engineering design.\n\nStudent exchange programs became popular after World War II, and are intended to increase the participants' understanding and tolerance of other cultures, as well as improving their language skills and broadening their social horizons. Between 1963 and 2006 the number of students studying in a foreign country increased 9 times.[53]\n\nSince the 1980s, modern globalization has spread rapidly through the expansion of capitalism and neoliberal ideologies.[54] The implementation of neoliberal policies has allowed for the privatization of public industry, deregulation of laws or policies that interfered with the free flow of the market, as well as cut-backs to governmental social services.[55] These neoliberal policies were introduced to many developing countries in the form of structural adjustment programs (SAPs) that were implemented by the World Bank and the International Monetary Fund (IMF).[54] These programs required that the country receiving monetary aid would open its markets to capitalism, privatize public industry, allow free trade, cut social services like healthcare and education and allow the free movement of giant multinational corporations.[56] These programs allowed the World Bank and the IMF to become global financial market regulators that would promote neoliberalism and the creation of free markets for multinational corporations on a global scale.[57]\n\nIn the late 19th and early 20th century, the connectedness of the world's economies and cultures grew very quickly. This slowed down from the 1910s onward due to the World Wars and the Cold War,[58] but picked up again in the 1980s and 1990s.[59] The revolutions of 1989 and subsequent liberalization in many parts of the world resulted in a significant expansion of global interconnectedness. The migration and movement of people can also be highlighted as a prominent feature of the globalization process. In the period between 1965 and 1990, the proportion of the labor force migrating approximately doubled. Most migration occurred between the developing countries and least developed countries (LDCs).[60] As economic integration intensified workers moved to areas with higher wages and most of the developing world oriented toward the international market economy. The collapse of the Soviet Union not only ended the Cold War's division of the world – it also left the United States its sole policeman and an unfettered advocate of free market.[according to whom?] It also resulted in the growing prominence of attention focused on the movement of diseases, the proliferation of popular culture and consumer values, the growing prominence of international institutions like the UN, and concerted international action on such issues as the environment and human rights.[61] Other developments as dramatic were the Internet's becoming influential in connecting people across the world; As of June 2012[update], more than 2.4 billion people—over a third of the world's human population—have used the services of the Internet.[62][63] Growth of globalization has never been smooth. One influential event was the late 2000s recession, which was associated with lower growth (in areas such as cross-border phone calls and Skype usage) or even temporarily negative growth (in areas such as trade) of global interconnectedness.[64][65]\n\nThe China–United States trade war, starting in 2018, negatively affected trade between the two largest national economies. The economic impact of the COVID-19 pandemic included a massive decline in tourism and international business travel as many countries temporarily closed borders. The 2021–2022 global supply chain crisis resulted from temporary shutdowns of manufacturing and transportation facilities, and labor shortages. Supply problems incentivized some switches to domestic production.[66] The economic impact of the 2022 Russian invasion of Ukraine included a blockade of Ukrainian ports and international sanctions on Russia, resulting in some de-coupling of the Russian economy with global trade, especially with the European Union and other Western countries.\n\nModern consensus for the last 15 years regards globalization as having run its course and gone into decline.[67] A common argument for this is that trade has dropped since its peak in 2008, and never recovered since the Great Recession. New opposing views from some economists have argued such trends are a result of price drops and in actuality, trade volume is increasing, especially with agricultural products, natural resources and refined petroleum.[68][69] The 21st century melting of the Arctic will also affect global trade, as it is paving the way for shorter trade routes.[70]\n\nEconomic globalization is the increasing economic interdependence of national economies across the world through a rapid increase in cross-border movement of goods, services, technology, and capital.[72] Whereas the globalization of business is centered around the diminution of international trade regulations as well as tariffs, taxes, and other impediments that suppresses global trade, economic globalization is the process of increasing economic integration between countries, leading to the emergence of a global marketplace or a single world market.[73] Depending on the paradigm, economic globalization can be viewed as either a positive or a negative phenomenon. Economic globalization comprises: globalization of production; which refers to the obtainment of goods and services from a particular source from locations around the globe to benefit from difference in cost and quality. Likewise, it also comprises globalization of markets; which is defined as the union of different and separate markets into a massive global marketplace. Economic globalization also includes[74] competition, technology, and corporations and industries.[72]\n\nCurrent globalization trends can be largely accounted for by developed economies integrating with less developed economies by means of foreign direct investment, the reduction of trade barriers as well as other economic reforms, and, in many cases, immigration.[75]\n\nInternational standards have made trade in goods and services more efficient. An example of such standard is the intermodal container. Containerization dramatically reduced the costs of transportation, supported the post-war boom in international trade, and was a major element in globalization.[50] International standards are set by the International Organization for Standardization, which is composed of representatives from various national standards organizations.\n\nA multinational corporation, or worldwide enterprise,[76] is an organization that owns or controls the production of goods or services in one or more countries other than their home country.[77] It can also be referred to as an international corporation, a transnational corporation, or a stateless corporation.[78]\n\nA free-trade area is the region encompassing a trade bloc whose member countries have signed a free-trade agreement (FTA). Such agreements involve cooperation between at least two countries to reduce trade barriers –  import quotas and tariffs –  and to increase trade of goods and services with each other.[79]\n\nIf people are also free to move between the countries, in addition to a free-trade agreement, it would also be considered an open border. Arguably, the most significant free-trade area in the world is the European Union, a politico-economic union of 27 member states that are primarily located in Europe. The EU has developed European Single Market through a standardized system of laws that apply in all member states. EU policies aim to ensure the free movement of people, goods, services, and capital within the internal market,[80]\n\nTrade facilitation looks at how procedures and controls governing the movement of goods across national borders can be improved to reduce associated cost burdens and maximize efficiency while safeguarding legitimate regulatory objectives.\n\nGlobal trade in services is also significant. For example, in India, business process outsourcing has been described as the \"primary engine of the country's development over the next few decades, contributing broadly to GDP growth, employment growth, and poverty alleviation\".[81][82]\n\nWilliam I. Robinson's theoretical approach to globalization is a critique of Wallerstein's World Systems Theory. He believes that the global capital experienced today is due to a new and distinct form of globalization which began in the 1980s. Robinson argues not only are economic activities expanded across national boundaries but also there is a transnational fragmentation of these activities.[83] One important aspect of Robinson's globalization theory is that production of goods are increasingly global. This means that one pair of shoes can be produced by six countries, each contributing to a part of the production process.\n\nCultural globalization refers to the transmission of ideas, meanings, and values around the world in such a way as to extend and intensify social relations.[84] This process is marked by the common consumption of cultures that have been diffused by the Internet, popular culture media, and international travel. This has added to processes of commodity exchange and colonization which have a longer history of carrying cultural meaning around the globe. The circulation of cultures enables individuals to partake in extended social relations that cross national and regional borders. The creation and expansion of such social relations is not merely observed on a material level. Cultural globalization involves the formation of shared norms and knowledge with which people associate their individual and collective cultural identities. It brings increasing interconnectedness among different populations and cultures.[85]\n\nCross-cultural communication is a field of study that looks at how people from differing cultural backgrounds communicate, in similar and different ways among themselves, and how they endeavor to communicate across cultures. Intercultural communication is a related field of study.\n\nCultural diffusion is the spread of cultural items—such as ideas, styles, religions, technologies, languages etc.\nCultural globalization has increased cross-cultural contacts, but may be accompanied by a decrease in the uniqueness of once-isolated communities. For example, sushi is available in Germany as well as Japan, but Euro-Disney outdraws the city of Paris, potentially reducing demand for \"authentic\" French pastry.[86][87][88] Globalization's contribution to the alienation of individuals from their traditions may be modest compared to the impact of modernity itself, as alleged by existentialists such as Jean-Paul Sartre and Albert Camus. Globalization has expanded recreational opportunities by spreading pop culture, particularly via the Internet and satellite television. The cultural diffusion can create a homogenizing force, where globalization is seen as synonymous with homogenizing force via connectedness of markets, cultures, politics and the desire for modernizations through imperial countries sphere of influence.[89]\n\nReligions were among the earliest cultural elements to globalize, being spread by force, migration, evangelists, imperialists, and traders. Christianity, Islam, Buddhism, and more recently sects such as Mormonism are among those religions which have taken root and influenced endemic cultures in places far from their origins.[90]\n\nGlobalization has strongly influenced sports.[91] For example, the modern Olympic Games has athletes from more than 200 nations participating in a variety of competitions.[92] The FIFA World Cup is the most widely viewed and followed sporting event in the world, exceeding even the Olympic Games; a ninth of the entire population of the planet watched the 2006 FIFA World Cup Final.[93][94][95]\n\nThe term globalization implies transformation. Cultural practices including traditional music can be lost or turned into a fusion of traditions. Globalization can trigger a state of emergency for the preservation of musical heritage. Archivists may attempt to collect, record, or transcribe repertoires before melodies are assimilated or modified, while local musicians may struggle for authenticity and to preserve local musical traditions. Globalization can lead performers to discard traditional instruments. Fusion genres can become interesting fields of analysis.[96]\n\nMusic has an important role in economic and cultural development during globalization. Music genres such as jazz and reggae began locally and later became international phenomena. Globalization gave support to the world music phenomenon by allowing music from developing countries to reach broader audiences.[97] Though the term \"World Music\" was originally intended for ethnic-specific music, globalization is now expanding its scope such that the term often includes hybrid subgenres such as \"world fusion\", \"global fusion\", \"ethnic fusion\",[98] and worldbeat.[99][100]\n\nBourdieu claimed that the perception of consumption can be seen as self-identification and the formation of identity. Musically, this translates into each individual having their own musical identity based on likes and tastes. These likes and tastes are greatly influenced by culture, as this is the most basic cause for a person's wants and behavior. The concept of one's own culture is now in a period of change due to globalization. Also, globalization has increased the interdependency of political, personal, cultural, and economic factors.[102]\n\nA 2005 UNESCO report[103] showed that cultural exchange is becoming more frequent from Eastern Asia, but that Western countries are still the main exporters of cultural goods. In 2002, China was the third largest exporter of cultural goods, after the UK and US. Between 1994 and 2002, both North America's and the European Union's shares of cultural exports declined while Asia's cultural exports grew to surpass North America. Related factors are the fact that Asia's population and area are several times that of North America. Americanization is related to a period of high political American clout and of significant growth of America's shops, markets and objects being brought into other countries.\n\nSome critics of globalization argue that it harms the diversity of cultures. As a dominating country's culture is introduced into a receiving country through globalization, it can become a threat to the diversity of local culture. Some argue that globalization may ultimately lead to Westernization or Americanization of culture, where the dominating cultural concepts of economically and politically powerful Western countries spread and cause harm to local cultures.[104]\n\nGlobalization is a diverse phenomenon that relates to a multilateral political world and to the increase of cultural objects and markets between countries. The Indian experience particularly reveals the plurality of the impact of cultural globalization.[105]\n\nTransculturalism is defined as \"seeing oneself in the other\".[106] Transcultural[107] is in turn described as \"extending through all human cultures\"[107] or \"involving, encompassing, or combining elements of more than one culture\".[108] Children brought up in transcultural backgrounds are sometimes called third-culture kids.\n\nPolitical globalization refers to the growth of the worldwide political system, both in size and complexity. That system includes national governments, their governmental and intergovernmental organizations as well as government-independent elements of global civil society such as international non-governmental organizations and social movement organizations. One of the key aspects of the political globalization is the declining importance of the nation-state and the rise of other actors on the political scene.\nWilliam R. Thompson has defined it as \"the expansion of a global political system, and its institutions, in which inter-regional transactions (including, but certainly not limited to trade) are managed\".[109] \nPolitical globalization is one of the three main dimensions of globalization commonly found in academic literature, with the two other being economic globalization and cultural globalization.[12]\n\nIntergovernmentalism is a term in political science with two meanings. The first refers to a theory of regional integration originally proposed by Stanley Hoffmann; the second treats states and the national government as the primary factors for integration. Multi-level governance is an approach in political science and public administration theory that originated from studies on European integration. Multi-level governance gives expression to the idea that there are many interacting authority structures at work in the emergent global political economy. It illuminates the intimate entanglement between the domestic and international levels of authority.\n\nSome people are citizens of multiple nation-states. Multiple citizenship, also called dual citizenship or multiple nationality or dual nationality, is a person's citizenship status, in which a person is concurrently regarded as a citizen of more than one state under the laws of those states.\n\nIncreasingly, non-governmental organizations influence public policy across national boundaries, including humanitarian aid and developmental efforts.[111] Philanthropic organizations with global missions are also coming to the forefront of humanitarian efforts; charities such as the Bill and Melinda Gates Foundation, Accion International, the Acumen Fund (now Acumen) and the Echoing Green have combined the business model with philanthropy, giving rise to business organizations such as the Global Philanthropy Group and new associations of philanthropists such as the Global Philanthropy Forum. The Bill and Melinda Gates Foundation projects include a current multibillion-dollar commitment to funding immunizations in some of the world's more impoverished but rapidly growing countries.[112] The Hudson Institute estimates total private philanthropic flows to developing countries at US$59 billion in 2010.[113]\n\nAs a response to globalization, some countries have embraced isolationist policies. For example, the North Korean government makes it very difficult for foreigners to enter the country and strictly monitors their activities when they do. Aid workers are subject to considerable scrutiny and excluded from places and regions the government does not wish them to enter. Citizens cannot freely leave the country.[114][115]\n\nGlobalization has been a gendered process where giant multinational corporations have outsourced jobs to low-wage, low skilled, quota free economies like the ready made garment industry in Bangladesh where poor women make up the majority of labor force. Despite a large proportion of women workers in the garment industry, women are still heavily underemployed compared to men. Most women that are employed in the garment industry come from the countryside of Bangladesh triggering migration of women in search of garment work. It is still unclear as to whether or not access to paid work for women where it did not exist before has empowered them. The answers varied depending on whether it is the employers perspective or the workers and how they view their choices. Women workers did not see the garment industry as economically sustainable for them in the long run due to long hours standing and poor working conditions. Although women workers did show significant autonomy over their personal lives including their ability to negotiate with family, more choice in marriage, and being valued as a wage earner in the family. This did not translate into workers being able to collectively organize themselves in order to negotiate a better deal for themselves at work.[116]\n\nAnother example of outsourcing in manufacturing includes the maquiladora industry in Ciudad Juarez, Mexico where poor women make up the majority of the labor force. Women in the maquiladora industry have produced high levels of turnover not staying long enough to be trained compared to men. A gendered two tiered system within the maquiladora industry has been created that focuses on training and worker loyalty. Women are seen as being untrainable, placed in un-skilled, low wage jobs, while men are seen as more trainable with less turnover rates, and placed in more high skilled technical jobs. The idea of training has become a tool used against women to blame them for their high turnover rates which also benefit the industry keeping women as temporary workers.[117]\n\nScholars also occasionally discuss other, less common dimensions of globalization, such as environmental globalization (the internationally coordinated practices and regulations, often in the form of international treaties, regarding environmental protection)[118] or military globalization (growth in global extent and scope of security relationships).[119] Those dimensions, however, receive much less attention the three described above, as academic literature commonly subdivides globalization into three major areas: economic globalization, cultural globalization and political globalization.[12]\n\nAn essential aspect of globalization is movement of people, and state-boundary limits on that movement have changed across history.[120] The movement of tourists and business people opened up over the last century. As transportation technology improved, travel time and costs decreased dramatically between the 18th and early 20th century. For example, travel across the Atlantic Ocean used to take up to 5 weeks in the 18th century, but around the time of the 20th century it took a mere 8 days.[121] Today, modern aviation has made long-distance transportation quick and affordable.\n\nTourism is travel for pleasure. The developments in technology and transportation infrastructure, such as jumbo jets, low-cost airlines, and more accessible airports have made many types of tourism more affordable. At any given moment half a million people are in the air.[122] International tourist arrivals surpassed the milestone of 1 billion tourists globally for the first time in 2012.[123]\nA visa is a conditional authorization granted by a country to a foreigner, allowing them to enter and temporarily remain within, or to leave that country. Some countries – such as those in the Schengen Area – have agreements with other countries allowing each other's citizens to travel between them without visas (for example, Switzerland is part of a Schengen Agreement allowing easy travel for people from countries within the European Union). The World Tourism Organization announced that the number of tourists who require a visa before traveling was at its lowest level ever in 2015.[124]\n\nImmigration is the international movement of people into a destination country of which they are not natives or where they do not possess citizenship in order to settle or reside there, especially as permanent residents or naturalized citizens, or to take-up employment as a migrant worker or temporarily as a foreign worker.[125][126][127] According to the International Labour Organization, as of 2014[update] there were an estimated 232 million international migrants in the world (defined as persons outside their country of origin for 12 months or more) and approximately half of them were estimated to be economically active (i.e. being employed or seeking employment).[128] International movement of labor is often seen as important to economic development. For example, freedom of movement for workers in the European Union means that people can move freely between member states to live, work, study or retire in another country.\n\nGlobalization is associated with a dramatic rise in international education. The development of global cross-cultural competence in the workforce through ad-hoc training has deserved increasing attention in recent times.[130][131] More and more students are seeking higher education in foreign countries and many international students now consider overseas study a stepping-stone to permanent residency within a country.[132] The contributions that foreign students make to host nation economies, both culturally and financially has encouraged major players to implement further initiatives to facilitate the arrival and integration of overseas students, including substantial amendments to immigration and visa policies and procedures.[53]\n\nA transnational marriage is a marriage between two people from different countries. A variety of special issues arise in marriages between people from different countries, including those related to citizenship and culture, which add complexity and challenges to these kinds of relationships. In an age of increasing globalization, where a growing number of people have ties to networks of people and places across the globe, rather than to a current geographic location, people are increasingly marrying across national boundaries. Transnational marriage is a by-product of the movement and migration of people.\n\nBefore electronic communications, long-distance communications relied on mail. Speed of global communications was limited by the maximum speed of courier services (especially horses and ships) until the mid-19th century. The electric telegraph was the first method of instant long-distance communication. For example, before the first transatlantic cable, communications between Europe and the Americas took weeks because ships had to carry mail across the ocean. The first transatlantic cable reduced communication time considerably, allowing a message and a response in the same day. Lasting transatlantic telegraph connections were achieved in the 1865–1866. The first wireless telegraphy transmitters were developed in 1895.\n\nThe Internet has been instrumental in connecting people across geographical boundaries. For example, Facebook is a social networking service which has more than 1.65 billion monthly active users as of 31 March 2016[update].[134]\n\nGlobalization can be spread by Global journalism which provides massive information and relies on the internet to interact, \"makes it into an everyday routine to investigate how people and their actions, practices, problems, life conditions, etc. in different parts of the world are interrelated. possible to assume that global threats such as climate change precipitate the further establishment of global journalism.\"[135]\n\nIn the current era of globalization, the world is more interdependent than at any other time. Efficient and inexpensive transportation has left few places inaccessible, and increased global trade has brought more and more people into contact with animal diseases that have subsequently jumped species barriers (see zoonosis).[136]\n\nCoronavirus disease 2019, abbreviated COVID-19, first appeared in Wuhan, China in November 2019. More than 180 countries have reported cases since then.[137] As of April 6, 2020[update], the U.S. has the most confirmed active cases in the world.[138] More than 3.4 million people from the worst-affected countries entered the U.S. in the first three months since the inception of the COVID-19 pandemic.[139] This has caused a detrimental impact on the global economy, particularly for SME's and Microbusinesses with unlimited liability/self-employed, leaving them vulnerable to financial difficulties, increasing the market share for oligopolistic markets as well as increasing the barriers of entry.\n\nOne index of globalization is the KOF Index of Globalization, which measures three important dimensions of globalization: economic, social, and political.[140] Another is the A.T. Kearney / Foreign Policy Magazine Globalization Index.[141]\n\nMeasurements of economic globalization typically focus on variables such as trade, Foreign Direct Investment (FDI), Gross Domestic Product (GDP), portfolio investment, and income. However, newer indices attempt to measure globalization in more general terms, including variables related to political, social, cultural, and even environmental aspects of globalization.[142][143]\n\nThe DHL Global Connectedness Index studies four main types of cross-border flow: trade (in both goods and services), information, people (including tourists, students, and migrants), and capital. It shows that the depth of global integration fell by about one-tenth after 2008, but by 2013 had recovered well above its pre-crash peak.[25][64] The report also found a shift of economic activity to emerging economies.[25]\n\nReactions to processes contributing to globalization have varied widely with a history as long as extraterritorial contact and trade. Philosophical differences regarding the costs and benefits of such processes give rise to a broad-range of ideologies and social movements. Proponents of economic growth, expansion, and development, in general, view globalizing processes as desirable or necessary to the well-being of human society.[144]\n\nAntagonists view one or more globalizing processes as detrimental to social well-being on a global or local scale;[144] this includes those who focus on social or natural sustainability of long-term and continuous economic expansion, the social structural inequality caused by these processes, and the colonial, imperialistic, or hegemonic ethnocentrism, cultural assimilation and cultural appropriation that underlie such processes.\n\nGlobalization tends to bring people into contact with foreign people and cultures. Xenophobia is the fear of that which is perceived to be foreign or strange.[145][146] Xenophobia can manifest itself in many ways involving the relations and perceptions of an ingroup towards an outgroup, including a fear of losing identity, suspicion of its activities, aggression, and desire to eliminate its presence to secure a presumed purity.[147]\n\nCritiques of globalization generally stem from discussions surrounding the impact of such processes on the planet as well as the human costs. They challenge directly traditional metrics, such as GDP, and look to other measures, such as the Gini coefficient[148] or the Happy Planet Index,[149] and point to a \"multitude of interconnected fatal consequences–social disintegration, a breakdown of democracy, more rapid and extensive deterioration of the environment, the spread of new diseases, increasing poverty and alienation\"[150] which they claim are the unintended consequences of globalization. Others point out that, while the forces of globalization have led to the spread of western-style democracy, this has been accompanied by an increase in inter-ethnic tension and violence as free market economic policies combine with democratic processes of universal suffrage as well as an escalation in militarization to impose democratic principles and as a means to conflict resolution.[151]\n\nOn 9 August 2019, Pope Francis denounced isolationism and hinted that the Catholic Church will embrace globalization at the October 2019 Amazonia Synod, stating \"the whole is greater than the parts. Globalization and unity should not be conceived as a sphere, but as a polyhedron: each people retains its identity in unity with others\"[152]\n\nAs a complex and multifaceted phenomenon, globalization is considered by some as a form of capitalist expansion which entails the integration of local and national economies into a global, unregulated market economy.[153] A 2005 study by Peer Fis and Paul Hirsch found a large increase in articles negative towards globalization in the years prior. In 1998, negative articles outpaced positive articles by two to one.[154] The number of newspaper articles showing negative framing rose from about 10% of the total in 1991 to 55% of the total in 1999. This increase occurred during a period when the total number of articles concerning globalization nearly doubled.[154]\n\nA number of international polls have shown that residents of Africa and Asia tend to view globalization more favorably than residents of Europe or North America. In Africa, a Gallup poll found that 70% of the population views globalization favorably.[155] The BBC found that 50% of people believed that economic globalization was proceeding too rapidly, while 35% believed it was proceeding too slowly.[156]\n\nIn 2004, Philip Gordon stated that \"a clear majority of Europeans believe that globalization can enrich their lives, while believing the European Union can help them take advantage of globalization's benefits while shielding them from its negative effects\". The main opposition consisted of socialists, environmental groups, and nationalists. Residents of the EU did not appear to feel threatened by globalization in 2004. The EU job market was more stable and workers were less likely to accept wage/benefit cuts. Social spending was much higher than in the US.[157] In a Danish poll in 2007, 76% responded that globalization is a good thing.[158]\n\nFiss, et al., surveyed US opinion in 1993. Their survey showed that, in 1993, more than 40% of respondents were unfamiliar with the concept of globalization. When the survey was repeated in 1998, 89% of the respondents had a polarized view of globalization as being either good or bad. At the same time, discourse on globalization, which began in the financial community before shifting to a heated debate between proponents and disenchanted students and workers. Polarization increased dramatically after the establishment of the WTO in 1995; this event and subsequent protests led to a large-scale anti-globalization movement.[154]\nInitially, college educated workers were likely to support globalization. Less educated workers, who were more likely to compete with immigrants and workers in developing countries, tended to be opponents. The situation changed after the Great Recession. According to a 1997 poll 58% of college graduates said globalization had been good for the US. By 2008 only 33% thought it was good. Respondents with high school education also became more opposed.[159]\n\nAccording to Takenaka Heizo and Chida Ryokichi, as of 1998[update] there was a perception in Japan that the economy was \"Small and Frail\". However, Japan was resource-poor and used exports to pay for its raw materials. Anxiety over their position caused terms such as internationalization and globalization to enter everyday language. However, Japanese tradition was to be as self-sufficient as possible, particularly in agriculture.[160]\n\nMany in developing countries see globalization as a positive force that lifts them out of poverty.[161] Those opposing globalization typically combine environmental concerns with nationalism. Opponents consider governments as agents of neo-colonialism that are subservient to multinational corporations.[162] Much of this criticism comes from the middle class; the Brookings Institution suggested this was because the middle class perceived upwardly mobile low-income groups as threatening to their economic security.[163]\n\nThe literature analyzing the economics of free trade is extremely rich with extensive work having been done on the theoretical and empirical effects. Though it creates winners and losers, the broad consensus among economists is that free trade is a large and unambiguous net gain for society.[164][165] In a 2006 survey of 83 American economists, \"87.5% agree that the U.S. should eliminate remaining tariffs and other barriers to trade\" and \"90.1% disagree with the suggestion that the U.S. should restrict employers from outsourcing work to foreign countries.\"[166]\n\nQuoting Harvard economics professor N. Gregory Mankiw, \"Few propositions command as much consensus among professional economists as that open world trade increases economic growth and raises living standards.\"[167] In a survey of leading economists, none disagreed with the notion that \"freer trade improves productive efficiency and offers consumers better choices, and in the long run these gains are much larger than any effects on employment.\"[168] Most economists would agree that although increasing returns to scale might mean that certain industry could settle in a geographical area without any strong economic reason derived from comparative advantage, this is not a reason to argue against free trade because the absolute level of output enjoyed by both \"winner\" and \"loser\" will increase with the \"winner\" gaining more than the \"loser\" but both gaining more than before in an absolute level.\n\nIn the book The End of Poverty, Jeffrey Sachs discusses how many factors can affect a country's ability to enter the world market, including government corruption; legal and social disparities based on gender, ethnicity, or caste; diseases such as AIDS and malaria; lack of infrastructure (including transportation, communications, health, and trade); unstable political landscapes; protectionism; and geographic barriers.[169] Jagdish Bhagwati, a former adviser to the U.N. on globalization, holds that, although there are obvious problems with overly rapid development, globalization is a very positive force that lifts countries out of poverty by causing a virtuous economic cycle associated with faster economic growth.[161] However, economic growth does not necessarily mean a reduction in poverty; in fact, the two can coexist. Economic growth is conventionally measured using indicators such as GDP and GNI that do not accurately reflect the growing disparities in wealth.[170] Additionally, Oxfam International argues that poor people are often excluded from globalization-induced opportunities \"by a lack of productive assets, weak infrastructure, poor education and ill-health;\"[171] effectively leaving these marginalized groups in a poverty trap. Economist Paul Krugman is another staunch supporter of globalization and free trade with a record of disagreeing with many critics of globalization. He argues that many of them lack a basic understanding of comparative advantage and its importance in today's world.[172]\n\nThe flow of migrants to advanced economies has been claimed to provide a means through which global wages converge. An IMF study noted a potential for skills to be transferred back to developing countries as wages in those a countries rise.[11] Lastly, the dissemination of knowledge has been an integral aspect of globalization. Technological innovations (or technological transfer) are conjectured to benefit most developing and least developing countries (LDCs), as for example in the adoption of mobile phones.[60]\n\nThere has been a rapid economic growth in Asia after embracing market orientation-based economic policies that encourage private property rights, free enterprise and competition. In particular, in East Asian developing countries, GDP per head rose at 5.9% a year from 1975 to 2001 (according to 2003 Human Development Report[173] of UNDP). Like this, the British economic journalist Martin Wolf says that incomes of poor developing countries, with more than half the world's population, grew substantially faster than those of the world's richest countries that remained relatively stable in its growth, leading to reduced international inequality and the incidence of poverty.\n\nCertain demographic changes in the developing world after active economic liberalization and international integration resulted in rising general welfare and, hence, reduced inequality. According to Wolf, in the developing world as a whole, life expectancy rose by four months each year after 1970 and infant mortality rate declined from 107 per thousand in 1970 to 58 in 2000 due to improvements in standards of living and health conditions. Also, adult literacy in developing countries rose from 53% in 1970 to 74% in 1998 and much lower illiteracy rate among the young guarantees that rates will continue to fall as time passes. Furthermore, the reduction in fertility rate in the developing world as a whole from 4.1 births per woman in 1980 to 2.8 in 2000 indicates improved education level of women on fertility, and control of fewer children with more parental attention and investment.[175] Consequently, more prosperous and educated parents with fewer children have chosen to withdraw their children from the labor force to give them opportunities to be educated at school improving the issue of child labor. Thus, despite seemingly unequal distribution of income within these developing countries, their economic growth and development have brought about improved standards of living and welfare for the population as a whole.\n\nPer capita gross domestic product (GDP) growth among post-1980 globalizing countries accelerated from 1.4 percent a year in the 1960s and 2.9 percent a year in the 1970s to 3.5 percent in the 1980s and 5.0 percent in the 1990s. This acceleration in growth seems even more remarkable given that the rich countries saw steady declines in growth from a high of 4.7 percent in the 1960s to 2.2 percent in the 1990s. Also, the non-globalizing developing countries seem to fare worse than the globalizers, with the former's annual growth rates falling from highs of 3.3 percent during the 1970s to only 1.4 percent during the 1990s. This rapid growth among the globalizers is not simply due to the strong performances of China and India in the 1980s and 1990s—18 out of the 24 globalizers experienced increases in growth, many of them quite substantial.[176]\n\nThe globalization of the late 20th and early 21st centuries has led to the resurfacing of the idea that the growth of economic interdependence promotes peace.[177] This idea had been very powerful during the globalization of the late 19th and early 20th centuries, and was a central doctrine of classical liberals of that era, such as the young John Maynard Keynes (1883–1946).[178]\n\nSome opponents of globalization see the phenomenon as a promotion of corporate interests.[179] They also claim that the increasing autonomy and strength of corporate entities shapes the political policy of countries.[180][181] They advocate global institutions and policies that they believe better address the moral claims of poor and working classes as well as environmental concerns.[182] Economic arguments by fair trade theorists claim that unrestricted free trade benefits those with more financial leverage (i.e. the rich) at the expense of the poor.[183]\n\nGlobalization allows corporations to outsource manufacturing and service jobs from high-cost locations, creating economic opportunities with the most competitive wages and worker benefits.[81] Critics of globalization say that it disadvantages poorer countries. While it is true that free trade encourages globalization among countries, some countries try to protect their domestic suppliers. The main export of poorer countries is usually agricultural productions. Larger countries often subsidize their farmers (e.g., the EU's Common Agricultural Policy), which lowers the market price for foreign crops.[184]\n\nDemocratic globalization is a movement towards an institutional system of global democracy that would give world citizens a say in political organizations. This would, in their view, bypass nation-states, corporate oligopolies, ideological non-governmental organizations (NGO), political cults and mafias. One of its most prolific proponents is the British political thinker David Held. Advocates of democratic globalization argue that economic expansion and development should be the first phase of democratic globalization, which is to be followed by a phase of building global political institutions. Francesco Stipo, Director of the United States Association of the Club of Rome, advocates unifying nations under a world government, suggesting that it \"should reflect the political and economic balances of world nations. A world confederation would not supersede the authority of the State governments but rather complement it, as both the States and the world authority would have power within their sphere of competence\".[185] Former Canadian Senator Douglas Roche, O.C., viewed globalization as inevitable and advocated creating institutions such as a directly elected United Nations Parliamentary Assembly to exercise oversight over unelected international bodies.[186]\n\nGlobal civics suggests that civics can be understood, in a global sense, as a social contract between global citizens in the age of interdependence and interaction. The disseminators of the concept define it as the notion that we have certain rights and responsibilities towards each other by the mere fact of being human on Earth.[187] World citizen has a variety of similar meanings, often referring to a person who disapproves of traditional geopolitical divisions derived from national citizenship. An early incarnation of this sentiment can be found in Socrates, whom Plutarch quoted as saying: \"I am not an Athenian, or a Greek, but a citizen of the world.\"[188] In an increasingly interdependent world, world citizens need a compass to frame their mindsets and create a shared consciousness and sense of global responsibility in world issues such as environmental problems and nuclear proliferation.[189]\n\nBaha'i-inspired author Meyjes, while favoring the single world community and emergent global consciousness, warns of globalization[190] as a cloak for an expeditious economic, social, and cultural Anglo-dominance that is insufficiently inclusive to inform the emergence of an optimal world civilization. He proposes a process of \"universalization\" as an alternative.\n\nCosmopolitanism is the proposal that all human ethnic groups belong to a single community based on a shared morality. A person who adheres to the idea of cosmopolitanism in any of its forms is called a cosmopolitan or cosmopolite.[191] A cosmopolitan community might be based on an inclusive morality, a shared economic relationship, or a political structure that encompasses different nations. The cosmopolitan community is one in which individuals from different places (e.g. nation-states) form relationships based on mutual respect. For instance, Kwame Anthony Appiah suggests the possibility of a cosmopolitan community in which individuals from varying locations (physical, economic, etc.) enter relationships of mutual respect despite their differing beliefs (religious, political, etc.).[192]\n\nCanadian philosopher Marshall McLuhan popularized the term Global Village beginning in 1962.[193] His view suggested that globalization would lead to a world where people from all countries will become more integrated and aware of common interests and shared humanity.[194]\n\nMilitary cooperation – Past examples of international cooperation exist. One example is the security cooperation between the United States and the former Soviet Union after the end of the Cold War, which astonished international society. Arms control and disarmament agreements, including the Strategic Arms Reduction Treaty (see START I, START II, START III, and New START) and the establishment of NATO's Partnership for Peace, the Russia NATO Council, and the G8 Global Partnership against the Spread of Weapons and Materials of Mass Destruction, constitute concrete initiatives of arms control and de-nuclearization. The US–Russian cooperation was further strengthened by anti-terrorism agreements enacted in the wake of 9/11.[195]\n\nEnvironmental cooperation – One of the biggest successes of environmental cooperation has been the agreement to reduce chlorofluorocarbon (CFC) emissions, as specified in the Montreal Protocol, in order to stop ozone depletion. The most recent debate around nuclear energy and the non-alternative coal-burning power plants constitutes one more consensus on what not to do. Thirdly, significant achievements in IC can be observed through development studies.[195]\n\nEconomic cooperation – One of the biggest challenges in 2019 with globalization is that many believe the progress made in the past decades are now back tracking. The back tracking of globalization has coined the term \"Slobalization.\" Slobalization is a new, slower pattern of globalization.[196]\n\nAnti-globalization, or counter-globalization,[197] consists of a number of criticisms of globalization but, in general, is critical of the globalization of corporate capitalism.[198] The movement is also commonly referred to as the alter-globalization movement, anti-globalist movement, anti-corporate globalization movement,[199] or movement against neoliberal globalization. Opponents of globalization argue that power and respect in terms of international trade between the developed and underdeveloped countries of the world are unequally distributed.[200] The diverse subgroups that make up this movement include some of the following: trade unionists, environmentalists, anarchists, land rights and indigenous rights activists, organizations promoting human rights and sustainable development, opponents of privatization, and anti-sweatshop campaigners.[201]\n\nIn The Revolt of the Elites and the Betrayal of Democracy, Christopher Lasch analyzed[202] the widening gap between the top and bottom of the social composition in the United States. For him, our epoch is determined by a social phenomenon: the revolt of the elites, in reference to The Revolt of the Masses (1929) by the Spanish philosopher José Ortega y Gasset. According to Lasch, the new elites, i.e. those who are in the top 20% in terms of income, through globalization which allows total mobility of capital, no longer live in the same world as their fellow-citizens. In this, they oppose the old bourgeoisie of the nineteenth and twentieth centuries, which was constrained by its spatial stability to a minimum of rooting and civic obligations. Globalization, according to the sociologist, has turned elites into tourists in their own countries. The denationalization of business enterprise tends to produce a class who see themselves as \"world citizens, but without accepting ... any of the obligations that citizenship in a polity normally implies\". Their ties to an international culture of work, leisure, information – make many of them deeply indifferent to the prospect of national decline. Instead of financing public services and the public treasury, new elites are investing their money in improving their voluntary ghettos: private schools in their residential neighborhoods, private police, garbage collection systems. They have \"withdrawn from common life\". Composed of those who control the international flows of capital and information, who preside over philanthropic foundations and institutions of higher education, manage the instruments of cultural production and thus fix the terms of public debate. So, the political debate is limited mainly to the dominant classes and political ideologies lose all contact with the concerns of the ordinary citizen. The result of this is that no one has a likely solution to these problems and that there are furious ideological battles on related issues. However, they remain protected from the problems affecting the working classes: the decline of industrial activity, the resulting loss of employment, the decline of the middle class, increasing the number of the poor, the rising crime rate, growing drug trafficking, the urban crisis.\n\nD.A. Snow et al. contend that the anti-globalization movement is an example of a new social movement, which uses tactics that are unique and use different resources than previously used before in other social movements.[203]\n\nOne of the most infamous tactics of the movement is the Battle of Seattle in 1999, where there were protests against the World Trade Organization's Third Ministerial Meeting. All over the world, the movement has held protests outside meetings of institutions such as the WTO, the International Monetary Fund (IMF), the World Bank, the World Economic Forum, and the Group of Eight (G8).[201] Within the Seattle demonstrations the protesters that participated used both creative and violent tactics to gain the attention towards the issue of globalization.\n\nCapital markets have to do with raising and investing money in various human enterprises. Increasing integration of these financial markets between countries leads to the emergence of a global capital marketplace or a single world market. In the long run, increased movement of capital between countries tends to favor owners of capital more than any other group; in the short run, owners and workers in specific sectors in capital-exporting countries bear much of the burden of adjusting to increased movement of capital.[204]\n\nThose opposed to capital market integration on the basis of human rights issues are especially disturbed[according to whom?] by the various abuses which they think are perpetuated by global and international institutions that, they say, promote neoliberalism without regard to ethical standards. Common targets include the World Bank (WB), International Monetary Fund (IMF), the Organisation for Economic Co-operation and Development (OECD) and the World Trade Organization (WTO) and free trade treaties like the North American Free Trade Agreement (NAFTA), Free Trade Area of the Americas (FTAA), the Multilateral Agreement on Investment (MAI) and the General Agreement on Trade in Services (GATS). In light of the economic gap between rich and poor countries, movement adherents claim free trade without measures in place to protect the under-capitalized will contribute only to the strengthening the power of industrialized nations (often termed the \"North\" in opposition to the developing world's \"South\").[205][better source needed]\n\nCorporatist ideology, which privileges the rights of corporations (artificial or juridical persons) over those of natural persons, is an underlying factor in the recent rapid expansion of global commerce.[206] In recent years, there have been an increasing number of books (Naomi Klein's 2000 No Logo, for example) and films (e.g. The Corporation & Surplus) popularizing an anti-corporate ideology to the public.\n\nA related contemporary ideology, consumerism, which encourages the personal acquisition of goods and services, also drives globalization.[207] Anti-consumerism is a social movement against equating personal happiness with consumption and the purchase of material possessions. Concern over the treatment of consumers by large corporations has spawned substantial activism, and the incorporation of consumer education into school curricula. Social activists hold materialism is connected to global retail merchandizing and supplier convergence, war, greed, anomie, crime, environmental degradation, and general social malaise and discontent. One variation on this topic is activism by postconsumers, with the strategic emphasis on moving beyond addictive consumerism.[208]\n\nThe global justice movement is the loose collection of individuals and groups—often referred to as a \"movement of movements\"—who advocate fair trade rules and perceive current institutions of global economic integration as problems.[210] The movement is often labeled an anti-globalization movement by the mainstream media. Those involved, however, frequently deny that they are anti-globalization, insisting that they support the globalization of communication and people and oppose only the global expansion of corporate power.[211] The movement is based in the idea of social justice, desiring the creation of a society or institution based on the principles of equality and solidarity, the values of human rights, and the dignity of every human being.[212][213][214] Social inequality within and between nations, including a growing global digital divide, is a focal point of the movement. Many nongovernmental organizations have now arisen to fight these inequalities that many in Latin America, Africa and Asia face. A few very popular and well known non-governmental organizations (NGOs) include: War Child, Red Cross, Free The Children and CARE International. They often create partnerships where they work towards improving the lives of those who live in developing countries by building schools, fixing infrastructure, cleaning water supplies, purchasing equipment and supplies for hospitals, and other aid efforts.\n\nThe economies of the world have developed unevenly, historically, such that entire geographical regions were left mired in poverty and disease while others began to reduce poverty and disease on a wholesale basis. From around 1980 through at least 2011, the GDP gap, while still wide, appeared to be closing and, in some more rapidly developing countries, life expectancies began to rise.[215] If we look at the Gini coefficient for world income, since the late 1980s, the gap between some regions has markedly narrowed—between Asia and the advanced economies of the West, for example—but huge gaps remain globally. Overall equality across humanity, considered as individuals, has improved very little. Within the decade between 2003 and 2013, income inequality grew even in traditionally egalitarian countries like Germany, Sweden and Denmark. With a few exceptions—France, Japan, Spain—the top 10 percent of earners in most advanced economies raced ahead, while the bottom 10 percent fell further behind.[216] By 2013, 85 multibillionaires had amassed wealth equivalent to all the wealth owned by the poorest half (3.5 billion) of the world's total population of 7 billion.[217]\n\nCritics of globalization argue that globalization results in weak labor unions: the surplus in cheap labor coupled with an ever-growing number of companies in transition weakened labor unions in high-cost areas. Unions become less effective and workers their enthusiasm for unions when membership begins to decline.[184] They also cite an increase in the exploitation of child labor: countries with weak protections for children are vulnerable to infestation by rogue companies and criminal gangs who exploit them. Examples include quarrying, salvage, and farm work as well as trafficking, bondage, forced labor, prostitution and pornography.[218]\n\nWomen often participate in the workforce in precarious work, including export-oriented employment. Evidence suggests that while globalization has expanded women's access to employment, the long-term goal[whose?] of transforming gender inequalities remains unmet and appears unattainable without regulation of capital and a reorientation and expansion of the state's role in funding public goods and providing a social safety net.[219] Furthermore, the intersectionality of gender, race, class, can be overlooked by scholars and commentators when assessing the impact of globalization.[220]\n\nIn 2016, a study published by the IMF posited that neoliberalism, the ideological backbone of contemporary globalized capitalism, has been \"oversold\", with the benefits of neoliberal policies being \"fairly difficult to establish when looking at a broad group of countries\" and the costs, most significantly higher income inequality within nations, \"hurt the level and sustainability of growth.\"[221]\n\nBeginning in the 1930s, opposition arose to the idea of a world government, as advocated by organizations such as the World Federalist Movement (WFM). Those who oppose global governance typically do so on objections that the idea is unfeasible, inevitably oppressive, or simply unnecessary.[222] In general, these opponents are wary of the concentration of power or wealth that such governance might represent. Such reasoning dates back to the founding of the League of Nations and, later, the United Nations.\n\nEnvironmentalism is a broad philosophy, ideology[223][224][225] and social movement regarding concerns for environmental conservation and improvement of the health of the environment. Environmentalist concerns with globalization include issues such as global warming, global water supply and water crises, inequity in energy consumption and energy conservation, transnational air pollution and pollution of the world ocean, overpopulation, world habitat sustainability, deforestation, biodiversity loss and species extinction.\n\nOne critique of globalization is that natural resources of the poor have been systematically taken over by the rich and the pollution promulgated by the rich is systematically dumped on the poor.[226] Some argue that Northern corporations are increasingly exploiting resources of less wealthy countries for their global activities while it is the South that is disproportionately bearing the environmental burden of the globalized economy. Globalization is thus leading to a type of\" environmental apartheid\".[227]\n\nHelena Norberg-Hodge, the director and founder of Local Futures/International Society for Ecology and Culture, criticizes globalization in many ways. In her book Ancient Futures, Norberg-Hodge claims that \"centuries of ecological balance and social harmony are under threat from the pressures of development and globalization.\" She also criticizes the standardization and rationalization of globalization, as it does not always yield the expected growth outcomes. Although globalization takes similar steps in most countries, scholars such as Hodge claim that it might not be effective to certain countries and that globalization has actually moved some countries backward instead of developing them.[228]\n\nA related area of concern is the pollution haven hypothesis, which posits that, when large industrialized nations seek to set up factories or offices abroad, they will often look for the cheapest option in terms of resources and labor that offers the land and material access they require (see Race to the bottom).[229] This often comes at the cost of environmentally sound practices. Developing countries with cheap resources and labor tend to have less stringent environmental regulations, and conversely, nations with stricter environmental regulations become more expensive for companies as a result of the costs associated with meeting these standards. Thus, companies that choose to physically invest in foreign countries tend to (re)locate to the countries with the lowest environmental standards or weakest enforcement.\n\nThe European Union–Mercosur Free Trade Agreement, which would form one of the world's largest free trade areas,[230] has been denounced by environmental activists and indigenous rights campaigners.[231] The fear is that the deal could lead to more deforestation of the Amazon rainforest as it expands market access to Brazilian beef.[232]\n"
    },
    {
        "title": "Blockchain",
        "url": "https://en.wikipedia.org/wiki/Blockchain",
        "content": "\n\nA blockchain is a distributed ledger with growing lists of records (blocks) that are securely linked together via cryptographic hashes.[1][2][3][4] Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a Merkle tree, where data nodes are represented by leaves). Since each block contains information about the previous block, they effectively form a chain (compare linked list data structure), with each additional block linking to the ones before it. Consequently, blockchain transactions are resistant to alteration because, once recorded, the data in any given block cannot be changed retroactively without altering all subsequent blocks and obtaining network consensus to accept these changes. This protects blockchains against nefarious activities such as creating assets \"out of thin air\", double-spending, counterfeiting, fraud, and theft.[5][6]\n\nBlockchains are typically managed by a peer-to-peer (P2P) computer network for use as a public distributed ledger, where nodes collectively adhere to a consensus algorithm protocol to add and validate new transaction blocks. Although blockchain records are not unalterable, since blockchain forks are possible, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance.[7]\n\nA blockchain was created by a person (or group of people) using the name (or pseudonym) Satoshi Nakamoto in 2008 to serve as the public distributed ledger for bitcoin cryptocurrency transactions, based on previous work by Stuart Haber, W. Scott Stornetta, and Dave Bayer.[8] The implementation of the blockchain within bitcoin made it the first digital currency to solve the double-spending problem without the need for a trusted authority or central server. The bitcoin design has inspired other applications[3][2] and blockchains that are readable by the public and are widely used by cryptocurrencies. The blockchain may be considered a type of payment rail.[9]\n\nPrivate blockchains have been proposed for business use. Computerworld called the marketing of such privatized blockchains without a proper security model \"snake oil\";[10] however, others have argued that permissioned blockchains, if carefully designed, may be more decentralized and therefore more secure in practice than permissionless ones.[4][11]\n\nCryptographer David Chaum first proposed a blockchain-like protocol in his 1982 dissertation \"Computer Systems Established, Maintained, and Trusted by Mutually Suspicious Groups\".[12] Further work on a cryptographically secured chain of blocks was described in 1991 by Stuart Haber and W. Scott Stornetta.[4][13] They wanted to implement a system wherein document timestamps could not be tampered with. In 1992, Haber, Stornetta, and Dave Bayer incorporated Merkle trees into the design, which improved its efficiency by allowing several document certificates to be collected into one block.[4][14] Under their company Surety, their document certificate hashes have been published in The New York Times every week since 1995.[15]\n\nThe first decentralized blockchain was conceptualized by a person (or group of people) known as Satoshi Nakamoto in 2008. Nakamoto improved the design in an important way using a Hashcash-like method to timestamp blocks without requiring them to be signed by a trusted party and introducing a difficulty parameter to stabilize the rate at which blocks are added to the chain.[4] The design was implemented the following year by Nakamoto as a core component of the cryptocurrency bitcoin, where it serves as the public ledger for all transactions on the network.[3]\n\nIn August 2014, the bitcoin blockchain file size, containing records of all transactions that have occurred on the network, reached 20 GB (gigabytes).[16] In January 2015, the size had grown to almost 30 GB, and from January 2016 to January 2017, the bitcoin blockchain grew from 50 GB to 100 GB in size. The ledger size had exceeded 200 GB by early 2020.[17]\n\nThe words block and chain were used separately in Satoshi Nakamoto's original paper, but were eventually popularized as a single word, blockchain, by 2016.[18]\n\nAccording to Accenture, an application of the diffusion of innovations theory suggests that blockchains attained a 13.5% adoption rate within financial services in 2016, therefore reaching the early adopters' phase.[19] Industry trade groups joined to create the Global Blockchain Forum in 2016, an initiative of the Chamber of Digital Commerce.\n\nIn May 2018, Gartner found that only 1% of CIOs indicated any kind of blockchain adoption within their organisations, and only 8% of CIOs were in the short-term \"planning or [looking at] active experimentation with blockchain\".[20] For the year 2019 Gartner reported 5% of CIOs believed blockchain technology was a 'game-changer' for their business.[21]\n\nA blockchain is a decentralized, distributed, and often public, digital ledger consisting of records called blocks that are used to record transactions across many computers so that any involved block cannot be altered retroactively, without the alteration of all subsequent blocks.[3][22] This allows the participants to verify and audit transactions independently and relatively inexpensively.[23] A blockchain database is managed autonomously using a peer-to-peer network and a distributed timestamping server. They are authenticated by mass collaboration powered by collective self-interests.[24] Such a design facilitates robust workflow where participants' uncertainty regarding data security is marginal. The use of a blockchain removes the characteristic of infinite reproducibility from a digital asset. It confirms that each unit of value was transferred only once, solving the long-standing problem of double-spending. A blockchain has been described as a value-exchange protocol.[25] A blockchain can maintain title rights because, when properly set up to detail the exchange agreement, it provides a record that compels offer and acceptance.[citation needed]\n\nLogically, a blockchain can be seen as consisting of several layers:[26]\n\nBlocks hold batches of valid transactions that are hashed and encoded into a Merkle tree.[3] Each block includes the cryptographic hash of the prior block in the blockchain, linking the two. The linked blocks form a chain.[3] This iterative process confirms the integrity of the previous block, all the way back to the initial block, which is known as the genesis block (Block 0).[28][29] To assure the integrity of a block and the data contained in it, the block is usually digitally signed.[30]\n\nSometimes separate blocks can be produced concurrently, creating a temporary fork. In addition to a secure hash-based history, any blockchain has a specified algorithm for scoring different versions of the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks.[29] Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of history forever. Blockchains are typically built to add the score of new blocks onto old blocks and are given incentives to extend with new blocks rather than overwrite old blocks. Therefore, the probability of an entry becoming superseded decreases exponentially[31] as more blocks are built on top of it, eventually becoming very low.[3][32]: ch. 08 [33] For example, bitcoin uses a proof-of-work system, where the chain with the most cumulative proof-of-work is considered the valid one by the network. There are a number of methods that can be used to demonstrate a sufficient level of computation. Within a blockchain the computation is carried out redundantly rather than in the traditional segregated and parallel manner.[34]\n\nThe block time is the average time it takes for the network to generate one extra block in the blockchain. By the time of block completion, the included data becomes verifiable. In cryptocurrency, this is practically when the transaction takes place, so a shorter block time means faster transactions. The block time for Ethereum is set to between 14 and 15 seconds, while for bitcoin it is on average 10 minutes.[35]\n\nA hard fork is a change to the blockchain protocol that is not backward compatible and requires all users to upgrade their software in order to continue participating in the network. In a hard fork, the network splits into two separate versions: one that follows the new rules and one that follows the old rules.\n\nFor example, Ethereum was hard forked in 2016 to \"make whole\" the investors in The DAO, which had been hacked by exploiting a vulnerability in its code. In this case, the fork resulted in a split creating Ethereum and Ethereum Classic chains. In 2014 the Nxt community was asked to consider a hard fork that would have led to a rollback of the blockchain records to mitigate the effects of a theft of 50 million NXT from a major cryptocurrency exchange. The hard fork proposal was rejected, and some of the funds were recovered after negotiations and ransom payment. Alternatively, to prevent a permanent split, a majority of nodes using the new software may return to the old rules, as was the case of bitcoin split on 12 March 2013.[36]\n\nBy storing data across its peer-to-peer network, the blockchain eliminates some risks that come with data being held centrally.[3] The decentralized blockchain may use ad hoc message passing and distributed networking.[39]\n\nIn a so-called \"51% attack\" a central entity gains control of more than half of a network and can then manipulate that specific blockchain record at will, allowing double-spending.[40]\n\nBlockchain security methods include the use of public-key cryptography.[41]: 5  A public key (a long, random-looking string of numbers) is an address on the blockchain. Value tokens sent across the network are recorded as belonging to that address. A private key is like a password that gives its owner access to their digital assets or the means to otherwise interact with the various capabilities that blockchains now support. Data stored on the blockchain is generally considered incorruptible.[3]\n\nEvery node in a decentralized system has a copy of the blockchain. Data quality is maintained by massive database replication[42] and computational trust. No centralized \"official\" copy exists and no user is \"trusted\" more than any other.[41] Transactions are broadcast to the network using the software. Messages are delivered on a best-effort basis. Early blockchains rely on energy-intensive mining nodes to validate transactions,[29] add them to the block they are building, and then broadcast the completed block to other nodes.[32]: ch. 08  Blockchains use various time-stamping schemes, such as proof-of-work, to serialize changes.[43] Later consensus methods include proof of stake.[29] The growth of a decentralized blockchain is accompanied by the risk of centralization because the computer resources required to process larger amounts of data become more expensive.[44]\n\nFinality is the level of confidence that the well-formed block recently appended to the blockchain will not be revoked in the future (is \"finalized\") and thus can be trusted. Most distributed blockchain protocols, whether proof of work or proof of stake, cannot guarantee the finality of a freshly committed block, and instead rely on \"probabilistic finality\": as the block goes deeper into a blockchain, it is less likely to be altered or reverted by a newly found consensus.[45]\n\nByzantine fault tolerance-based proof-of-stake protocols purport to provide so called \"absolute finality\": a randomly chosen validator proposes a block, the rest of validators vote on it, and, if a supermajority decision approves it, the block is irreversibly committed into the blockchain.[45] A modification of this method, an \"economic finality\", is used in practical protocols, like the Casper protocol used in Ethereum: validators which sign two different blocks at the same position in the blockchain are subject to \"slashing\", where their leveraged stake is forfeited.[45]\n\nOpen blockchains are more user-friendly than some traditional ownership records, which, while open to the public, still require physical access to view. Because all early blockchains were permissionless, controversy has arisen over the blockchain definition. An issue in this ongoing debate is whether a private system with verifiers tasked and authorized (permissioned) by a central authority should be considered a blockchain.[46][47][48][49][50] Proponents of permissioned or private chains argue that the term \"blockchain\" may be applied to any data structure that batches data into time-stamped blocks. These blockchains serve as a distributed version of multiversion concurrency control (MVCC) in databases.[51] Just as MVCC prevents two transactions from concurrently modifying a single object in a database, blockchains prevent two transactions from spending the same single output in a blockchain.[52]: 30–31  Opponents say that permissioned systems resemble traditional corporate databases, not supporting decentralized data verification, and that such systems are not hardened against operator tampering and revision.[46][48] Nikolai Hampton of Computerworld said that \"many in-house blockchain solutions will be nothing more than cumbersome databases,\" and \"without a clear security model, proprietary blockchains should be eyed with suspicion.\"[10][53]\n\nAn advantage to an open, permissionless, or public, blockchain network is that guarding against bad actors is not required and no access control is needed.[31] This means that applications can be added to the network without the approval or trust of others, using the blockchain as a transport layer.[31]\n\nBitcoin and other cryptocurrencies currently secure their blockchain by requiring new entries to include proof of work. To prolong the blockchain, bitcoin uses Hashcash puzzles. While Hashcash was designed in 1997 by Adam Back, the original idea was first proposed by Cynthia Dwork and Moni Naor and Eli Ponyatovski in their 1992 paper \"Pricing via Processing or Combatting Junk Mail\".\n\nIn 2016, venture capital investment for blockchain-related projects was weakening in the USA but increasing in China.[54] Bitcoin and many other cryptocurrencies use open (public) blockchains. As of April 2018[update], bitcoin has the highest market capitalization.\n\nPermissioned blockchains use an access control layer to govern who has access to the network.[55] It has been argued that permissioned blockchains can guarantee a certain level of decentralization, if carefully designed, as opposed to permissionless blockchains, which are often centralized in practice.[11]\n\nNikolai Hampton argued in Computerworld that \"There is also no need for a '51 percent' attack on a private blockchain, as the private blockchain (most likely) already controls 100 percent of all block creation resources. If you could attack or damage the blockchain creation tools on a private corporate server, you could effectively control 100 percent of their network and alter transactions however you wished.\"[10] This has a set of particularly profound adverse implications during a financial crisis or debt crisis like the financial crisis of 2007–08, where politically powerful actors may make decisions that favor some groups at the expense of others,[56] and \"the bitcoin blockchain is protected by the massive group mining effort. It's unlikely that any private blockchain will try to protect records using gigawatts of computing power — it's time-consuming and expensive.\"[10] He also said, \"Within a private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\"[10]\n\nThe analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies.[57] A blockchain, if it is public, provides anyone who wants access to observe and analyse the chain data, given one has the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks.[58][59] The reason for this is accusations of blockchain-enabled cryptocurrencies enabling illicit dark market trading of drugs, weapons, money laundering, etc.[60] A common belief has been that cryptocurrency is private and untraceable, thus leading many actors to use it for illegal purposes. This is changing now that specialised tech companies provide blockchain tracking services, making crypto exchanges, law-enforcement and banks more aware of what is happening with crypto funds and fiat-crypto exchanges. The development, some argue, has led criminals to prioritise the use of new cryptos such as Monero.[61][62][63]\n\nIn April 2016, Standards Australia submitted a proposal to the International Organization for Standardization to consider developing standards to support blockchain technology. This proposal resulted in the creation of ISO Technical Committee 307, Blockchain and Distributed Ledger Technologies.[64] The technical committee has working groups relating to blockchain terminology, reference architecture, security and privacy, identity, smart contracts, governance and interoperability for blockchain and DLT, as well as standards specific to industry sectors and generic government requirements.[65][non-primary source needed] More than 50 countries are participating in the standardization process together with external liaisons such as the Society for Worldwide Interbank Financial Telecommunication (SWIFT), the European Commission, the International Federation of Surveyors, the International Telecommunication Union (ITU) and the United Nations Economic Commission for Europe (UNECE).[65]\n\nMany other national standards bodies and open standards bodies are also working on blockchain standards.[66] These include the National Institute of Standards and Technology[67] (NIST), the European Committee for Electrotechnical Standardization[68] (CENELEC), the Institute of Electrical and Electronics Engineers[69] (IEEE), the Organization for the Advancement of Structured Information Standards (OASIS), and some individual participants in the Internet Engineering Task Force[70] (IETF).\n\nAlthough most of blockchain implementation are decentralized and distributed, Oracle launched a centralized blockchain table feature in Oracle 21c database. The Blockchain Table in Oracle 21c database is a centralized blockchain which provide immutable feature. Compared to decentralized blockchains, centralized blockchains normally can provide a higher throughput and lower latency of transactions than consensus-based distributed blockchains.[71][72]\n\nCurrently, there are at least four types of blockchain networks — public blockchains, private blockchains, consortium blockchains and hybrid blockchains.\n\nA public blockchain has absolutely no access restrictions. Anyone with an Internet connection can send transactions to it as well as become a validator (i.e., participate in the execution of a consensus protocol).[73][self-published source?] Usually, such networks offer economic incentives for those who secure them and utilize some type of a proof-of-stake or proof-of-work algorithm.\n\nSome of the largest, most known public blockchains are the bitcoin blockchain and the Ethereum blockchain.\n\nA private blockchain is permissioned.[55] One cannot join it unless invited by the network administrators. Participant and validator access is restricted. To distinguish between open blockchains and other peer-to-peer decentralized database applications that are not open ad-hoc compute clusters, the terminology Distributed Ledger (DLT) is normally used for private blockchains.\n\nA hybrid blockchain has a combination of centralized and decentralized features.[74] The exact workings of the chain can vary based on which portions of centralization and decentralization are used.\n\nA sidechain is a designation for a blockchain ledger that runs in parallel to a primary blockchain.[75][76] Entries from the primary blockchain (where said entries typically represent digital assets) can be linked to and from the sidechain; this allows the sidechain to otherwise operate independently of the primary blockchain (e.g., by using an alternate means of record keeping, alternate consensus algorithm, etc.).[77][better source needed]\n\nA consortium blockchain is a type of blockchain that combines elements of both public and private blockchains. In a consortium blockchain, a group of organizations come together to create and operate the blockchain, rather than a single entity. The consortium members jointly manage the blockchain network and are responsible for validating transactions. Consortium blockchains are permissioned, meaning that only certain individuals or organizations are allowed to participate in the network. This allows for greater control over who can access the blockchain and helps to ensure that sensitive information is kept confidential.\n\nConsortium blockchains are commonly used in industries where multiple organizations need to collaborate on a common goal, such as supply chain management or financial services. One advantage of consortium blockchains is that they can be more efficient and scalable than public blockchains, as the number of nodes required to validate transactions is typically smaller. Additionally, consortium blockchains can provide greater security and reliability than private blockchains, as the consortium members work together to maintain the network. Some examples of consortium blockchains include Quorum and Hyperledger.[78]\n\nBlockchain technology can be integrated into multiple areas. The primary use of blockchains is as a distributed ledger for cryptocurrencies such as bitcoin; there were also a few other operational products that had matured from proof of concept by late 2016.[54] As of 2016, some businesses have been testing the technology and conducting low-level implementation to gauge blockchain's effects on organizational efficiency in their back office.[79]\n\nBlockchain is seen as a pivotal technological advancement of the 21st century, with the ability to impact organizations at strategic, operational, and market levels.[80] In 2019, it was estimated that around $2.9 billion were invested in blockchain technology, which represents an 89% increase from the year prior. Additionally, the International Data Corp estimated that corporate investment into blockchain technology would reach $12.4 billion by 2022.[81] Furthermore, According to PricewaterhouseCoopers (PwC), the second-largest professional services network in the world, blockchain technology has the potential to generate an annual business value of more than $3 trillion by 2030. PwC's estimate is further augmented by a 2018 study that they have conducted, in which PwC surveyed 600 business executives and determined that 84% have at least some exposure to utilizing blockchain technology, which indicates a significant demand and interest in blockchain technology.[82]\n\nIn 2019, the BBC World Service radio and podcast series Fifty Things That Made the Modern Economy identified blockchain as a technology that would have far-reaching consequences for economics and society. The economist and Financial Times journalist and broadcaster Tim Harford discussed why the underlying technology might have much wider applications and the challenges that needed to be overcome.[83] His first broadcast was on 29 June 2019.\n\nThe number of blockchain wallets quadrupled to 40 million between 2016 and 2020.[84]\n\nA paper published in 2022 discussed the potential use of blockchain technology in sustainable management.[85]\n\nMost cryptocurrencies use blockchain technology to record transactions. For example, the bitcoin network and Ethereum network are both based on blockchain.\n\nThe criminal enterprise Silk Road, which operated on Tor, utilized cryptocurrency for payments, some of which the US federal government seized through research on the blockchain and forfeiture.[86]\n\nGovernments have mixed policies on the legality of their citizens or banks owning cryptocurrencies. China implements blockchain technology in several industries including a national digital currency which launched in 2020.[87] To strengthen their respective currencies, Western governments including the European Union and the United States have initiated similar projects.[88]\n\nBlockchain-based smart contracts are contracts that can be partially or fully executed or enforced without human interaction.[89] One of the main objectives of a smart contract is automated escrow. A key feature of smart contracts is that they do not need a trusted third party (such as a trustee) to act as an intermediary between contracting entities — the blockchain network executes the contract on its own. This may reduce friction between entities when transferring value and could subsequently open the door to a higher level of transaction automation.[90] An IMF staff discussion from 2018 reported that smart contracts based on blockchain technology might reduce moral hazards and optimize the use of contracts in general, but \"no viable smart contract systems have yet emerged.\" Due to the lack of widespread use, their legal status was unclear.[91][92]\n\nAccording to Reason, many banks have expressed interest in implementing distributed ledgers for use in banking and are cooperating with companies creating private blockchains;[93][94][95] according to a September 2016 IBM study, it is occurring faster than expected.[96] It has been estimated by the World Economic Forum that by 2025, 10% of the world's GDP will be stored on blockchain related technology.[80]\n\nBanks are interested in this technology not least because it has the potential to speed up back office settlement systems.[97] Moreover, as the blockchain industry has reached early maturity institutional appreciation has grown that it is, practically speaking, the infrastructure of a whole new financial industry, with all the implications which that entails.[98] This technology will transform financial transactions due to its ability to enhance data storage, process simultaneous transactions, lessen transaction costs, and improve capital market transparency for debt and equity capital administration.[80]\n\nBanks such as UBS are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and reduce costs.[99][100]\n\nBerenberg, a German bank, believes that blockchain is an \"overhyped technology\" that has had a large number of \"proofs of concept\", but still has major challenges, and very few success stories.[101]\n\nThe blockchain has also given rise to initial coin offerings (ICOs) as well as a new category of digital asset called security token offerings (STOs), also sometimes referred to as digital security offerings (DSOs).[102] STO/DSOs may be conducted privately or on public, regulated stock exchange and are used to tokenize traditional assets such as company shares as well as more innovative ones like intellectual property, real estate,[103] art, or individual products. A number of companies are active in this space providing services for compliant tokenization, private STOs, and public STOs.\n\nBlockchain technology, such as cryptocurrencies and non-fungible tokens (NFTs), has been used in video games for monetization. Many live-service games offer in-game customization options, such as character skins or other in-game items, which the players can earn and trade with other players using in-game currency. Some games also allow for trading of virtual items using real-world currency, but this may be illegal in some countries where video games are seen as akin to gambling, and has led to gray market issues such as skin gambling, and thus publishers typically have shied away from allowing players to earn real-world funds from games.[104] Blockchain games typically allow players to trade these in-game items for cryptocurrency, which can then be exchanged for money.[105]\n\nThe first known game to use blockchain technologies was CryptoKitties, launched in November 2017, where the player would purchase NFTs with Ethereum cryptocurrency, each NFT consisting of a virtual pet that the player could breed with others to create offspring with combined traits as new NFTs.[106][105] The game made headlines in December 2017 when one virtual pet sold for more than US$100,000.[107] CryptoKitties also illustrated scalability problems for games on Ethereum when it created significant congestion on the Ethereum network in early 2018 with approximately 30% of all Ethereum transactions[clarification needed] being for the game.[108][109]\n\nBy the early 2020s, there had not been a breakout success in video games using blockchain, as these games tend to focus on using blockchain for speculation instead of more traditional forms of gameplay, which offers limited appeal to most players. Such games also represent a high risk to investors as their revenues can be difficult to predict.[105] However, limited successes of some games, such as Axie Infinity during the COVID-19 pandemic, and corporate plans towards metaverse content, refueled interest in the area of GameFi, a term describing the intersection of video games and financing typically backed by blockchain currency, in the second half of 2021.[110] Several major publishers, including Ubisoft, Electronic Arts, and Take Two Interactive, have stated that blockchain and NFT-based games are under serious consideration for their companies in the future.[111]\n\nIn October 2021, Valve Corporation banned blockchain games, including those using cryptocurrency and NFTs, from being hosted on its Steam digital storefront service, which is widely used for personal computer gaming, claiming that this was an extension of their policy banning games that offered in-game items with real-world value. Valve's prior history with gambling, specifically skin gambling, was speculated to be a factor in the decision to ban blockchain games.[112] Journalists and players responded positively to Valve's decision as blockchain and NFT games have a reputation for scams and fraud among most PC gamers,[104][112] and Epic Games, which runs the Epic Games Store in competition to Steam, said that they would be open to accepted blockchain games in the wake of Valve's refusal.[113]\n\nThere have been several different efforts to employ blockchains in supply chain management.\n\nThere are several different efforts to offer domain name services via the blockchain. These domain names can be controlled by the use of a private key, which purports to allow for uncensorable websites. This would also bypass a registrar's ability to suppress domains used for fraud, abuse, or illegal content.[121]\n\nNamecoin is a cryptocurrency that supports the \".bit\" top-level domain (TLD). Namecoin was forked from bitcoin in 2011. The .bit TLD is not sanctioned by ICANN, instead requiring an alternative DNS root.[121] As of 2015, .bit was used by 28 websites, out of 120,000 registered names.[122] Namecoin was dropped by OpenNIC in 2019, due to malware and potential other legal issues.[123] Other blockchain alternatives to ICANN include The Handshake Network,[122] EmerDNS, and Unstoppable Domains.[121]\n\nSpecific TLDs include \".eth\", \".luxe\", and \".kred\", which are associated with the Ethereum blockchain through the Ethereum Name Service (ENS). The .kred TLD also acts as an alternative to conventional cryptocurrency wallet addresses as a convenience for transferring cryptocurrency.[124]\n\n\nBlockchain technology can be used to create a permanent, public, transparent ledger system for compiling data on sales, tracking digital use and payments to content creators, such as wireless users[125] or musicians.[126] The Gartner 2019 CIO Survey reported 2% of higher education respondents had launched blockchain projects and another 18% were planning academic projects in the next 24 months.[127] In 2017, IBM partnered with ASCAP and PRS for Music to adopt blockchain technology in music distribution.[128] Imogen Heap's Mycelia service has also been proposed as a blockchain-based alternative \"that gives artists more control over how their songs and associated data circulate among fans and other musicians.\"[129][130]\n\nNew distribution methods are available for the insurance industry such as peer-to-peer insurance, parametric insurance and microinsurance following the adoption of blockchain.[131][132] The sharing economy and IoT are also set to benefit from blockchains because they involve many collaborating peers.[133] The use of blockchain in libraries is being studied with a grant from the U.S. Institute of Museum and Library Services.[134]\n\nOther blockchain designs include Hyperledger, a collaborative effort from the Linux Foundation to support blockchain-based distributed ledgers, with projects under this initiative including Hyperledger Burrow (by Monax) and Hyperledger Fabric (spearheaded by IBM).[135][136][137] Another is Quorum, a permissioned private blockchain by JPMorgan Chase with private storage, used for contract applications.[138]\n\nOracle introduced a blockchain table feature in its Oracle 21c database.[71][72]\n\nBlockchain is also being used in peer-to-peer energy trading.[139][140][141]\n\nLightweight blockchains, or simplified blockchains, are more suitable for internet of things (IoT) applications than conventional blockchains.[142] One experiment suggested that a lightweight blockchain-based network could accommodate up to 1.34 million authentication processes every second, which could be sufficient for resource-constrained IoT networks.[143]\n\nBlockchain could be used in detecting counterfeits by associating unique identifiers to products, documents and shipments, and storing records associated with transactions that cannot be forged or altered.[144][145] It is however argued that blockchain technology needs to be supplemented with technologies that provide a strong binding between physical objects and blockchain systems,[146] as well as provisions for content creator verification ala KYC standards.[147] The EUIPO established an Anti-Counterfeiting Blockathon Forum, with the objective of \"defining, piloting and implementing\" an anti-counterfeiting infrastructure at the European level.[148][149] The Dutch Standardisation organisation NEN uses blockchain together with QR Codes to authenticate certificates.[150]\n\nBeijing and Shanghai are among the cities designated by China to trial blockchain applications as January 30, 2022.[151] In Chinese legal proceedings, blockchain technology was first accepted as a method for authenticating internet evidence by the Hangzhou Internet Court in 2019 and has since been accepted by other Chinese courts.[152]: 123–125 \n\nWith the increasing number of blockchain systems appearing, even only those that support cryptocurrencies, blockchain interoperability is becoming a topic of major importance. The objective is to support transferring assets from one blockchain system to another blockchain system. Wegner[153] stated that \"interoperability is the ability of two or more software components to cooperate despite differences in language, interface, and execution platform\". The objective of blockchain interoperability is therefore to support such cooperation among blockchain systems, despite those kinds of differences.\n\nThere are already several blockchain interoperability solutions available.[154] They can be classified into three categories: cryptocurrency interoperability approaches, blockchain engines, and blockchain connectors.\n\nSeveral individual IETF participants produced the draft of a blockchain interoperability architecture.[155]\n\nSome cryptocurrencies use blockchain mining — the peer-to-peer computer computations by which transactions are validated and verified. This requires a large amount of energy. In June 2018, the Bank for International Settlements criticized the use of public proof-of-work blockchains for their high energy consumption.[156][157][158]\n\nEarly concern over the high energy consumption was a factor in later blockchains such as Cardano (2017), Solana (2020) and Polkadot (2020) adopting the less energy-intensive proof-of-stake model. Researchers have estimated that bitcoin consumes 100,000 times as much energy as proof-of-stake networks.[159][160]\n\nIn 2021, a study by Cambridge University determined that bitcoin (at 121 terawatt-hours per year) used more electricity than Argentina (at 121TWh) and the Netherlands (109TWh).[161] According to Digiconomist, one bitcoin transaction required 708 kilowatt-hours of electrical energy, the amount an average U.S. household consumed in 24 days.[162]\n\nIn February 2021, U.S. Treasury secretary Janet Yellen called bitcoin \"an extremely inefficient way to conduct transactions\", saying \"the amount of energy consumed in processing those transactions is staggering\".[163] In March 2021, Bill Gates stated that \"Bitcoin uses more electricity per transaction than any other method known to mankind\", adding \"It's not a great climate thing.\"[164]\n\nNicholas Weaver, of the International Computer Science Institute at the University of California, Berkeley, examined blockchain's online security, and the energy efficiency of proof-of-work public blockchains, and in both cases found it grossly inadequate.[165][166] The 31TWh-45TWh of electricity used for bitcoin in 2018 produced 17-23 million tonnes of CO2.[167][168] By 2022, the University of Cambridge and Digiconomist estimated that the two largest proof-of-work blockchains, bitcoin and Ethereum, together used twice as much electricity in one year as the whole of Sweden, leading to the release of up to 120 million tonnes of CO2 each year.[169]\n\nSome cryptocurrency developers are considering moving from the proof-of-work model to the proof-of-stake model.[170] In Sept, 2022, Ethereum converted from proof-of-work to proof-of-stake.[171]\n\nIn October 2014, the MIT Bitcoin Club, with funding from MIT alumni, provided undergraduate students at the Massachusetts Institute of Technology access to $100 of bitcoin. The adoption rates, as studied by Catalini and Tucker (2016), revealed that when people who typically adopt technologies early are given delayed access, they tend to reject the technology.[172] Many universities have founded departments focusing on crypto and blockchain, including MIT, in 2017. In the same year, Edinburgh became \"one of the first big European universities to launch a blockchain course\", according to the Financial Times.[173]\n\nMotivations for adopting blockchain technology (an aspect of innovation adoption) have been investigated by researchers. For example, Janssen, et al. provided a framework for analysis,[174] and Koens & Poll pointed out that adoption could be heavily driven by non-technical factors.[175] Based on behavioral models, Li[176] has discussed the differences between adoption at the individual level and organizational levels.\n\nScholars in business and management have started studying the role of blockchains to support collaboration.[177][178] It has been argued that blockchains can foster both cooperation (i.e., prevention of opportunistic behavior) and coordination (i.e., communication and information sharing). Thanks to reliability, transparency, traceability of records, and information immutability, blockchains facilitate collaboration in a way that differs both from the traditional use of contracts and from relational norms. Contrary to contracts, blockchains do not directly rely on the legal system to enforce agreements.[179] In addition, contrary to the use of relational norms, blockchains do not require a trust or direct connections between collaborators.\n\nThe need for internal audits to provide effective oversight of organizational efficiency will require a change in the way that information is accessed in new formats.[181] Blockchain adoption requires a framework to identify the risk of exposure associated with transactions using blockchain. The Institute of Internal Auditors has identified the need for internal auditors to address this transformational technology. New methods are required to develop audit plans that identify threats and risks. The Internal Audit Foundation study, Blockchain and Internal Audit, assesses these factors.[182] The American Institute of Certified Public Accountants has outlined new roles for auditors as a result of blockchain.[183]\n\nIn September 2015, the first peer-reviewed academic journal dedicated to cryptocurrency and blockchain technology research, Ledger, was announced. The inaugural issue was published in December 2016.[184] The journal covers aspects of mathematics, computer science, engineering, law, economics and philosophy that relate to cryptocurrencies.[185][186] The journal encourages authors to digitally sign a file hash of submitted papers, which are then timestamped into the bitcoin blockchain. Authors are also asked to include a personal bitcoin address on the first page of their papers for non-repudiation purposes.[187]\n"
    },
    {
        "title": "2007–2008 financial crisis",
        "url": "https://en.wikipedia.org/wiki/Financial_crisis_of_2007–2008",
        "content": "\n\nThe 2007–2008 financial crisis, or the global financial crisis (GFC), was the most severe worldwide economic crisis since the 1929 Wall Street crash that began the Great Depression. Causes of the crisis included predatory lending in the form of subprime mortgages to low-income homebuyers and a resulting housing bubble, excessive risk-taking by global financial institutions,[1] and lack of regulatory oversight, which culminated in a \"perfect storm\" that triggered the Great Recession, which lasted from late 2007 to mid-2009. The financial crisis began in early 2007, as mortgage-backed securities (MBS) tied to U.S. real estate, as well as a vast web of derivatives linked to those MBS, collapsed in value. Financial institutions worldwide suffered severe damage,[2] reaching a climax with the bankruptcy of Lehman Brothers on September 15, 2008, and a subsequent international banking crisis.[3]\n\nThe prerequisites for the crisis were complex.[4][5][6] During the 1990s, the U.S. Congress had passed legislation intended to expand affordable housing through looser financing.[7] In 1999, parts of the Glass–Steagall legislation (passed in 1933) were repealed, permitting institutions to mix low-risk operations, such as commercial banking and insurance, with higher-risk operations such as investment banking and proprietary trading.[8] As the Federal Reserve (\"Fed\") lowered the federal funds rate from 2000 to 2003, institutions increasingly targeted low-income homebuyers, largely belonging to racial minorities, with high-risk loans;[9] this development went unattended by regulators.[10] As interest rates rose from 2004 to 2006, the cost of mortgages rose and the demand for housing fell, causing property values to decline. In early 2007, as more U.S. mortgage holders began defaulting on their repayments, subprime lenders went bankrupt, culminating in April with the bankruptcy of New Century Financial. As demand and prices continued to fall, the contagion spread to worldwide credit markets by August, and central banks began injecting liquidity. By July 2008, Fannie Mae and Freddie Mac, companies which together owned or guaranteed half of the U.S. housing market, were on the verge of collapse; the Housing and Economic Recovery Act enabled the government to take over and cover their combined $1.6 trillion debt on September 7.\n\nIn response to the growing crisis, governments around the world deployed massive bail-outs of financial institutions and other monetary and fiscal policies to prevent a collapse of the global financial system.[11] After the bankruptcy of Lehman Brothers, the fourth largest U.S. investment bank, on September 15, the next day the Fed bailed out the American International Group (the largest U.S. insurance company), and on September 25 the government seized Washington Mutual (the largest savings and loan firm). On October 3, Congress passed the $800 billion Emergency Economic Stabilization Act, which authorized the Treasury Department to purchase troubled assets and bank stocks. The Fed began a program of quantitative easing by buying treasury bonds and other assets, such as MBS, and the February 2009 American Recovery and Reinvestment Act, signed by newly elected President Barack Obama, included a range of measures intended to preserve existing jobs and create new ones. Combined, the initiatives, coupled with actions taken in other countries, ended the worst of the Great Recession by mid-2009.\n\nAssessments of the crisis's impact in the U.S. vary, but suggest that some 8.7 million jobs were lost, causing unemployment to rise from 5 percent in 2007 to a high of 10 percent in October 2009. The percentage of citizens living in poverty rose from 12.5 percent in 2007 to 15.1 percent in 2010. The Dow Jones Industrial Average fell by 53 percent between October 2007 and March 2009, and some estimates suggest that one in four households lost 75 percent or more of their net worth. In 2010, the Dodd–Frank Wall Street Reform and Consumer Protection Act was passed, overhauling financial regulations.[12] It was opposed by many Republicans, and it was weakened by the Economic Growth, Regulatory Relief, and Consumer Protection Act in 2018. The Basel III capital and liquidity standards were also adopted by countries around the world.[13][14] The recession was a significant factor in the European debt crisis of the 2010s.\n\nThe crisis sparked the Great Recession, which, at the time, was the most severe global recession since the Great Depression.[16][17][18][19][20] It was also followed by the European debt crisis, which began with a deficit in Greece in late 2009, and the 2008–2011 Icelandic financial crisis, which involved the bank failure of all three of the major banks in Iceland and, relative to the size of its economy, was the largest economic collapse suffered by any country in history.[21] It was among the five worst financial crises the world had experienced and led to a loss of more than $2 trillion from the global economy.[22][23] U.S. home mortgage debt relative to GDP increased from an average of 46% during the 1990s to 73% during 2008, reaching $10.5 (~$14.6 trillion in 2023) trillion.[24] The increase in cash out refinancings, as home values rose, fueled an increase in consumption that could no longer be sustained when home prices declined.[25][26][27] Many financial institutions owned investments whose value was based on home mortgages such as mortgage-backed securities, or credit derivatives used to insure them against failure, which declined in value significantly.[28][29][30] The International Monetary Fund estimated that large U.S. and European banks lost more than $1 trillion on toxic assets and from bad loans from January 2007 to September 2009.[31]\n\nLack of investor confidence in bank solvency and declines in credit availability led to plummeting stock and commodity prices in late 2008 and early 2009.[32] The crisis rapidly spread into a global economic shock, resulting in several bank failures.[33] Economies worldwide slowed during this period since credit tightened and international trade declined.[34] Housing markets suffered and unemployment soared, resulting in evictions and foreclosures. Several businesses failed.[35][36] From its peak in the second quarter of 2007 at $61.4 trillion, household wealth in the United States fell $11 trillion, to $50.4 trillion by the end of the first quarter of 2009, resulting in a decline in consumption, then a decline in business investment.[37][38] In the fourth quarter of 2008, the quarter-over-quarter decline in real GDP in the U.S. was 8.4%.[39] The U.S. unemployment rate peaked at 11.0% in October 2009, the highest rate since 1983 and roughly twice the pre-crisis rate. The average hours per work week declined to 33, the lowest level since the government began collecting the data in 1964.[40][41]\n\nThe economic crisis started in the U.S. but spread to the rest of the world.[35] U.S. consumption accounted for more than a third of the growth in global consumption between 2000 and 2007 and the rest of the world depended on the U.S. consumer as a source of demand.[citation needed][42][43] Toxic securities were owned by corporate and institutional investors globally. Derivatives such as credit default swaps also increased the linkage between large financial institutions. The de-leveraging of financial institutions, as assets were sold to pay back obligations that could not be refinanced in frozen credit markets, further accelerated the solvency crisis and caused a decrease in international trade. Reductions in the growth rates of developing countries were due to falls in trade, commodity prices, investment and remittances sent from migrant workers (example: Armenia[44]). States with fragile political systems feared that investors from Western states would withdraw their money because of the crisis.[45]\n\nAs part of national fiscal policy response to the Great Recession, governments and central banks, including the Federal Reserve, the European Central Bank and the Bank of England, provided then-unprecedented trillions of dollars in bailouts and stimulus, including expansive fiscal policy and monetary policy to offset the decline in consumption and lending capacity, avoid a further collapse, encourage lending, restore faith in the integral commercial paper markets, avoid the risk of a deflationary spiral, and provide banks with enough funds to allow customers to make withdrawals.[46] In effect, the central banks went from being the \"lender of last resort\" to the \"lender of only resort\" for a significant portion of the economy. In some cases the Fed was considered the \"buyer of last resort\".[47][48][49][50][51] During the fourth quarter of 2008, these central banks purchased US$2.5 (~$3.47 trillion in 2023) trillion of government debt and troubled private assets from banks. This was the largest liquidity injection into the credit market, and the largest monetary policy action in world history. Following a model initiated by the 2008 United Kingdom bank rescue package,[52][53] the governments of European nations and the United States guaranteed the debt issued by their banks and raised the capital of their national banking systems, ultimately purchasing $1.5 trillion newly issued preferred stock in major banks.[38] The Federal Reserve created then-significant amounts of new currency as a method to combat the liquidity trap.[54]\n\nBailouts came in the form of trillions of dollars of loans, asset purchases, guarantees, and direct spending.[55] Significant controversy accompanied the bailouts, such as in the case of the AIG bonus payments controversy, leading to the development of a variety of \"decision making frameworks\", to help balance competing policy interests during times of financial crisis.[56] Alistair Darling, the U.K.'s Chancellor of the Exchequer at the time of the crisis, stated in 2018 that Britain came within hours of \"a breakdown of law and order\" the day that Royal Bank of Scotland was bailed-out.[57] Instead of financing more domestic loans, some banks instead spent some of the stimulus money in more profitable areas such as investing in emerging markets and foreign currencies.[58]\n\nIn July 2010, the Dodd–Frank Wall Street Reform and Consumer Protection Act was enacted in the United States to \"promote the financial stability of the United States\".[59] The Basel III capital and liquidity standards were adopted worldwide.[60] Since the 2008 financial crisis, consumer regulators in America have more closely supervised sellers of credit cards and home mortgages in order to deter anticompetitive practices that led to the crisis.[61]\n\nAt least two major reports on the causes of the crisis were produced by the U.S. Congress: the Financial Crisis Inquiry Commission report, released January 2011, and a report by the United States Senate Homeland Security Permanent Subcommittee on Investigations entitled Wall Street and the Financial Crisis: Anatomy of a Financial Collapse, released April 2011.\n\nIn total, 47 bankers served jail time as a result of the crisis, over half of which were from Iceland, where the crisis was the most severe and led to the collapse of all three major Icelandic banks.[62] In April 2012, Geir Haarde of Iceland became the only politician to be convicted as a result of the crisis.[63][64] Only one banker in the United States served jail time as a result of the crisis, Kareem Serageldin, a banker at Credit Suisse who was sentenced to 30 months in jail and returned $24.6 million in compensation for manipulating bond prices to hide $1 billion of losses.[65][62] No individuals in the United Kingdom were convicted as a result of the crisis.[66][67] Goldman Sachs paid $550 million to settle fraud charges after allegedly anticipating the crisis and selling toxic investments to its clients.[68]\n\nWith fewer resources to risk in creative destruction, the number of patent applications was flat, compared to exponential increases in patent application in prior years.[69]\n\nTypical American families did not fare well, nor did the \"wealthy-but-not-wealthiest\" families just beneath the pyramid's top.[70][71][72] However, half of the poorest families in the United States did not have wealth declines at all during the crisis because they generally did not own financial investments whose value can fluctuate. The Federal Reserve surveyed 4,000 households between 2007 and 2009, and found that the total wealth of 63% of all Americans declined in that period and 77% of the richest families had a decrease in total wealth, while only 50% of those on the bottom of the pyramid suffered a decrease.[73][74][75]\n\nThe following is a timeline of the major events of the financial crisis, including government responses, and the subsequent economic recovery.[76][77][78][79]\n\nThere is a really good reason for tighter credit. Tens of millions of homeowners who had substantial equity in their homes two years ago have little or nothing today. Businesses are facing the worst downturn since the Great Depression. This matters for credit decisions. A homeowner with equity in her home is very unlikely to default on a car loan or credit card debt. They will draw on this equity rather than lose their car and/or have a default placed on their credit record. On the other hand, a homeowner who has no equity is a serious default risk. In the case of businesses, their creditworthiness depends on their future profits. Profit prospects look much worse in November 2008 than they did in November 2007 ... While many banks are obviously at the brink, consumers and businesses would be facing a much harder time getting credit right now even if the financial system were rock solid. The problem with the economy is the loss of close to $6 trillion in housing wealth and an even larger amount of stock wealth.[170]\n... the pace of economic contraction is slowing. Conditions in financial markets have generally improved in recent months. Household spending has shown further signs of stabilizing but remains constrained by ongoing job losses, lower housing wealth, and tight credit. Businesses are cutting back on fixed investment and staffing but appear to be making progress in bringing inventory stocks into better alignment with sales. Although economic activity is likely to remain weak for a time, the Committee continues to anticipate that policy actions to stabilize financial markets and institutions, fiscal and monetary stimulus, and market forces will contribute to a gradual resumption of sustainable economic growth in a context of price stability.[194]\nIn the table, the names of emerging and developing economies are shown in boldface type, while the names of developed economies are in Roman (regular) type.\n\nThe twenty largest economies contributing to global GDP (PPP) growth (2007–2017)[223]\n\nThe expansion of central bank lending in response to the crisis was not only confined to the Federal Reserve's provision of aid to individual financial institutions. The Federal Reserve has also conducted a number of innovative lending programs with the goal of improving liquidity and strengthening different financial institutions and markets, such as Freddie Mac and Fannie Mae. In this case, the major problem among the market is the lack of free cash reserves and flows to secure the loans. The Federal Reserve took a number of steps to deal with worries about liquidity in the financial markets. One of these steps was a credit line for major traders, who act as the Fed's partners in open market activities.[224] Also, loan programs were set up to make the money market mutual funds and commercial paper market more flexible. Also, the Term Asset-Backed Securities Loan Facility (TALF) was put in place thanks to a joint effort with the US Department of the Treasury. This plan was meant to make it easier for consumers and businesses to get credit by giving Americans who owned high-quality asset-backed securities more credit.\n\nBefore the crisis, the Federal Reserve's stocks of Treasury securities were sold to pay for the increase in credit. This method was meant to keep banks from trying to give out their extra savings, which could cause the federal funds rate to drop below where it was supposed to be.[225] However, in October 2008, the Federal Reserve was granted the power to provide banks with interest payments on their surplus reserves. This created a motivation for banks to retain their reserves instead of disbursing them, so reducing the need for the Federal Reserve to hedge its increased lending by decreases in alternative assets.[226]\n\nMoney market funds also went through runs when people lost faith in the market. To keep it from getting worse, the Fed said it would give money to mutual fund companies. Also,  Department of Treasury said that it would briefly cover the assets of the fund. Both of these things helped get the fund market back to normal, which helped the commercial paper market, which most businesses use to run. The FDIC also did a number of things, like raise the insurance cap from $100,000 to $250,000, to boost customer trust.\n\nThey engaged in Quantitative Easing, which added more than $4 trillion to the financial system and got banks to start lending again, both to each other and to people. Many homeowners who were trying to keep their homes from going into default got housing credits. A package of policies was passed that let borrowers refinance their loans even though the value of their homes was less than what they still owed on their mortgages.[227]\n\nWhile the causes of the bubble and subsequent crash are disputed, the precipitating factor for the Financial Crisis of 2007–2008 was the bursting of the United States housing bubble and the subsequent subprime mortgage crisis, which occurred due to a high default rate and resulting foreclosures of mortgage loans, particularly adjustable-rate mortgages. Some or all of the following factors contributed to the crisis:[228][82][83]\n\nThe relaxing of credit lending standards by investment banks and commercial banks allowed for a significant increase in subprime lending. Subprime had not become less risky; Wall Street just accepted this higher risk.[267]\n\nDue to competition between mortgage lenders for revenue and market share, and when the supply of creditworthy borrowers was limited, mortgage lenders relaxed underwriting standards and originated riskier mortgages to less creditworthy borrowers. In the view of some analysts, the relatively conservative government-sponsored enterprises (GSEs) policed mortgage originators and maintained relatively high underwriting standards prior to 2003. However, as market power shifted from securitizers to originators, and as intense competition from private securitizers undermined GSE power, mortgage standards declined and risky loans proliferated. The riskiest loans were originated in 2004–2007, the years of the most intense competition between securitizers and the lowest market share for the GSEs. The GSEs eventually relaxed their standards to try to catch up with the private banks.[268][269]\n\nA contrarian view is that Fannie Mae and Freddie Mac led the way to relaxed underwriting standards, starting in 1995, by advocating the use of easy-to-qualify automated underwriting and appraisal systems, by designing no-down-payment products issued by lenders, by the promotion of thousands of small mortgage brokers, and by their close relationship to subprime loan aggregators such as Countrywide.[270][271]\n\nDepending on how \"subprime\" mortgages are defined, they remained below 10% of all mortgage originations until 2004, when they rose to nearly 20% and remained there through the 2005–2006 peak of the United States housing bubble.[272]\n\nThe majority report of the Financial Crisis Inquiry Commission, written by the six Democratic appointees, the minority report, written by three of the four Republican appointees, studies by Federal Reserve economists, and the work of several independent scholars generally contend that government affordable housing policy was not the primary cause of the financial crisis. Although they concede that governmental policies had some role in causing the crisis, they contend that GSE loans performed better than loans securitized by private investment banks, and performed better than some loans originated by institutions that held loans in their own portfolios.\n\nIn his dissent to the majority report of the Financial Crisis Inquiry Commission, conservative American Enterprise Institute fellow Peter J. Wallison[273] stated his belief that the roots of the financial crisis can be traced directly and primarily to affordable housing policies initiated by the United States Department of Housing and Urban Development (HUD) in the 1990s and to massive risky loan purchases by government-sponsored entities Fannie Mae and Freddie Mac. Based upon information in the SEC's December 2011 securities fraud case against six former executives of Fannie and Freddie, Peter Wallison and Edward Pinto estimated that, in 2008, Fannie and Freddie held 13 million substandard loans totaling over $2 trillion.[274]\n\nIn the early and mid-2000s, the Bush administration called numerous times for investigations into the safety and soundness of the GSEs and their swelling portfolio of subprime mortgages. On September 10, 2003, the United States House Committee on Financial Services held a hearing, at the urging of the administration, to assess safety and soundness issues and to review a recent report by the Office of Federal Housing Enterprise Oversight (OFHEO) that had uncovered accounting discrepancies within the two entities.[275][276] The hearings never resulted in new legislation or formal investigation of Fannie Mae and Freddie Mac, as many of the committee members refused to accept the report and instead rebuked OFHEO for their attempt at regulation.[277] Some, such as Wallison, believe this was an early warning to the systemic risk that the growing market in subprime mortgages posed to the U.S. financial system that went unheeded.[278]\n\nA 2000 United States Department of the Treasury study of lending trends for 305 cities from 1993 to 1998 showed that $467 billion of mortgage lending was made by Community Reinvestment Act (CRA)-covered lenders into low and mid-level income (LMI) borrowers and neighborhoods, representing 10% of all U.S. mortgage lending during the period. The majority of these were prime loans. Sub-prime loans made by CRA-covered institutions constituted a 3% market share of LMI loans in 1998,[279] but in the run-up to the crisis, fully 25% of all subprime lending occurred at CRA-covered institutions and another 25% of subprime loans had some connection with CRA.[280] However, most sub-prime loans were not made to the LMI borrowers targeted by the CRA,[citation needed][281][282] especially in the years 2005–2006 leading up to the crisis,[citation needed][283][282][284] nor did it find any evidence that lending under the CRA rules increased delinquency rates or that the CRA indirectly influenced independent mortgage lenders to ramp up sub-prime lending.[285][verification needed]\n\nTo other analysts the delay between CRA rule changes in 1995 and the explosion of subprime lending is not surprising, and does not exonerate the CRA. They contend that there were two, connected causes to the crisis: the relaxation of underwriting standards in 1995 and the ultra-low interest rates initiated by the Federal Reserve after the terrorist attack on September 11, 2001. Both causes had to be in place before the crisis could take place.[286] Critics also point out that publicly announced CRA loan commitments were massive, totaling $4.5 trillion in the years between 1994 and 2007.[287] They also argue that the Federal Reserve's classification of CRA loans as \"prime\" is based on the faulty and self-serving assumption that high-interest-rate loans (3 percentage points over average) equal \"subprime\" loans.[288]\n\nOthers have pointed out that there were not enough of these loans made to cause a crisis of this magnitude. In an article in Portfolio magazine, Michael Lewis spoke with one trader who noted that \"There weren't enough Americans with [bad] credit taking out [bad loans] to satisfy investors' appetite for the end product.\" Essentially, investment banks and hedge funds used financial innovation to enable large wagers to be made, far beyond the actual value of the underlying mortgage loans, using derivatives called credit default swaps, collateralized debt obligations and synthetic CDOs.\n\nBy March 2011, the FDIC had paid out $9 billion (c. $12 billion in 2023[289]) to cover losses on bad loans at 165 failed financial institutions.[290][291] The Congressional Budget Office estimated, in June 2011, that the bailout to Fannie Mae and Freddie Mac exceeds $300 billion (c. $401 billion in 2023[289]) (calculated by adding the fair value deficits of the entities to the direct bailout funds at the time).[292]\n\nEconomist Paul Krugman argued in January 2010 that the simultaneous growth of the residential and commercial real estate pricing bubbles and the global nature of the crisis undermines the case made by those who argue that Fannie Mae, Freddie Mac, CRA, or predatory lending were primary causes of the crisis. In other words, bubbles in both markets developed even though only the residential market was affected by these potential causes.[293]\n\nCountering Krugman, Wallison wrote: \"It is not true that every bubble—even a large bubble—has the potential to cause a financial crisis when it deflates.\" Wallison notes that other developed countries had \"large bubbles during the 1997–2007 period\" but \"the losses associated with mortgage delinquencies and defaults when these bubbles deflated were far lower than the losses suffered in the United States when the 1997–2007 [bubble] deflated.\" According to Wallison, the reason the U.S. residential housing bubble (as opposed to other types of bubbles) led to financial crisis was that it was supported by a huge number of substandard loans—generally with low or no downpayments.[294]\n\nKrugman's contention (that the growth of a commercial real estate bubble indicates that U.S. housing policy was not the cause of the crisis) is challenged by additional analysis. After researching the default of commercial loans during the financial crisis, Xudong An and Anthony B. Sanders reported (in December 2010): \"We find limited evidence that substantial deterioration in CMBS [commercial mortgage-backed securities] loan underwriting occurred prior to the crisis.\"[295] Other analysts support the contention that the crisis in commercial real estate and related lending took place after the crisis in residential real estate. Business journalist Kimberly Amadeo reported: \"The first signs of decline in residential real estate occurred in 2006. Three years later, commercial real estate started feeling the effects.\"[verification needed][296] Denice A. Gierach, a real estate attorney and CPA, wrote:\n\n... most of the commercial real estate loans were good loans destroyed by a really bad economy. In other words, the borrowers did not cause the loans to go bad-it was the economy.[297]\nBetween 1998 and 2006, the price of the typical American house increased by 124%.[298] During the 1980s and 1990s, the national median home price ranged from 2.9 to 3.1 times median household income. By contrast, this ratio increased to 4.0 in 2004, and 4.6 in 2006.[299] This housing bubble resulted in many homeowners refinancing their homes at lower interest rates, or financing consumer spending by taking out second mortgages secured by the price appreciation.\n\nIn a Peabody Award-winning program, NPR correspondents argued that a \"Giant Pool of Money\" (represented by $70 trillion in worldwide fixed income investments) sought higher yields than those offered by U.S. Treasury bonds early in the decade. This pool of money had roughly doubled in size from 2000 to 2007, yet the supply of relatively safe, income generating investments had not grown as fast. Investment banks on Wall Street answered this demand with products such as the mortgage-backed security and the collateralized debt obligation that were assigned safe ratings by the credit rating agencies.[2]\n\nIn effect, Wall Street connected this pool of money to the mortgage market in the US, with enormous fees accruing to those throughout the mortgage supply chain, from the mortgage broker selling the loans to small banks that funded the brokers and the large investment banks behind them. By approximately 2003, the supply of mortgages originated at traditional lending standards had been exhausted, and continued strong demand began to drive down lending standards.[2]\n\nThe collateralized debt obligation in particular enabled financial institutions to obtain investor funds to finance subprime and other lending, extending or increasing the housing bubble and generating large fees. This essentially places cash payments from multiple mortgages or other debt obligations into a single pool from which specific securities draw in a specific sequence of priority. Those securities first in line received investment-grade ratings from rating agencies. Securities with lower priority had lower credit ratings but theoretically a higher rate of return on the amount invested.[300]\n\nBy September 2008, average U.S. housing prices had declined by over 20% from their mid-2006 peak.[301][302] As prices declined, borrowers with adjustable-rate mortgages could not refinance to avoid the higher payments associated with rising interest rates and began to default. During 2007, lenders began foreclosure proceedings on nearly 1.3 million properties, a 79% increase over 2006.[303] This increased to 2.3 million in 2008, an 81% increase vs. 2007.[304] By August 2008, approximately 9% of all U.S. mortgages outstanding were either delinquent or in foreclosure.[305] By September 2009, this had risen to 14.4%.[306][307]\n\nAfter the bubble burst, Australian economist John Quiggin wrote, \"And, unlike the Great Depression, this crisis was entirely the product of financial markets. There was nothing like the postwar turmoil of the 1920s, the struggles over gold convertibility and reparations, or the Smoot-Hawley tariff, all of which have shared the blame for the Great Depression.\" Instead, Quiggin lays the blame for the 2008 near-meltdown on financial markets, on political decisions to lightly regulate them, and on rating agencies which had self-interested incentives to give good ratings.[308]\n\nLower interest rates encouraged borrowing. From 2000 to 2003, the Federal Reserve lowered the federal funds rate target from 6.5% to 1.0%.[309][310] This was done to soften the effects of the collapse of the dot-com bubble and the September 11 attacks, as well as to combat a perceived risk of deflation.[311] As early as 2002, it was apparent that credit was fueling housing instead of business investment as some economists went so far as to advocate that the Fed \"needs to create a housing bubble to replace the Nasdaq bubble\".[312] Moreover, empirical studies using data from advanced countries show that excessive credit growth contributed greatly to the severity of the crisis.[313]\n\nAdditional downward pressure on interest rates was created by rising U.S. current account deficit, which peaked along with the housing bubble in 2006. Federal Reserve chairman Ben Bernanke explained how trade deficits required the U.S. to borrow money from abroad, in the process bidding up bond prices and lowering interest rates.[314]\n\nBernanke explained that between 1996 and 2004, the U.S. current account deficit increased by $650 billion, from 1.5% to 5.8% of GDP. Financing these deficits required the country to borrow large sums from abroad, much of it from countries running trade surpluses. These were mainly the emerging economies in Asia and oil-exporting nations. The balance of payments identity requires that a country (such as the US) running a current account deficit also have a capital account (investment) surplus of the same amount. Hence large and growing amounts of foreign funds (capital) flowed into the U.S. to finance its imports.\n\nAll of this created demand for various types of financial assets, raising the prices of those assets while lowering interest rates. Foreign investors had these funds to lend either because they had very high personal savings rates (as high as 40% in China) or because of high oil prices. Ben Bernanke referred to this as a \"saving glut\".[315]\n\nA flood of funds (capital or liquidity) reached the U.S. financial markets. Foreign governments supplied funds by purchasing Treasury bonds and thus avoided much of the direct effect of the crisis. U.S. households, used funds borrowed from foreigners to finance consumption or to bid up the prices of housing and financial assets. Financial institutions invested foreign funds in mortgage-backed securities.[citation needed]\n\nThe Fed then raised the Fed funds rate significantly between July 2004 and July 2006.[316] This contributed to an increase in one-year and five-year adjustable-rate mortgage (ARM) rates, making ARM interest rate resets more expensive for homeowners.[317] This may have also contributed to the deflating of the housing bubble, as asset prices generally move inversely to interest rates, and it became riskier to speculate in housing.[318][319] U.S. housing and financial assets dramatically declined in value after the housing bubble burst.[320][38]\n\nSubprime lending standards declined in the U.S.: in early 2000, a subprime borrower had a FICO score of 660 or less. By 2005, many lenders dropped the required FICO score to 620, making it much easier to qualify for prime loans and making subprime lending a riskier business. Proof of income and assets were de-emphasized. Loans at first required full documentation, then low documentation, then no documentation. One subprime mortgage product that gained wide acceptance was the no income, no job, no asset verification required (NINJA) mortgage. Informally, these loans were aptly referred to as \"liar loans\" because they encouraged borrowers to be less than honest in the loan application process.[321] Testimony given to the Financial Crisis Inquiry Commission by whistleblower Richard M. Bowen III, on events during his tenure as the Business Chief Underwriter for Correspondent Lending in the Consumer Lending Group for Citigroup, where he was responsible for over 220 professional underwriters, suggests that by 2006 and 2007, the collapse of mortgage underwriting standards was endemic. His testimony stated that by 2006, 60% of mortgages purchased by Citigroup from some 1,600 mortgage companies were \"defective\" (were not underwritten to policy, or did not contain all policy-required documents)—this, despite the fact that each of these 1,600 originators was contractually responsible (certified via representations and warrantees) that its mortgage originations met Citigroup standards. Moreover, during 2007, \"defective mortgages (from mortgage originators contractually bound to perform underwriting to Citi's standards) increased ... to over 80% of production\".[322]\n\nIn separate testimony to the Financial Crisis Inquiry Commission, officers of Clayton Holdings, the largest residential loan due diligence and securitization surveillance company in the United States and Europe, testified that Clayton's review of over 900,000 mortgages issued from January 2006 to June 2007 revealed that scarcely 54% of the loans met their originators' underwriting standards. The analysis (conducted on behalf of 23 investment and commercial banks, including 7 \"too big to fail\" banks) additionally showed that 28% of the sampled loans did not meet the minimal standards of any issuer. Clayton's analysis further showed that 39% of these loans (i.e. those not meeting any issuer's minimal underwriting standards) were subsequently securitized and sold to investors.[323][324]\n\nPredatory lending refers to the practice of unscrupulous lenders, enticing borrowers to enter into \"unsafe\" or \"unsound\" secured loans for inappropriate purposes.[325][326][327]\n\nIn June 2008, Countrywide Financial was sued by then California Attorney General Jerry Brown for \"unfair business practices\" and \"false advertising\", alleging that Countrywide used \"deceptive tactics to push homeowners into complicated, risky, and expensive loans so that the company could sell as many loans as possible to third-party investors\".[328] In May 2009, Bank of America modified 64,000 Countrywide loans as a result.[329] When housing prices decreased, homeowners in ARMs then had little incentive to pay their monthly payments, since their home equity had disappeared. This caused Countrywide's financial condition to deteriorate, ultimately resulting in a decision by the Office of Thrift Supervision to seize the lender. One Countrywide employee—who would later plead guilty to two counts of wire fraud and spent 18 months in prison—stated that, \"If you had a pulse, we gave you a loan.\"[330]\n\nFormer employees from Ameriquest, which was United States' leading wholesale lender, described a system in which they were pushed to falsify mortgage documents and then sell the mortgages to Wall Street banks eager to make fast profits. There is growing evidence that such mortgage frauds may be a cause of the crisis.[331]\n\nAccording to Barry Eichengreen, the roots of the financial crisis lay in the deregulation of financial markets.[332] A 2012 OECD study[333] suggest that bank regulation based on the Basel accords encourage unconventional business practices and contributed to or even reinforced the financial crisis. In other cases, laws were changed or enforcement weakened in parts of the financial system. Key examples include:\n\nA 2011 paper suggested that Canada's avoidance of a banking crisis in 2008 (as well as in prior eras) could be attributed to Canada possessing a single, powerful, overarching regulator, while the United States had a weak, crisis prone and fragmented banking system with multiple competing regulatory bodies.[351]\n\nPrior to the crisis, financial institutions became highly leveraged, increasing their appetite for risky investments and reducing their resilience in case of losses. Much of this leverage was achieved using complex financial instruments such as off-balance sheet securitization and derivatives, which made it difficult for creditors and regulators to monitor and try to reduce financial institution risk levels.[352][verification needed]\n\nU.S. households and financial institutions became increasingly indebted or overleveraged during the years preceding the crisis.[353] This increased their vulnerability to the collapse of the housing bubble and worsened the ensuing economic downturn.[354] Key statistics include:\n\nFree cash used by consumers from home equity extraction doubled from $627 billion in 2001 to $1,428 billion in 2005 as the housing bubble built, a total of nearly $5 trillion over the period, contributing to economic growth worldwide.[25][26][27] U.S. home mortgage debt relative to GDP increased from an average of 46% during the 1990s to 73% during 2008, reaching $10.5 trillion (c. $14.6 trillion in 2023[289]).[24]\n\nU.S. household debt as a percentage of annual disposable personal income was 127% at the end of 2007, versus 77% in 1990.[353] In 1981, U.S. private debt was 123% of GDP; by the third quarter of 2008, it was 290%.[355]\n\nFrom 2004 to 2007, the top five U.S. investment banks each significantly increased their financial leverage, which increased their vulnerability to a financial shock. Changes in capital requirements, intended to keep U.S. banks competitive with their European counterparts, allowed lower risk weightings for AAA-rated securities. The shift from first-loss tranches to AAA-rated tranches was seen by regulators as a risk reduction that compensated the higher leverage.[356] These five institutions reported over $4.1 trillion in debt for fiscal year 2007, about 30% of U.S. nominal GDP for 2007. Lehman Brothers went bankrupt and was liquidated, Bear Stearns and Merrill Lynch were sold at fire-sale prices, and Goldman Sachs and Morgan Stanley became commercial banks, subjecting themselves to more stringent regulation. With the exception of Lehman, these companies required or received government support.[357]\n\nFannie Mae and Freddie Mac, two U.S. government-sponsored enterprises, owned or guaranteed nearly $5 trillion (c. $6.95 trillion in 2023[289]) trillion in mortgage obligations at the time they were placed into conservatorship by the U.S. government in September 2008.[358][359]\n\nThese seven entities were highly leveraged and had $9 trillion in debt or guarantee obligations; yet they were not subject to the same regulation as depository banks.[342][360]\n\nBehavior that may be optimal for an individual, such as saving more during adverse economic conditions, can be detrimental if too many individuals pursue the same behavior, as ultimately one person's consumption is another person's income. Too many consumers attempting to save or pay down debt simultaneously is called the paradox of thrift and can cause or deepen a recession. Economist Hyman Minsky also described a \"paradox of deleveraging\" as financial institutions that have too much leverage (debt relative to equity) cannot all de-leverage simultaneously without significant declines in the value of their assets.[354]\n\nIn April 2009, Federal Reserve vice-chair Janet Yellen discussed these paradoxes:\n\nOnce this massive credit crunch hit, it didn't take long before we were in a recession. The recession, in turn, deepened the credit crunch as demand and employment fell, and credit losses of financial institutions surged. Indeed, we have been in the grips of precisely this adverse feedback loop for more than a year. A process of balance sheet deleveraging has spread to nearly every corner of the economy. Consumers are pulling back on purchases, especially on durable goods, to build their savings. Businesses are cancelling planned investments and laying off workers to preserve cash. And financial institutions are shrinking assets to bolster capital and improve their chances of weathering the current storm. Once again, Minsky understood this dynamic. He spoke of the paradox of deleveraging, in which precautions that may be smart for individuals and firms—and indeed essential to return the economy to a normal state—nevertheless magnify the distress of the economy as a whole.[354]\nThe term financial innovation refers to the ongoing development of financial products designed to achieve particular client objectives, such as offsetting a particular risk exposure (such as the default of a borrower) or to assist with obtaining financing. Examples pertinent to this crisis included: the adjustable-rate mortgage; the bundling of subprime mortgages into mortgage-backed securities (MBS) or collateralized debt obligations (CDO) for sale to investors, a type of securitization; and a form of credit insurance called credit default swaps (CDS). The usage of these products expanded dramatically in the years leading up to the crisis. These products vary in complexity and the ease with which they can be valued on the books of financial institutions.[citation needed]\n\nCDO issuance grew from an estimated $20 billion in Q1 2004 to its peak of over $180 billion by Q1 2007, then declined back under $20 billion by Q1 2008. Further, the credit quality of CDO's declined from 2000 to 2007, as the level of subprime and other non-prime mortgage debt increased from 5% to 36% of CDO assets. As described in the section on subprime lending, the CDS and portfolio of CDS called synthetic CDO enabled a theoretically infinite amount to be wagered on the finite value of housing loans outstanding, provided that buyers and sellers of the derivatives could be found. For example, buying a CDS to insure a CDO ended up giving the seller the same risk as if they owned the CDO, when those CDO's became worthless.[361]\n\nThis boom in innovative financial products went hand in hand with more complexity. It multiplied the number of actors connected to a single mortgage (including mortgage brokers, specialized originators, the securitizers and their due diligence firms, managing agents and trading desks, and finally investors, insurances and providers of repo funding). With increasing distance from the underlying asset these actors relied more and more on indirect information (including FICO scores on creditworthiness, appraisals and due diligence checks by third party organizations, and most importantly the computer models of rating agencies and risk management desks). Instead of spreading risk this provided the ground for fraudulent acts, misjudgments and finally market collapse.[362] Economists have studied the crisis as an instance of cascades in financial networks, where institutions' instability destabilized other institutions and led to knock-on effects.[363][364]\n\nMartin Wolf, chief economics commentator at the Financial Times, wrote in June 2009 that certain financial innovations enabled firms to circumvent regulations, such as off-balance sheet financing that affects the leverage or capital cushion reported by major banks, stating: \"an enormous part of what banks did in the early part of this decade—the off-balance-sheet vehicles, the derivatives and the 'shadow banking system' itself—was to find a way round regulation.\"[365]\n\nMortgage risks were underestimated by almost all institutions in the chain from originator to investor by underweighting the possibility of falling housing prices based on historical trends of the past 50 years. Limitations of default and prepayment models, the heart of pricing models, led to overvaluation of mortgage and asset-backed products and their derivatives by originators, securitizers, broker-dealers, rating-agencies, insurance underwriters and the vast majority of investors (with the exception of certain hedge funds).[366][367] While financial derivatives and structured products helped partition and shift risk between financial participants, it was the underestimation of falling housing prices and the resultant losses that led to aggregate risk.[367]\n\nFor a variety of reasons, market participants did not accurately measure the risk inherent with financial innovation such as MBS and CDOs or understand its effect on the overall stability of the financial system.[255] The pricing model for CDOs clearly did not reflect the level of risk they introduced into the system. Banks estimated that $450 billion of CDO were sold between \"late 2005 to the middle of 2007\"; among the $102 billion of those that had been liquidated, JPMorgan estimated that the average recovery rate for \"high quality\" CDOs was approximately 32 cents on the dollar, while the recovery rate for mezzanine capital CDO was approximately five cents for every dollar.\n\nAIG insured obligations of various financial institutions through the usage of credit default swaps. The basic CDS transaction involved AIG receiving a premium in exchange for a promise to pay money to party A in the event party B defaulted. However, AIG did not have the financial strength to support its many CDS commitments as the crisis progressed and was taken over by the government in September 2008. U.S. taxpayers provided over $180 billion in government loans and investments in AIG during 2008 and early 2009, through which the money flowed to various counterparties to CDS transactions, including many large global financial institutions.[368][unreliable source?][369]\n\nThe Financial Crisis Inquiry Commission (FCIC) made the major government study of the crisis. It concluded in January 2011:\n\nThe Commission concludes AIG failed and was rescued by the government primarily because its enormous sales of credit default swaps were made without putting up the initial collateral, setting aside capital reserves, or hedging its exposure—a profound failure in corporate governance, particularly its risk management practices. AIG's failure was possible because of the sweeping deregulation of over-the-counter (OTC) derivatives, including credit default swaps, which effectively eliminated federal and state regulation of these products, including capital and margin requirements that would have lessened the likelihood of AIG's failure.[370][371][372]\nThe limitations of a widely used financial model also were not properly understood.[373][374] This formula assumed that the price of CDS was correlated with and could predict the correct price of mortgage-backed securities. Because it was highly tractable, it rapidly came to be used by a huge percentage of CDO and CDS investors, issuers, and rating agencies.[374] According to one Wired article:\n\nThen the model fell apart. Cracks started appearing early on, when financial markets began behaving in ways that users of Li's formula hadn't expected. The cracks became full-fledged canyons in 2008—when ruptures in the financial system's foundation swallowed up trillions of dollars and put the survival of the global banking system in serious peril ... Li's Gaussian copula formula will go down in history as instrumental in causing the unfathomable losses that brought the world financial system to its knees.[374] \nAs financial assets became more complex and harder to value, investors were reassured by the fact that the international bond rating agencies and bank regulators accepted as valid some complex mathematical models that showed the risks were much smaller than they actually were.[375] George Soros commented that \"The super-boom got out of hand when the new products became so complicated that the authorities could no longer calculate the risks and started relying on the risk management methods of the banks themselves. Similarly, the rating agencies relied on the information provided by the originators of synthetic products. It was a shocking abdication of responsibility.\"[376]\n\nA conflict of interest between investment management professional and institutional investors, combined with a global glut in investment capital, led to bad investments by asset managers in over-priced credit assets. Professional investment managers generally are compensated based on the volume of client assets under management. There is, therefore, an incentive for asset managers to expand their assets under management in order to maximize their compensation. As the glut in global investment capital caused the yields on credit assets to decline, asset managers were faced with the choice of either investing in assets where returns did not reflect true credit risk or returning funds to clients. Many asset managers continued to invest client funds in over-priced (under-yielding) investments, to the detriment of their clients, so they could maintain their assets under management. They supported this choice with a \"plausible deniability\" of the risks associated with subprime-based credit assets because the loss experience with early \"vintages\" of subprime loans was so low.[377]\n\nDespite the dominance of the above formula, there are documented attempts of the financial industry, occurring before the crisis, to address the formula limitations, specifically the lack of dependence dynamics and the poor representation of extreme events.[378] The volume Credit Correlation: Life After Copulas, published in 2007 by World Scientific, summarizes a 2006 conference held by Merrill Lynch in London where several practitioners attempted to propose models rectifying some of the copula limitations. See also the article by Donnelly and Embrechts[379] and the book by Brigo, Pallavicini and Torresetti, that reports relevant warnings and research on CDOs appeared in 2006.[380]\n\nThere is strong evidence that the riskiest, worst performing mortgages were funded through the \"shadow banking system\" and that competition from the shadow banking system may have pressured more traditional institutions to lower their underwriting standards and originate riskier loans.\n\nIn a June 2008 speech, President and CEO of the Federal Reserve Bank of New York Timothy Geithner—who in 2009 became United States Secretary of the Treasury—placed significant blame for the freezing of credit markets on a run on the entities in the \"parallel\" banking system, also called the shadow banking system. These entities became critical to the credit markets underpinning the financial system, but were not subject to the same regulatory controls. Further, these entities were vulnerable because of asset–liability mismatch, meaning that they borrowed short-term in liquid markets to purchase long-term, illiquid and risky assets. This meant that disruptions in credit markets would force them to engage in rapid deleveraging, selling their long-term assets at depressed prices. He described the significance of these entities:\n\nIn early 2007, asset-backed commercial paper conduits, in structured investment vehicles, in auction-rate preferred securities, tender option bonds and variable rate demand notes, had a combined asset size of roughly $2.2 trillion. Assets financed overnight in tri-party repo grew to $2.5 trillion. Assets held in hedge funds grew to roughly $1.8 trillion. The combined balance sheets of the five largest investment banks totaled $4 trillion. In comparison, the total assets of the top five bank holding companies in the United States at that point were just over $6 trillion, and total assets of the entire banking system were about $10 trillion. The combined effect of these factors was a financial system vulnerable to self-reinforcing asset price and credit cycles.[381]\nEconomist Paul Krugman, laureate of the Nobel Memorial Prize in Economic Sciences, described the run on the shadow banking system as the \"core of what happened\" to cause the crisis. He referred to this lack of controls as \"malign neglect\" and argued that regulation should have been imposed on all banking-like activity.[342] Without the ability to obtain investor funds in exchange for most types of mortgage-backed securities or asset-backed commercial paper, investment banks and other entities in the shadow banking system could not provide funds to mortgage firms and other corporations.[381][342]\n\nThis meant that nearly one-third of the U.S. lending mechanism was frozen and continued to be frozen into June 2009.[382] According to the Brookings Institution, at that time the traditional banking system did not have the capital to close this gap: \"It would take a number of years of strong profits to generate sufficient capital to support that additional lending volume.\" The authors also indicate that some forms of securitization were \"likely to vanish forever, having been an artifact of excessively loose credit conditions\". While traditional banks raised their lending standards, it was the collapse of the shadow banking system that was the primary cause of the reduction in funds available for borrowing.[35]\n\nIn a 2008 paper, Ricardo J. Caballero, Emmanuel Farhi, and Pierre-Olivier Gourinchas argued that the financial crisis was attributable to \"global asset scarcity, which led to large capital flows toward the United States and to the creation of asset bubbles that eventually burst\".[383] Caballero, Farhi, and Gourinchas argued \"that the sharp rise in oil prices following the subprime crisis – nearly 100 percent in just a matter of months and on the face of recessionary shocks – was the result of a speculative response to the financial crisis itself, in an attempt to rebuild asset supply. That is, the global economy was subject to one shock with multiple implications rather than to two separate shocks (financial and oil).\"[383]\n\nLong-only commodity index funds became popular – by one estimate investment increased from $90 billion in 2006 to $200 billion at the end of 2007, while commodity prices increased 71% – which raised concern as to whether these index funds caused the commodity bubble. The empirical research has been mixed.[384]\n\nThe cause of the global asset bubble can be partially attributable to the global savings glut. As theorized by Andrew Metrick, the demand for safe assets following the Asian Financial Crisis coupled with the lack of circulating treasuries created an unmet demand for \"risk free\" assets. Thus, institutional investors like sovereign wealth funds and pension funds began purchasing synthetic safe assets like Triple-A Mortgage Backed Securities.[385]\n\nAs a consequence, the demand for so-called safe assets fueled the free flow of capital into housing in the United States. This greatly worsened the crisis as banks and other financial institutions were incentivized to issue more mortgages than before.\n\nIn a 1998 book, John McMurtry suggested that a financial crisis is a systemic crisis of capitalism itself.[386]\n\nIn his 1978 book, The Downfall of Capitalism and Communism, Ravi Batra suggests that growing inequality of financial capitalism produces speculative bubbles that burst and result in depression and major political changes. He also suggested that a \"demand gap\" related to differing wage and productivity growth explains deficit and debt dynamics important to stock market developments.[387]\n\nJohn Bellamy Foster, a political economy analyst and editor of the Monthly Review, believed that the decrease in GDP growth rates since the early 1970s is due to increasing market saturation.[388]\n\nMarxian economics followers Andrew Kliman, Michael Roberts, and Guglielmo Carchedi, in contradistinction to the Monthly Review school represented by Foster, pointed to capitalism's long-term tendency of the rate of profit to fall as the underlying cause of crises generally. From this point of view, the problem was the inability of capital to grow or accumulate at sufficient rates through productive investment alone. Low rates of profit in productive sectors led to speculative investment in riskier assets, where there was potential for greater return on investment. The speculative frenzy of the late 1990s and 2000s was, in this view, a consequence of a rising organic composition of capital, expressed through the fall in the rate of profit. According to Michael Roberts, the fall in the rate of profit \"eventually triggered the credit crunch of 2007 when credit could no longer support profits\".[389]\n\nIn 2005 book, The Battle for the Soul of Capitalism, John C. Bogle wrote that \"Corporate America went astray largely because the power of managers went virtually unchecked by our gatekeepers for far too long\". Echoing the central thesis of James Burnham's 1941 seminal book, The Managerial Revolution, Bogle cites issues, including:[390]\n\nIn his book The Big Mo, Mark Roeder, a former executive at the Swiss-based UBS Bank, suggested that large-scale momentum, or The Big Mo, \"played a pivotal role\" in the financial crisis. Roeder suggested that \"recent technological advances, such as computer-driven trading programs, together with the increasingly interconnected nature of markets, has magnified the momentum effect. This has made the financial sector inherently unstable.\"[391]\n\nRobert Reich attributed the economic downturn to the stagnation of wages in the United States, particularly those of the hourly workers who comprise 80% of the workforce. This stagnation forced the population to borrow to meet the cost of living.[392]\n\nEconomists Ailsa McKay and Margunn Bjørnholt argued that the financial crisis and the response to it revealed a crisis of ideas in mainstream economics and within the economics profession, and call for a reshaping of both the economy, economic theory and the economics profession.[393]\n\nA report by the International Labour Organization concluded that cooperative banking institutions were less likely to fail than their competitors during the crisis. The cooperative banking sector had 20% market share of the European banking sector, but accounted for only 7% of all the write-downs and losses between the third quarter of 2007 and first quarter of 2011.[394] In 2008, in the U.S., the rate of commercial bank failures was almost triple that of credit unions, and almost five times the credit union rate in 2010.[395] Credit unions increased their lending to small- and medium-sized businesses while overall lending to those businesses decreased.[396]\n\nEconomists, particularly followers of mainstream economics, mostly failed to predict the crisis.[397] The Wharton School of the University of Pennsylvania's online business journal examined why economists failed to predict a major global financial crisis and concluded that economists used mathematical models that failed to account for the critical roles that banks and other financial institutions, as opposed to producers and consumers of goods and services, play in the economy.[398]\n\nSeveral followers of heterodox economics predicted the crisis, with varying arguments. Dirk Bezemer[399] credits 12 economists with predicting the crisis: Dean Baker (US), Wynne Godley (UK), Fred Harrison (UK), Michael Hudson (US), Eric Janszen (US), Steve Keen (Australia), Jakob Broechner Madsen & Jens Kjaer Sørensen (Denmark), Med Jones (US)[400] Kurt Richebächer (US), Nouriel Roubini (US), Peter Schiff (US), and Robert Shiller (US).\n\nShiller, a founder of the Case–Shiller index that measures home prices, wrote an article a year before the collapse of Lehman Brothers in which he predicted that a slowing U.S. housing market would cause the housing bubble to burst, leading to financial collapse.[401] Peter Schiff regularly appeared on television in the years before the crisis and warned of the impending real estate collapse.[402]\n\nThe Austrian School regarded the crisis as a vindication and classic example of a predictable credit-fueled bubble caused by laxity in monetary supply.[403]\n\nThere were other economists that did warn of a pending crisis.[404]\n\nThe former Governor of the Reserve Bank of India, Raghuram Rajan, had predicted the crisis in 2005 when he became chief economist at the International Monetary Fund. In 2005, at a celebration honoring Alan Greenspan, who was about to retire as chairman of the US Federal Reserve, Rajan delivered a controversial paper that was critical of the financial sector.[405] In that paper, Rajan \"argued that disaster might loom\".[406] Rajan argued that financial sector managers were encouraged to \"take risks that generate severe adverse consequences with small probability but, in return, offer generous compensation the rest of the time. These risks are known as tail risks. But perhaps the most important concern is whether banks will be able to provide liquidity to financial markets so that if the tail risk does materialize, financial positions can be unwound and losses allocated so that the consequences to the real economy are minimized.\"\n\nStock trader and financial risk engineer Nassim Nicholas Taleb, author of the 2007 book The Black Swan, spent years warning against the breakdown of the banking system in particular and the economy in general owing to their use of and reliance on bad risk models and reliance on forecasting, and framed the problem as part of \"robustness and fragility\".[407][408] He also took action against the establishment view by making a big financial bet on banking stocks and making a fortune from the crisis (\"They didn't listen, so I took their money\").[409] According to David Brooks from The New York Times, \"Taleb not only has an explanation for what's happening, he saw it coming.\"[410]\n\nPopular articles published in the mass media have led the general public to believe that the majority of economists have failed in their obligation to predict the financial crisis. For example, an article in The New York Times noted that economist Nouriel Roubini warned of such crisis as early as September 2006, and stated that the profession of economics is bad at predicting recessions.[411] According to The Guardian, Roubini was ridiculed for predicting a collapse of the housing market and worldwide recession, while The New York Times labelled him \"Dr. Doom\".[412]\n\nIn a 2012 article in the journal Japan and the World Economy, Andrew K. Rose and Mark M. Spiegel used a Multiple Indicator Multiple Cause (MIMIC) model on a cross-section of 107 countries to evaluate potential causes of the 2008 crisis. The authors examined various economic indicators, ignoring contagion effects across countries. The authors concluded: \"We include over sixty potential causes of the crisis, covering such categories as: financial system policies and conditions; asset price appreciation in real estate and equity markets; international imbalances and foreign reserve adequacy; macroeconomic policies; and institutional and geographic features. Despite the fact that we use a wide number of possible causes in a flexible statistical framework, we are unable to link most of the commonly cited causes of the crisis to its incidence across countries. This negative finding in the cross-section makes us skeptical of the accuracy of 'early warning' systems of potential crises, which must also predict their timing.\"[413]\n\nThe first visible institution to run into trouble in the United States was the Southern California–based IndyMac, a spin-off of Countrywide Financial. Before its failure, IndyMac Bank was the largest savings and loan association in the Los Angeles market and the seventh largest mortgage loan originator in the United States.[414] The failure of IndyMac Bank on July 11, 2008, was the fourth largest bank failure in United States history up until the crisis precipitated even larger failures,[415] and the second largest failure of a regulated thrift.[416] IndyMac Bank's parent corporation was IndyMac Bancorp until the FDIC seized IndyMac Bank.[417] IndyMac Bancorp filed for Chapter 7 bankruptcy in July 2008.[417]\n\nIndyMac Bank was founded as Countrywide Mortgage Investment in 1985 by David S. Loeb and Angelo Mozilo[418][419] as a means of collateralizing Countrywide Financial loans too big to be sold to Freddie Mac and Fannie Mae. In 1997, Countrywide spun off IndyMac as an independent company run by Mike Perry, who remained its CEO until the downfall of the bank in July 2008.[420]\n\nThe primary causes of its failure were largely associated with its business strategy of originating and securitizing Alt-A loans on a large scale. This strategy resulted in rapid growth and a high concentration of risky assets. From its inception as a savings association in 2000, IndyMac grew to the seventh largest savings and loan and ninth largest originator of mortgage loans in the United States. During 2006, IndyMac originated over $90 billion (~$131 billion in 2023) of mortgages.\n\nIndyMac's aggressive growth strategy, use of Alt-A and other nontraditional loan products, insufficient underwriting, credit concentrations in residential real estate in the California and Florida markets—states, alongside Nevada and Arizona, where the housing bubble was most pronounced—and heavy reliance on costly funds borrowed from a Federal Home Loan Bank (FHLB) and from brokered deposits, led to its demise when the mortgage market declined in 2007.\n\nIndyMac often made loans without verification of the borrower's income or assets, and to borrowers with poor credit histories. Appraisals obtained by IndyMac on underlying collateral were often questionable as well. As an Alt-A lender, IndyMac's business model was to offer loan products to fit the borrower's needs, using an extensive array of risky option-adjustable-rate mortgages (option ARMs), subprime loans, 80/20 loans, and other nontraditional products. Ultimately, loans were made to many borrowers who simply could not afford to make their payments. The thrift remained profitable only as long as it was able to sell those loans in the secondary mortgage market. IndyMac resisted efforts to regulate its involvement in those loans or tighten their issuing criteria: see the comment by Ruthann Melbourne, Chief Risk Officer, to the regulating agencies.[421][422][423]\n\nOn May 12, 2008, in the \"Capital\" section of its last 10-Q, IndyMac revealed that it may not be well capitalized in the future.[424]\n\nIndyMac reported that during April 2008, Moody's and Standard & Poor's downgraded the ratings on a significant number of Mortgage-backed security (MBS) bonds—including $160 million (~$222 million in 2023) issued by IndyMac that the bank retained in its MBS portfolio. IndyMac concluded that these downgrades would have harmed its risk-based capital ratio as of June 30, 2008. Had these lowered ratings been in effect on March 31, 2008, IndyMac concluded that the bank's capital ratio would have been 9.27% total risk-based. IndyMac warned that if its regulators found its capital position to have fallen below \"well capitalized\" (minimum 10% risk-based capital ratio) to \"adequately capitalized\" (8–10% risk-based capital ratio) the bank might no longer be able to use brokered deposits as a source of funds.\n\nSenator Charles Schumer (D-NY) later pointed out that brokered deposits made up more than 37% of IndyMac's total deposits, and ask the Federal Deposit Insurance Corporation (FDIC) whether it had considered ordering IndyMac to reduce its reliance on these deposits.[425] With $18.9 billion in total deposits reported on March 31,[424] Senator Schumer would have been referring to a little over $7 billion in brokered deposits. While the breakout of maturities of these deposits is not known exactly, a simple averaging would have put the threat of brokered deposits loss to IndyMac at $500 million a month, had the regulator disallowed IndyMac from acquiring new brokered deposits on June 30.\n\nIndyMac was taking new measures to preserve capital, such as deferring interest payments on some preferred securities. Dividends on common shares had already been suspended for the first quarter of 2008, after being cut in half the previous quarter. The company still had not secured a significant capital infusion nor found a ready buyer.[426]\n\nIndyMac reported that the bank's risk-based capital was only $47 million above the minimum required for this 10% mark. But it did not reveal some of that $47 million (~$65.3 million in 2023) capital it claimed it had, as of March 31, 2008, was fabricated.[427]\n\nWhen home prices declined in the latter half of 2007 and the secondary mortgage market collapsed, IndyMac was forced to hold $10.7 billion (~$15.2 billion in 2023) of loans it could not sell in the secondary market. Its reduced liquidity was further exacerbated in late June 2008 when account holders withdrew $1.55 billion (~$2.15 billion in 2023) or about 7.5% of IndyMac's deposits.[424] This bank run on the thrift followed the public release of a letter from Senator Charles Schumer to the FDIC and OTS. The letter outlined the Senator's concerns with IndyMac. While the run was a contributing factor in the timing of IndyMac's demise, the underlying cause of the failure was the unsafe and unsound way it was operated.[421]\n\nOn June 26, 2008, Senator Charles Schumer (D-NY), a member of the Senate Banking Committee, chairman of Congress' Joint Economic Committee and the third-ranking Democrat in the Senate, released several letters he had sent to regulators, in which he was\"concerned that IndyMac's financial deterioration poses significant risks to both taxpayers and borrowers.\" Some worried depositors began to withdraw money.[428][429]\n\nOn July 7, 2008, IndyMac announced on the company blog that it:\n\nIndyMac announced the closure of both its retail lending and wholesale divisions, halted new loan submissions, and cut 3,800 jobs.[430]\n\nOn July 11, 2008, citing liquidity concerns, the FDIC put IndyMac Bank into conservatorship. A bridge bank, IndyMac Federal Bank, FSB, was established to assume control of IndyMac Bank's assets, its secured liabilities, and its insured deposit accounts. The FDIC announced plans to open IndyMac Federal Bank, FSB on July 14, 2008. Until then, depositors would have access to their insured deposits through ATMs, their existing checks, and their existing debit cards. Telephone and Internet account access was restored when the bank reopened.[127][431][432] The FDIC guarantees the funds of all insured accounts up to US$100,000, and declared a special advance dividend to the roughly 10,000 depositors with funds in excess of the insured amount, guaranteeing 50% of any amounts in excess of $100,000.[416] Yet, even with the pending sale of Indymac to IMB Management Holdings, an estimated 10,000 uninsured depositors of Indymac are still at a loss of over $270 million.[433][434]\n\nWith $32 billion in assets, IndyMac Bank was one of the largest bank failures in American history.[435]\n\nIndyMac Bancorp filed for Chapter 7 bankruptcy on July 31, 2008.[417]\n\nInitially the companies affected were those directly involved in home construction and mortgage lending such as Northern Rock and Countrywide Financial, as they could no longer obtain financing through the credit markets. Over 100 mortgage lenders went bankrupt during 2007 and 2008. Concerns that investment bank Bear Stearns would collapse in March 2008 resulted in its fire-sale to JP Morgan Chase. The financial institution crisis hit its peak in September and October 2008. Several major institutions either failed, were acquired under duress, or were subject to government takeover. These included Lehman Brothers, Merrill Lynch, Fannie Mae, Freddie Mac, Washington Mutual, Wachovia, Citigroup, and AIG.[38] On October 6, 2008, three weeks after Lehman Brothers filed the largest bankruptcy in U.S. history, Lehman's former CEO Richard S. Fuld Jr. found himself before Representative Henry A. Waxman, the California Democrat who chaired the House Committee on Oversight and Government Reform. Fuld said he was a victim of the collapse, blaming a \"crisis of confidence\" in the markets for dooming his firm.[436]\n\nThe initial articles and some subsequent material were adapted from the Wikinfo article Financial crisis of 2007–2008 released under the GNU Free Documentation License Version 1.2\n\nReports on causes\n\nJournalism and interviews\n"
    },
    {
        "title": "Vaccine",
        "url": "https://en.wikipedia.org/wiki/Vaccine",
        "content": "\n\nA vaccine is a biological preparation that provides active acquired immunity to a particular infectious or malignant disease.[1][2] The safety and effectiveness of vaccines has been widely studied and verified.[3][4] A vaccine typically contains an agent that resembles a disease-causing microorganism and is often made from weakened or killed forms of the microbe, its toxins, or one of its surface proteins. The agent stimulates the body's immune system to recognize the agent as a threat, destroy it, and recognize further and destroy any of the microorganisms associated with that agent that it may encounter in the future.\n\nVaccines can be  prophylactic (to prevent or alleviate the effects of a future infection by a natural or \"wild\" pathogen), or therapeutic (to fight a disease that has already occurred, such as cancer).[5][6][7][8] Some vaccines offer full sterilizing immunity, in which infection is prevented.[9]\n\nThe administration of vaccines is called vaccination. Vaccination is the most effective method of preventing infectious diseases;[10] widespread immunity due to vaccination is largely responsible for the worldwide eradication of smallpox and the restriction of diseases such as polio, measles, and tetanus from much of the world. The World Health Organization (WHO) reports that licensed vaccines are currently available for twenty-five different preventable infections.[11]\n\nThe first recorded use of inoculation to prevent smallpox occurred in the 16th century in China, with the earliest hints of the practice in China coming during the 10th century.[12] It was also the first disease for which a vaccine was produced.[13][14] The folk practice of inoculation against smallpox was brought from Turkey to Britain in 1721 by Lady Mary Wortley Montagu.[15]\nThe terms vaccine and vaccination are derived from Variolae vaccinae (smallpox of the cow), the term devised by Edward Jenner (who both developed the concept of vaccines and created the first vaccine) to denote cowpox. He used the phrase in 1798 for the long title of his Inquiry into the Variolae vaccinae Known as the Cow Pox, in which he described the protective effect of cowpox against smallpox.[16] In 1881, to honor Jenner, Louis Pasteur proposed that the terms should be extended to cover the new protective inoculations then being developed.[17] The science of vaccine development and production is termed vaccinology.\n\nThere is overwhelming scientific consensus that vaccines are a very safe and effective way to fight and eradicate infectious diseases.[19][20][21][22] The immune system recognizes vaccine agents as foreign, destroys them, and \"remembers\" them. When the virulent version of an agent is encountered, the body recognizes the protein coat on the agent, and thus is prepared to respond, by first neutralizing the target agent before it can enter cells, and secondly by recognizing and destroying infected cells before that agent can multiply to vast numbers.[23][24]\n\nLimitations to their effectiveness, nevertheless, exist.[25] Sometimes, protection fails for vaccine-related reasons such as failures in vaccine attenuation, vaccination regimens or administration.[26]\n\nFailure may also occur for host-related reasons if the host's immune system does not respond adequately or at all. Host-related lack of response occurs in an estimated 2-10% of individuals, due to factors including genetics, immune status, age, health and nutritional status.[26] One type of primary immunodeficiency disorder resulting in genetic failure is X-linked agammaglobulinemia, in which the absence of an enzyme essential for B cell development prevents the host's immune system from generating antibodies to a pathogen.[27][28]\n\nHost–pathogen interactions and responses to infection are dynamic processes involving multiple pathways in the immune system.[29][30] A host does not develop antibodies instantaneously: while the body's innate immunity may be activated in as little as twelve hours, adaptive immunity can take 1–2 weeks to fully develop. During that time, the host can still become infected.[31]\n\nOnce antibodies are produced, they may promote immunity in any of several ways, depending on the class of antibodies involved. Their success in clearing or inactivating a pathogen will depend on the amount of antibodies produced and on the extent to which those antibodies are effective at countering the strain of the pathogen involved, since different strains may be differently susceptible to a given immune reaction.[30] \nIn some cases vaccines may result in partial immune protection (in which immunity is less than 100% effective but still reduces risk of infection) or in temporary immune protection (in which immunity wanes over time) rather than full or permanent immunity. They can still raise the reinfection threshold for the population as a whole and make a substantial impact.[32] They can also mitigate the severity of infection, resulting in a lower mortality rate, lower morbidity, faster recovery from illness, and a wide range of other effects.[33][34]\n\nThose who are older often display less of a response than those who are younger, a pattern known as Immunosenescence.[35]\nAdjuvants commonly are used to boost immune response, particularly for older people whose immune response to a simple vaccine may have weakened.[36]\n\nThe efficacy or performance of the vaccine is dependent on several factors:\n\nIf a vaccinated individual does develop the disease vaccinated against (breakthrough infection), the disease is likely to be less virulent than in unvaccinated cases.[38]\n\nImportant considerations in an effective vaccination program:[39]\n\nIn 1958, there were 763,094 cases of measles in the United States; 552 deaths resulted.[40][41] After the introduction of new vaccines, the number of cases dropped to fewer than 150 per year (median of 56).[41] In early 2008, there were 64 suspected cases of measles. Fifty-four of those infections were associated with importation from another country, although only thirteen percent were actually acquired outside the United States; 63 of the 64 individuals either had never been vaccinated against measles or were uncertain whether they had been vaccinated.[41]\n\nVaccines led to the eradication of smallpox, one of the most contagious and deadly diseases in humans.[42] Other diseases such as rubella, polio, measles, mumps, chickenpox, and typhoid are nowhere near as common as they were a hundred years ago thanks to widespread vaccination programs. As long as the vast majority of people are vaccinated, it is much more difficult for an outbreak of disease to occur, let alone spread. This effect is called herd immunity. Polio, which is transmitted only among humans, is targeted by an extensive eradication campaign that has seen endemic polio restricted to only parts of three countries (Afghanistan, Nigeria, and Pakistan).[43] However, the difficulty of reaching all children, cultural misunderstandings, and disinformation have caused the anticipated eradication date to be missed several times.[44][45][46][47]\n\nVaccines also help prevent the development of antibiotic resistance. For example, by greatly reducing the incidence of pneumonia caused by Streptococcus pneumoniae, vaccine programs have greatly reduced the prevalence of infections resistant to penicillin or other first-line antibiotics.[48]\n\nThe measles vaccine is estimated to prevent a million deaths every year.[49]\n\nVaccinations given to children, adolescents, or adults are generally safe.[50][51] Adverse effects, if any, are generally mild.[52] The rate of side effects depends on the vaccine in question.[52] Some common side effects include fever, pain around the injection site, and muscle aches.[52] Additionally, some individuals may be allergic to ingredients in the vaccine.[53] MMR vaccine is rarely associated with febrile seizures.[51]\n\nHost-(\"vaccinee\")-related determinants that render a person susceptible to infection, such as genetics, health status (underlying disease, nutrition, pregnancy, sensitivities or allergies), immune competence, age, and economic impact or cultural environment can be primary or secondary factors affecting the severity of infection and response to a vaccine.[26] Elderly (above age 60), allergen-hypersensitive, and obese people have susceptibility to compromised immunogenicity, which prevents or inhibits vaccine effectiveness, possibly requiring separate vaccine technologies for these specific populations or repetitive booster vaccinations to limit virus transmission.[26]\n\nSevere side effects are extremely rare.[51] Varicella vaccine is rarely associated with complications in immunodeficient individuals, and rotavirus vaccines are moderately associated with intussusception.[51]\n\nAt least 19 countries have no-fault compensation programs to provide compensation for those with severe adverse effects of vaccination.[54] The United States' program is known as the National Childhood Vaccine Injury Act, and the United Kingdom employs the Vaccine Damage Payment.\n\nVaccines typically contain attenuated, inactivated or dead organisms or purified products derived from them. There are several types of vaccines in use.[55] These represent different strategies used to try to reduce the risk of illness while retaining the ability to induce a beneficial immune response.\n\nSome vaccines contain live, attenuated microorganisms. Many of these are active viruses that have been cultivated under conditions that disable their virulent properties, or that use closely related but less dangerous organisms to produce a broad immune response. Although most attenuated vaccines are viral, some are bacterial in nature. Examples include the viral diseases yellow fever, measles, mumps, and rubella, and the bacterial disease typhoid. The live Mycobacterium tuberculosis vaccine developed by Calmette and Guérin is not made of a contagious strain but contains a virulently modified strain called \"BCG\" used to elicit an immune response to the vaccine. The live attenuated vaccine containing strain Yersinia pestis EV is used for plague immunization. Attenuated vaccines have some advantages and disadvantages. Attenuated, or live, weakened, vaccines typically provoke more durable immunological responses. Attenuated vaccines also elicit a cellular and humoral response. However, they may not be safe for use in immunocompromised individuals, and on rare occasions mutate to a virulent form and cause disease.[56]\n\n\nSome vaccines contain microorganisms that have been killed or inactivated by physical or chemical means. Examples include IPV (polio vaccine), hepatitis A vaccine, rabies vaccine and most influenza vaccines.[57][58]\nToxoid vaccines are made from inactivated toxic compounds that cause illness rather than the microorganism.[58] Examples of toxoid-based vaccines include tetanus and diphtheria.[58] Not all toxoids are for microorganisms; for example, Crotalus atrox toxoid is used to vaccinate dogs against rattlesnake bites.[59]\n\nRather than introducing an inactivated or attenuated microorganism to an immune system (which would constitute a \"whole-agent\" vaccine), a subunit vaccine uses a fragment of it to create an immune response. One example is the subunit vaccine against hepatitis B, which is composed of only the surface proteins of the virus (previously extracted from the blood serum of chronically infected patients but now produced by recombination of the viral genes into yeast).[60] Other examples include the Gardasil virus-like particle human papillomavirus (HPV) vaccine,[61] the hemagglutinin and neuraminidase subunits of the influenza virus,[58] and edible algae vaccines. A subunit vaccine is being used for plague immunization.[62]\n\nCertain bacteria have a polysaccharide outer coat that is poorly immunogenic. By linking these outer coats to proteins (e.g., toxins), the immune system can be led to recognize the polysaccharide as if it were a protein antigen. This approach is used in the Haemophilus influenzae type B vaccine.[63]\n\nOuter membrane vesicles (OMVs) are naturally immunogenic and can be manipulated to produce potent vaccines. The best known OMV vaccines are those developed for serotype B meningococcal disease.[64][65]\n\nHeterologous vaccines also known as \"Jennerian vaccines\", are vaccines that are pathogens of other animals that either do not cause disease or cause mild disease in the organism being treated. The classic example is Jenner's use of cowpox to protect against smallpox. A current example is the use of BCG vaccine made from Mycobacterium bovis to protect against tuberculosis.[66]\n\nGenetic vaccines are based on the principle of uptake of a nucleic acid into cells, whereupon a protein is produced according to the nucleic acid template. This protein is usually the immunodominant antigen of the pathogen or a surface protein that enables the formation of neutralizing antibodies. The subgroup of genetic vaccines encompass viral vector vaccines, RNA vaccines and DNA vaccines.[citation needed]\n\nViral vector vaccines use a safe virus to insert pathogen genes in the body to produce specific antigens, such as surface proteins, to stimulate an immune response.[67][68] Viruses being researched for use as viral vectors include adenovirus, vaccinia virus, and VSV.\n\nAn mRNA vaccine (or RNA vaccine) is a novel type of vaccine which is composed of the nucleic acid RNA, packaged within a vector such as lipid nanoparticles.[69] Among the COVID-19 vaccines are a number of RNA vaccines to combat the COVID-19 pandemic and some have been approved or have received emergency use authorization in some countries. For example, the Pfizer-BioNTech vaccine and Moderna mRNA vaccine are approved for use in adults and children in the US.[70][71][72]\n\nA DNA vaccine uses a DNA plasmid (pDNA)) that encodes for an antigenic protein originating from the pathogen upon which the vaccine will be targeted. pDNA is inexpensive, stable, and relatively safe, making it an excellent option for vaccine delivery.[73]\n\nThis approach offers a number of potential advantages over traditional approaches, including the stimulation of both B- and T-cell responses, improved vaccine stability, the absence of any infectious agent and the relative ease of large-scale manufacture.[74]\n\nMany innovative vaccines are also in development and use.\n\nWhile most vaccines are created using inactivated or attenuated compounds from microorganisms, synthetic vaccines are composed mainly or wholly of synthetic peptides, carbohydrates, or antigens.[citation needed]\n\nVaccines may be monovalent (also called univalent) or multivalent (also called polyvalent). A monovalent vaccine is designed to immunize against a single antigen or single microorganism.[82] A multivalent or polyvalent vaccine is designed to immunize against two or more strains of the same microorganism, or against two or more microorganisms.[83] The valency of a multivalent vaccine may be denoted with a Greek or Latin prefix (e.g., bivalent, trivalent, or tetravalent/quadrivalent). In certain cases, a monovalent vaccine may be preferable for rapidly developing a strong immune response.[84]\n\nWhen two or more vaccines are mixed in the same formulation, the two vaccines can interfere. This most frequently occurs with live attenuated vaccines, where one of the vaccine components is more robust than the others and suppresses the growth and immune response to the other components.[85]\n\nThis phenomenon was first[when?] noted in the trivalent Sabin polio vaccine, where the amount of serotype 2 virus in the vaccine had to be reduced to stop it from interfering with the \"take\" of the serotype 1 and 3 viruses in the vaccine.[86] It was also noted in a 2001 study to be a problem with dengue vaccines, where the DEN-3 serotype was found to predominate and suppress the response to DEN-1, -2 and -4 serotypes.[87]\n\nVaccines typically contain one or more adjuvants, used to boost the immune response. Tetanus toxoid, for instance, is usually adsorbed onto alum. This presents the antigen in such a way as to produce a greater action than the simple aqueous tetanus toxoid. People who have an adverse reaction to adsorbed tetanus toxoid may be given the simple vaccine when the time comes for a booster.[88]\n\nIn the preparation for the 1990 Persian Gulf campaign, the whole cell pertussis vaccine was used as an adjuvant for anthrax vaccine. This produces a more rapid immune response than giving only the anthrax vaccine, which is of some benefit if exposure might be imminent.[89]\n\nVaccines may also contain preservatives to prevent contamination with bacteria or fungi. Until recent years, the preservative thiomersal (a.k.a. Thimerosal in the US and Japan) was used in many vaccines that did not contain live viruses. As of 2005, the only childhood vaccine in the U.S. that contains thiomersal in greater than trace amounts is the influenza vaccine,[90] which is currently recommended only for children with certain risk factors.[91] Single-dose influenza vaccines supplied in the UK do not list thiomersal in the ingredients. Preservatives may be used at various stages of the production of vaccines, and the most sophisticated methods of measurement might detect traces of them in the finished product, as they may in the environment and population as a whole.[92]\n\nMany vaccines need preservatives to prevent serious adverse effects such as Staphylococcus infection, which in one 1928 incident killed 12 of 21 children inoculated with a diphtheria vaccine that lacked a preservative.[93] Several preservatives are available, including thiomersal, phenoxyethanol, and formaldehyde. Thiomersal is more effective against bacteria, has a better shelf-life, and improves vaccine stability, potency, and safety; but, in the U.S., the European Union, and a few other affluent countries, it is no longer used as a preservative in childhood vaccines, as a precautionary measure due to its mercury content.[94] Although controversial claims have been made that thiomersal contributes to autism, no convincing scientific evidence supports these claims.[95] Furthermore, a 10–11-year study of 657,461 children found that the MMR vaccine does not cause autism and actually reduced the risk of autism by seven percent.[96][97]\n\nBeside the active vaccine itself, the following excipients and residual manufacturing compounds are present or may be present in vaccine preparations:[98]\n\nVarious fairly standardized abbreviations for vaccine names have developed, although the standardization is by no means centralized or global. For example, the vaccine names used in the United States have well-established abbreviations that are also widely known and used elsewhere. An extensive list of them provided in a sortable table and freely accessible is available at a US Centers for Disease Control and Prevention web page.[100] The page explains that \"The abbreviations [in] this table (Column 3) were standardized jointly by staff of the Centers for Disease Control and Prevention, ACIP Work Groups, the editor of the Morbidity and Mortality Weekly Report (MMWR), the editor of Epidemiology and Prevention of Vaccine-Preventable Diseases (the Pink Book), ACIP members, and liaison organizations to the ACIP.\"[100]\n\nSome examples are \"DTaP\" for diphtheria and tetanus toxoids and acellular pertussis vaccine, \"DT\" for diphtheria and tetanus toxoids, and \"Td\" for tetanus and diphtheria toxoids. At its page on tetanus vaccination,[101] the CDC further explains that \"Upper-case letters in these abbreviations denote full-strength doses of diphtheria (D) and tetanus (T) toxoids and pertussis (P) vaccine. Lower-case \"d\" and \"p\" denote reduced doses of diphtheria and pertussis used in the adolescent/adult-formulations. The 'a' in DTaP and Tdap stands for 'acellular', meaning that the pertussis component contains only a part of the pertussis organism.\"[101]\n\nAnother list of established vaccine abbreviations is at the CDC's page called \"Vaccine Acronyms and Abbreviations\", with abbreviations used on U.S. immunization records.[102] The United States Adopted Name system has some conventions for the word order of vaccine names, placing head nouns first and adjectives postpositively. This is why the USAN for \"OPV\" is \"poliovirus vaccine live oral\" rather than \"oral poliovirus vaccine\".\n\nA vaccine licensure occurs after the successful conclusion of the development cycle and further the clinical trials and other programs involved through Phases I–III demonstrating safety, immunoactivity, immunogenetic safety at a given specific dose, proven effectiveness in preventing infection for target populations, and enduring preventive effect (time endurance or need for revaccination must be estimated).[103] Because preventive vaccines are predominantly evaluated in healthy population cohorts and distributed among the general population, a high standard of safety is required.[104] As part of a multinational licensing of a vaccine, the World Health Organization Expert Committee on Biological Standardization developed guidelines of international standards for manufacturing and quality control of vaccines, a process intended as a platform for national regulatory agencies to apply for their own licensing process.[103] Vaccine manufacturers do not receive licensing until a complete clinical cycle of development and trials proves the vaccine is safe and has long-term effectiveness, following scientific review by a multinational or national regulatory organization, such as the European Medicines Agency (EMA) or the US Food and Drug Administration (FDA).[105][106]\n\nUpon developing countries adopting WHO guidelines for vaccine development and licensure, each country has its own responsibility to issue a national licensure, and to manage, deploy, and monitor the vaccine throughout its use in each nation.[103] Building trust and acceptance of a licensed vaccine among the public is a task of communication by governments and healthcare personnel to ensure a vaccination campaign proceeds smoothly, saves lives, and enables economic recovery.[107][108] When a vaccine is licensed, it will initially be in limited supply due to variable manufacturing, distribution, and logistical factors, requiring an allocation plan for the limited supply and which population segments should be prioritized to first receive the vaccine.[107]\n\nVaccines developed for multinational distribution via the United Nations Children's Fund (UNICEF) require pre-qualification by the WHO to ensure international standards of quality, safety, immunogenicity, and efficacy for adoption by numerous countries.[103]\n\nThe process requires manufacturing consistency at WHO-contracted laboratories following Good Manufacturing Practice (GMP).[103] When UN agencies are involved in vaccine licensure, individual nations collaborate by 1) issuing marketing authorization and a national license for the vaccine, its manufacturers, and distribution partners; and 2) conducting postmarketing surveillance, including records for adverse events after the vaccination program. The WHO works with national agencies to monitor inspections of manufacturing facilities and distributors for compliance with GMP and regulatory oversight.[103]\n\nSome countries choose to buy vaccines licensed by reputable national organizations, such as EMA, FDA, or national agencies in other affluent countries, but such purchases typically are more expensive and may not have distribution resources suitable to local conditions in developing countries.[103]\n\nIn the European Union (EU), vaccines for pandemic pathogens, such as seasonal influenza, are licensed EU-wide where all the member states comply (\"centralized\"), are licensed for only some member states (\"decentralized\"), or are licensed on an individual national level.[105] Generally, all EU states follow regulatory guidance and clinical programs defined by the European Committee for Medicinal Products for Human Use (CHMP), a scientific panel of the European Medicines Agency (EMA) responsible for vaccine licensure.[105] The CHMP is supported by several expert groups who assess and monitor the progress of a vaccine before and after licensure and distribution.[105]\n\nUnder the FDA, the process of establishing evidence for vaccine clinical safety and efficacy is the same as for the approval process for prescription drugs.[109] If successful through the stages of clinical development, the vaccine licensing process is followed by a Biologics License Application which must provide a scientific review team (from diverse disciplines, such as physicians, statisticians, microbiologists, chemists) and comprehensive documentation for the vaccine candidate having efficacy and safety throughout its development. Also during this stage, the proposed manufacturing facility is examined by expert reviewers for GMP compliance, and the label must have a compliant description to enable health care providers' definition of vaccine-specific use, including its possible risks, to communicate and deliver the vaccine to the public.[109] After licensure, monitoring of the vaccine and its production, including periodic inspections for GMP compliance, continue as long as the manufacturer retains its license, which may include additional submissions to the FDA of tests for potency, safety, and purity for each vaccine manufacturing step.[109]\n\nIn India, the Drugs Controller General, the head of department of the Central Drugs Standard Control Organization, India's national regulatory body for cosmetics, pharmaceuticals and medical devices, is responsible for the approval of licences for specified categories of drugs such as vaccines and other medicinal items, such as blood or blood products, IV fluids, and sera.[110]\n\nUntil a vaccine is in use amongst the general population, all potential adverse events from the vaccine may not be known, requiring manufacturers to conduct Phase IV studies for postmarketing surveillance of the vaccine while it is used widely in the public.[103][109] The WHO works with UN member states to implement post-licensing surveillance.[103] The FDA relies on a Vaccine Adverse Event Reporting System to monitor safety concerns about a vaccine throughout its use in the American public.[109]\n\nIn order to provide the best protection, children are recommended to receive vaccinations as soon as their immune systems are sufficiently developed to respond to particular vaccines, with additional \"booster\" shots often required to achieve \"full immunity\". This has led to the development of complex vaccination schedules. Global recommendations of vaccination schedule are issued by Strategic Advisory Group of Experts and will be further translated by advisory committee at the country level with considering of local factors such as disease epidemiology, acceptability of vaccination, equity in local populations, and programmatic and financial constraint.[111] In the United States, the Advisory Committee on Immunization Practices, which recommends schedule additions for the Centers for Disease Control and Prevention, recommends routine vaccination of children against[112] hepatitis A, hepatitis B, polio, mumps, measles, rubella, diphtheria, pertussis, tetanus, HiB, chickenpox, rotavirus, influenza, meningococcal disease and pneumonia.[113]\n\nThe large number of vaccines and boosters recommended (up to 24 injections by age two) has led to problems with achieving full compliance. To combat declining compliance rates, various notification systems have been instituted and many combination injections are now marketed (e.g., Pentavalent vaccine and MMRV vaccine), which protect against multiple diseases.\n\nBesides recommendations for infant vaccinations and boosters, many specific vaccines are recommended for other ages or for repeated injections throughout life – most commonly for measles, tetanus, influenza, and pneumonia. Pregnant women are often screened for continued resistance to rubella. The human papillomavirus vaccine is recommended in the U.S. (as of 2011)[114] and UK (as of 2009).[115] Vaccine recommendations for the elderly concentrate on pneumonia and influenza, which are more deadly to that group. In 2006, a vaccine was introduced against shingles, a disease caused by the chickenpox virus, which usually affects the elderly.[116]\n\nScheduling and dosing of a vaccination may be tailored to the level of immunocompetence of an individual[117] and to optimize population-wide deployment of a vaccine when it supply is limited,[118] e.g. in the setting of a pandemic.\n\nOne challenge in vaccine development is economic: Many of the diseases most demanding a vaccine, including HIV, malaria and tuberculosis, exist principally in poor countries. Pharmaceutical firms and biotechnology companies have little incentive to develop vaccines for these diseases because there is little revenue potential. Even in more affluent countries, financial returns are usually minimal and the financial and other risks are great.[119]\n\nMost vaccine development to date has relied on \"push\" funding by government, universities and non-profit organizations.[120] Many vaccines have been highly cost effective and beneficial for public health.[121] The number of vaccines actually administered has risen dramatically in recent decades.[122] This increase, particularly in the number of different vaccines administered to children before entry into schools may be due to government mandates and support, rather than economic incentive.[123]\n\nAccording to the World Health Organization, the biggest barrier to vaccine production in less developed countries has not been patents, but the substantial financial, infrastructure, and workforce requirements needed for market entry. Vaccines are complex mixtures of biological compounds, and unlike the case for prescription drugs, there are no true generic vaccines. The vaccine produced by a new facility must undergo complete clinical testing for safety and efficacy by the manufacturer. For most vaccines, specific processes in technology are patented. These can be circumvented by alternative manufacturing methods, but this required R&D infrastructure and a suitably skilled workforce. In the case of a few relatively new vaccines, such as the human papillomavirus vaccine, the patents may impose an additional barrier.[124]\n\nWhen increased production of vaccines was urgently needed during the COVID-19 pandemic in 2021, the World Trade Organization and governments around the world evaluated whether to waive intellectual property rights and patents on COVID-19 vaccines, which would \"eliminate all potential barriers to the timely access of affordable COVID-19 medical products, including vaccines and medicines, and scale up the manufacturing and supply of essential medical products\".[125]\n\nVaccine production is fundamentally different from other kinds of manufacturing – including regular pharmaceutical manufacturing – in that vaccines are intended to be administered to millions of people of whom the vast majority are perfectly healthy.[126] This fact drives an extraordinarily rigorous production process with strict compliance requirements that go far beyond what is required of other products.[126]\n\nDepending upon the antigen, it can cost anywhere from US$50 to $500 million to build a vaccine production facility, which requires highly specialized equipment, clean rooms, and containment rooms.[127] There is a global scarcity of personnel with the right combination of skills, expertise, knowledge, competence and personality to staff vaccine production lines.[127] With the notable exceptions of Brazil, China, and India, many developing countries' educational systems are unable to provide enough qualified candidates, and vaccine makers based in such countries must hire expatriate personnel to keep production going.[127]\n\nVaccine production has several stages. First, the antigen itself is generated. Viruses are grown either on primary cells such as chicken eggs (e.g., for influenza) or on continuous cell lines such as cultured human cells (e.g., for hepatitis A).[128] Bacteria are grown in bioreactors (e.g., Haemophilus influenzae type b). Likewise, a recombinant protein derived from the viruses or bacteria can be generated in yeast, bacteria, or cell cultures.[129][130]\n\nAfter the antigen is generated, it is isolated from the cells used to generate it. A virus may need to be inactivated, possibly with no further purification required. Recombinant proteins need many operations involving ultrafiltration and column chromatography. Finally, the vaccine is formulated by adding adjuvant, stabilizers, and preservatives as needed. The adjuvant enhances the immune response to the antigen, stabilizers increase the storage life, and preservatives allow the use of multidose vials.[129][130] Combination vaccines are harder to develop and produce, because of potential incompatibilities and interactions among the antigens and other ingredients involved.[131]\n\nThe final stage in vaccine manufacture before distribution is fill and finish, which is the process of filling vials with vaccines and packaging them for distribution. Although this is a conceptually simple part of the vaccine manufacture process, it is often a bottleneck in the process of distributing and administering vaccines.[132][133][134]\n\nVaccine production techniques are evolving. Cultured mammalian cells are expected to become increasingly important, compared to conventional options such as chicken eggs, due to greater productivity and low incidence of problems with contamination. Recombination technology that produces genetically detoxified vaccines is expected to grow in popularity for the production of bacterial vaccines that use toxoids. Combination vaccines are expected to reduce the quantities of antigens they contain, and thereby decrease undesirable interactions, by using pathogen-associated molecular patterns.[131]\n\nThe companies with the highest market share in vaccine production are Merck, Sanofi, GlaxoSmithKline, Pfizer and Novartis, with 70% of vaccine sales concentrated in the EU or US (2013).[135]: 42  Vaccine manufacturing plants require large capital investments ($50 million up to $300 million) and may take between 4 and 6 years to construct, with the full process of vaccine development taking between 10 and 15 years.[135]: 43  Manufacturing in developing countries is playing an increasing role in supplying these countries, specifically with regards to older vaccines and in Brazil, India and China.[135]: 47  The manufacturers in India are the most advanced in the developing world and include the Serum Institute of India, one of the largest producers of vaccines by number of doses and an innovator in processes, recently improving efficiency of producing the measles vaccine by 10 to 20-fold, due to switching to a MRC-5 cell culture instead of chicken eggs.[135]: 48  China's manufacturing capabilities are focused on supplying their own domestic need, with Sinopharm (CNPGC) alone providing over 85% of the doses for 14 different vaccines in China.[135]: 48  Brazil is approaching the point of supplying its own domestic needs using technology transferred from the developed world.[135]: 49 \n\nOne of the most common methods of delivering vaccines into the human body is injection.\n\nThe development of new delivery systems raises the hope of vaccines that are safer and more efficient to deliver and administer. Lines of research include liposomes and ISCOM (immune stimulating complex).[136]\n\nNotable developments in vaccine delivery technologies have included oral vaccines. Early attempts to apply oral vaccines showed varying degrees of promise, beginning early in the 20th century, at a time when the very possibility of an effective oral antibacterial vaccine was controversial.[137] By the 1930s there was increasing interest in the prophylactic value of an oral typhoid fever vaccine for example.[138]\n\nAn oral polio vaccine turned out to be effective when vaccinations were administered by volunteer staff without formal training; the results also demonstrated increased ease and efficiency of administering the vaccines. Effective oral vaccines have many advantages; for example, there is no risk of blood contamination. Vaccines intended for oral administration need not be liquid, and as solids, they commonly are more stable and less prone to damage or spoilage by freezing in transport and storage.[139] Such stability reduces the need for a \"cold chain\": the resources required to keep vaccines within a restricted temperature range from the manufacturing stage to the point of administration, which, in turn, may decrease costs of vaccines.\n\nA microneedle approach, which is still in stages of development, uses \"pointed projections fabricated into arrays that can create vaccine delivery pathways through the skin\".[140]\n\nAn experimental needle-free[141] vaccine delivery system is undergoing animal testing.[142][143] A stamp-size patch similar to an adhesive bandage contains about 20,000 microscopic projections per square cm.[144] This dermal administration potentially increases the effectiveness of vaccination, while requiring less vaccine than injection.[145]\n\nVaccinations of animals are used both to prevent their contracting diseases and to prevent transmission of disease to humans.[146] Both animals kept as pets and animals raised as livestock are routinely vaccinated. In some instances, wild populations may be vaccinated. This is sometimes accomplished with vaccine-laced food spread in a disease-prone area and has been used to attempt to control rabies in raccoons.\n\nWhere rabies occurs, rabies vaccination of dogs may be required by law. Other canine vaccines include canine distemper, canine parvovirus, infectious canine hepatitis, adenovirus-2, leptospirosis, Bordetella, canine parainfluenza virus, and Lyme disease, among others.\n\nCases of veterinary vaccines used in humans have been documented, whether intentional or accidental, with some cases of resultant illness, most notably with brucellosis.[147] However, the reporting of such cases is rare and very little has been studied about the safety and results of such practices. With the advent of aerosol vaccination in veterinary clinics, human exposure to pathogens not naturally carried in humans, such as Bordetella bronchiseptica, has likely increased in recent years.[147] In some cases, most notably rabies, the parallel veterinary vaccine against a pathogen may be as much as orders of magnitude more economical than the human one.\n\nDIVA (Differentiation of Infected from Vaccinated Animals), also known as SIVA (Segregation of Infected from Vaccinated Animals) vaccines, make it possible to differentiate between infected and vaccinated animals. DIVA vaccines carry at least one epitope less than the equivalent wild microorganism. An accompanying diagnostic test that detects the antibody against that epitope assists in identifying whether the animal has been vaccinated or not.[citation needed]\n\nThe first DIVA vaccines (formerly termed marker vaccines and since 1999 coined as DIVA vaccines) and companion diagnostic tests were developed by J. T. van Oirschot and colleagues at the Central Veterinary Institute in Lelystad, The Netherlands.[148][149] They found that some existing vaccines against pseudorabies (also termed Aujeszky's disease) had deletions in their viral genome (among which was the gE gene). Monoclonal antibodies were produced against that deletion and selected to develop an ELISA that demonstrated antibodies against gE. In addition, novel genetically engineered gE-negative vaccines were constructed.[150] Along the same lines, DIVA vaccines and companion diagnostic tests against bovine herpesvirus 1 infections have been developed.[149][151]\n\nThe DIVA strategy has been applied in various countries to successfully eradicate pseudorabies virus from those countries. Swine populations were intensively vaccinated and monitored by the companion diagnostic test and, subsequently, the infected pigs were removed from the population. Bovine herpesvirus 1 DIVA vaccines are also widely used in practice.[citation needed] Considerable efforts are ongoing to apply the DIVA principle to a wide range of infectious diseases, such as classical swine fever,[152] avian influenza,[153] Actinobacillus pleuropneumonia[154] and Salmonella infections in pigs.[155]\n\nPrior to the introduction of vaccination with material from cases of cowpox (heterotypic immunisation), smallpox could be prevented by deliberate variolation with smallpox virus. The earliest hints of the practice of variolation for smallpox in China come during the tenth century.[156][further explanation needed] The Chinese also practiced the oldest documented use of variolation, dating back to the fifteenth century. They implemented a method of \"nasal insufflation\" administered by blowing powdered smallpox material, usually scabs, up the nostrils. Various insufflation techniques have been recorded throughout the sixteenth and seventeenth centuries within China.[157]: 60  Two reports on the Chinese practice of inoculation were received by the Royal Society in London in 1700; one by Martin Lister who received a report by an employee of the East India Company stationed in China and another by Clopton Havers.[158] In France, Voltaire reports that the Chinese have practiced variolation \"these hundred years\".[159]\n\nMary Wortley Montagu, who had witnessed variolation in Turkey, had her four-year-old daughter variolated in the presence of physicians of the Royal Court in 1721 upon her return to England.[157] Later on that year, Charles Maitland conducted an experimental variolation of six prisoners in Newgate Prison in London.[160] The experiment was a success, and soon variolation was drawing attention from the royal family, who helped promote the procedure. However, in 1783, several days after Prince Octavius of Great Britain was inoculated, he died.[161] In 1796, the physician Edward Jenner took pus from the hand of a milkmaid with cowpox, scratched it into the arm of an 8-year-old boy, James Phipps, and six weeks later variolated the boy with smallpox, afterwards observing that he did not catch smallpox.[162][163] Jenner extended his studies and, in 1798, reported that his vaccine was safe in children and adults, and could be transferred from arm-to-arm, which reduced reliance on uncertain supplies from infected cows.[161] In 1804, the Spanish Balmis smallpox vaccination expedition to Spain's colonies Mexico and Philippines used the arm-to-arm transport method to get around the fact the vaccine survived for only 12 days in vitro. They used cowpox.[164] Since vaccination with cowpox was much safer than smallpox inoculation,[165] the latter, though still widely practiced in England, was banned in 1840.[166]\n\nFollowing on from Jenner's work, the second generation of vaccines was introduced in the 1880s by Louis Pasteur who developed vaccines for chicken cholera and anthrax,[17] and from the late nineteenth century vaccines were considered a matter of national prestige. National vaccination policies were adopted and compulsory vaccination laws were passed.[162] In 1931 Alice Miles Woodruff and Ernest Goodpasture documented that the fowlpox virus could be grown in embryonated chicken egg. Soon scientists began cultivating other viruses in eggs. Eggs were used for virus propagation in the development of a yellow fever vaccine in 1935 and an influenza vaccine in 1945. In 1959 growth media and cell culture replaced eggs as the standard method of virus propagation for vaccines.[167]\n\nVaccinology flourished in the twentieth century, which saw the introduction of several successful vaccines, including those against diphtheria, measles, mumps, and rubella. Major achievements included the development of the polio vaccine in the 1950s and the eradication of smallpox during the 1960s and 1970s. Maurice Hilleman was the most prolific of the developers of the vaccines in the twentieth century. As vaccines became more common, many people began taking them for granted. However, vaccines remain elusive for many important diseases, including herpes simplex, malaria, gonorrhea, and HIV.[162][168]\n\nFirst generation vaccines are whole-organism vaccines – either live and weakened, or killed forms.[169] Live, attenuated vaccines, such as smallpox and polio vaccines, are able to induce killer T-cell (TC or CTL) responses, helper T-cell (TH) responses and antibody immunity. However, attenuated forms of a pathogen can convert to a dangerous form and may cause disease in immunocompromised vaccine recipients (such as those with AIDS). While killed vaccines do not have this risk, they cannot generate specific killer T-cell responses and may not work at all for some diseases.[169]\n\nSecond generation vaccines were developed to reduce the risks from live vaccines. These are subunit vaccines, consisting of specific protein antigens (such as tetanus or diphtheria toxoid) or recombinant protein components (such as the hepatitis B surface antigen). They can generate TH and antibody responses, but not killer T cell responses.[citation needed]\n\nRNA vaccines and DNA vaccines are examples of third generation vaccines.[169][170][171] In 2016 a DNA vaccine for the Zika virus began testing at the National Institutes of Health. Separately, Inovio Pharmaceuticals and GeneOne Life Science began tests of a different DNA vaccine against Zika in Miami. Manufacturing the vaccines in volume was unsolved as of 2016.[172] Clinical trials for DNA vaccines to prevent HIV are underway.[173] mRNA vaccines such as BNT162b2 were developed in the year 2020 with the help of Operation Warp Speed and massively deployed to combat the COVID-19 pandemic. In 2021, Katalin Karikó and Drew Weissman received Columbia University's Horwitz Prize for their pioneering research in mRNA vaccine technology.[174]\n\nSince at least 2013, scientists have been trying to develop synthetic third-generation vaccines by reconstructing the outside structure of a virus; it was hoped that this will help prevent vaccine resistance.[175]\n\nPrinciples that govern the immune response can now be used in tailor-made vaccines against many noninfectious human diseases, such as cancers and autoimmune disorders.[176] For example, the experimental vaccine CYT006-AngQb has been investigated as a possible treatment for high blood pressure.[177] Factors that affect the trends of vaccine development include progress in translatory medicine, demographics, regulatory science, political, cultural, and social responses.[178]\n\nThe idea of vaccine production via transgenic plants was identified as early as 2003. Plants such as tobacco, potato, tomato, and banana can have genes inserted that cause them to produce vaccines usable for humans.[179] In 2005, bananas were developed that produce a human vaccine against hepatitis B.[180]\n\nVaccine hesitancy is a delay in acceptance, or refusal of vaccines despite the availability of vaccine services. The term covers outright refusals to vaccinate, delaying vaccines, accepting vaccines but remaining uncertain about their use, or using certain vaccines but not others.[182][183][184][185] There is an overwhelming scientific consensus that vaccines are generally safe and effective.[186][187][188][189] Vaccine hesitancy often results in disease outbreaks and deaths from vaccine-preventable diseases.[190][191][192][193][194][195] The World Health Organization therefore characterized vaccine hesitancy as one of the top ten global health threats in 2019.[196][197]\n"
    },
    {
        "title": "Mental health",
        "url": "https://en.wikipedia.org/wiki/Mental_health",
        "content": "\n\nMental health encompasses emotional, psychological, and social well-being, influencing cognition, perception, and behavior. According to the World Health Organization (WHO), it is a \"state of well-being in which the individual realizes his or her abilities, can cope with the normal stresses of life, can work productively and fruitfully, and can contribute to his or her community\".[1] It likewise determines how an individual handles stress, interpersonal relationships, and decision-making.[2] Mental health includes subjective well-being, perceived self-efficacy, autonomy, competence, intergenerational dependence,  and self-actualization of one's intellectual and emotional potential, among others.[3]\n\nFrom the perspectives of positive psychology or holism, mental health may include an individual's ability to enjoy life and to create a balance between life activities and efforts to achieve psychological resilience.[4] Cultural differences, personal philosophy, subjective assessments, and competing professional theories all affect how one defines \"mental health\".[5] Some early signs related to mental health difficulties are sleep irritation, lack of energy, lack of appetite, thinking of harming oneself or others, self-isolating (though introversion and isolation aren't necessarily unhealthy), and frequently zoning out.[5]\n\nMental health, as defined by the Public Health Agency of Canada,[6] is an individual's capacity to feel, think, and act in ways to achieve a better quality of life while respecting personal, social, and cultural boundaries.[7] Impairment of any of these are risk factor for mental disorders, or mental illnesses,[8] which are a component of mental health. In 2019, about 970 million people worldwide suffered from a mental disorder, with anxiety and depression being the most common. The number of people suffering from mental disorders has risen significantly throughout the years.[9] Mental disorders are defined as health conditions that affect and alter cognitive functioning, emotional responses, and behavior associated with distress and/or impaired functioning.[10][11] The ICD-11 is the global standard used to diagnose, treat, research, and report various mental disorders.[12][13] In the United States, the DSM-5 is used as the classification system of mental disorders.[14]\n\nMental health is associated with a number of lifestyle factors such as diet, exercise, stress, drug abuse, social connections and interactions.[14][15] Psychiatrists, psychologists, licensed professional clinical counselors, social workers, nurse practitioners, and family physicians can help manage mental illness with treatments such as therapy, counseling, and medication.[16]\n\nIn the mid-19th century, William Sweetser was the first to coin the term mental hygiene, which can be seen as the precursor to contemporary approaches to work on promoting positive mental health.[17][18] Isaac Ray, the fourth president[19] of the American Psychiatric Association and one of its founders, further defined mental hygiene as \"the art of preserving the mind against all incidents and influences calculated to deteriorate its qualities, impair its energies, or derange its movements\".[18]\n\nIn American history, mentally ill patients were thought to be religiously punished. This response persisted through the 1700s, along with the inhumane confinement and stigmatization of such individuals.[20] Dorothea Dix (1802–1887) was an important figure in the development of the \"mental hygiene\" movement. Dix was a school teacher who endeavored to help people with mental disorders and to expose the sub-standard conditions into which they were put.[21] This became known as the \"mental hygiene movement\".[21] Before this movement, it was not uncommon that people affected by mental illness would be considerably neglected, often left alone in deplorable conditions without sufficient clothing.[21] From 1840 to 1880, she won the support of the federal government to set up over 30 state psychiatric hospitals; however, they were understaffed, under-resourced, and were accused of violating human rights.[20]\n\nEmil Kraepelin in 1896 developed the taxonomy of mental disorders which has dominated the field for nearly 80 years. Later, the proposed disease model of abnormality was subjected to analysis and considered normality to be relative to the physical, geographical and cultural aspects of the defining group.[22]\n\nAt the beginning of the 20th century, Clifford Beers founded \"Mental Health America – National Committee for Mental Hygiene\", after publication of his accounts as a patient in several lunatic asylums, A Mind That Found Itself, in 1908[23][24][25] and opened the first outpatient mental health clinic in the United States.[24]\n\nThe mental hygiene movement, similar to the social hygiene movement, had at times been associated with advocating eugenics and sterilization of those considered too mentally deficient to be assisted into productive work and contented family life.[26][27] In the post-WWII years, references to mental hygiene were gradually replaced by the term 'mental health' due to its positive aspect that evolves from the treatment of illness to preventive and promotive areas of healthcare.[25]\n\nWhen US government-run hospitals were accused of violating human rights, advocates pushed for deinstitutionalization: the replacement of federal mental hospitals for community mental health services. The closure of state-provisioned psychiatric hospitals was enforced by the Community Mental Health Centers Act in 1963 that laid out terms in which only patients who posed an imminent danger to others or themselves could be admitted into state facilities.[28] This was seen as an improvement from previous conditions. However, there remains a debate on the conditions of these community resources.\n\nIt has been proven that this transition was beneficial for many patients: there was an increase in overall satisfaction, a better quality of life, and more friendships between patients all at an affordable cost. This proved to be true only in the circumstance that treatment facilities had enough funding for staff and equipment as well as proper management.[29] However, this idea is a polarizing issue. Critics of deinstitutionalization argue that poor living conditions prevailed, patients were lonely, and they did not acquire proper medical care in these treatment homes.[30] Additionally, patients that were moved from state psychiatric care to nursing and residential homes had deficits in crucial aspects of their treatment. Some cases result in the shift of care from health workers to patients' families, where they do not have the proper funding or medical expertise to give proper care.[30] On the other hand, patients that are treated in community mental health centers lack sufficient cancer testing, vaccinations, or otherwise regular medical check-ups.[30]\n\nOther critics of state deinstitutionalization argue that this was simply a transition to \"transinstitutionalization\", or the idea that prisons and state-provisioned hospitals are interdependent. In other words, patients become inmates. This draws on the Penrose Hypothesis of 1939, which theorized that there was an inverse relationship between prisons' population size and the number of psychiatric hospital beds.[31] This means that populations that require psychiatric mental care will transition between institutions, which in this case, includes state psychiatric hospitals and criminal justice systems. Thus, a decrease in available psychiatric hospital beds occurred at the same time as an increase in inmates.[31] Although some are skeptical that this is due to other external factors, others will reason this conclusion to a lack of empathy for the mentally ill. There is no argument for the social stigmatization of those with mental illnesses, they have been widely marginalized and discriminated against in society.[20] In this source, researchers analyze how most compensation prisoners (detainees who are unable or unwilling to pay a fine for petty crimes) are unemployed, homeless, and with an extraordinarily high degree of mental illnesses and substance use disorders.[31] Compensation prisoners then lose prospective job opportunities, face social marginalization, and lack access to resocialization programs, which ultimately facilitate reoffending.[31] The research sheds light on how the mentally ill—and in this case, the poor—are further punished for certain circumstances that are beyond their control, and that this is a vicious cycle that repeats itself. Thus, prisons embody another state-provisioned mental hospital.\n\nFamilies of patients, advocates, and mental health professionals still call for increase in more well-structured community facilities and treatment programs with a higher quality of long-term inpatient resources and care. With this more structured environment, the United States will continue with more access to mental health care and an increase in the overall treatment of the mentally ill.\n\nHowever, there is still a lack of studies for mental health conditions (MHCs) to raise awareness, knowledge development, and attitudes toward seeking medical treatment for MHCs in Bangladesh. People in rural areas often seek treatment from the traditional healers and MHCs are sometimes considered a spiritual matter.[32]\n\nMental illnesses are more common than cancer, diabetes, or heart disease. As of 2021, over 22 percent of all Americans over the age of 18 meet the criteria for having a mental illness.[33] Evidence suggests that 970 million people worldwide have a mental disorder.[34] Major depression ranks third among the top 10 leading causes of disease worldwide. By 2030, it is predicted to become the leading cause of disease worldwide.[35] Over 700 thousand people commit suicide every year and around 14 million attempt it.[36] A World Health Organization (WHO) report estimates the global cost of mental illness at nearly $2.5 trillion (two-thirds in indirect costs) in 2010, with a projected increase to over $6 trillion by 2030.[37]\n\nEvidence from the WHO suggests that nearly half of the world's population is affected by mental illness with an impact on their self-esteem, relationships and ability to function in everyday life.[38] An individual's emotional health can impact their physical health. Poor mental health can lead to problems such as the inability to make adequate decisions and substance use disorders.[39]\n\nGood mental health can improve life quality whereas poor mental health can worsen it. According to Richards, Campania, & Muse-Burke, \"There is growing evidence that is showing emotional abilities are associated with pro-social behaviors such as stress management and physical health.\"[39] Their research also concluded that people who lack emotional expression are inclined to anti-social behaviors (e.g., substance use disorder and alcohol use disorder, physical fights, vandalism), which reflects one's mental health and suppressed emotions.[39] Adults and children who face mental illness may experience social stigma, which can exacerbate the issues.[40]\n\nMental health can be seen as a continuum, where an individual's mental health may have many different possible values.[41] Mental wellness is viewed as a positive attribute; this definition of mental health highlights emotional well-being, the capacity to live a full and creative life, and the flexibility to deal with life's inevitable challenges. Some discussions are formulated in terms of contentment or happiness.[42] Many therapeutic systems and self-help books offer methods and philosophies espousing strategies and techniques vaunted as effective for further improving the mental wellness. Positive psychology is increasingly prominent in mental health.\n\nA holistic model of mental health generally includes concepts based upon anthropological, educational, psychological, religious, and sociological perspectives. There are also models as theoretical perspectives from personality, social, clinical, health and developmental psychology.[43][44]\n\nThe tripartite model of mental well-being[41][45] views mental well-being as encompassing three components of emotional well-being, social well-being, and psychological well-being. Emotional well-being is defined as having high levels of positive emotions, whereas social and psychological well-being are defined as the presence of psychological and social skills and abilities that contribute to optimal functioning in daily life. The model has received empirical support across cultures.[45][46][47] The Mental Health Continuum-Short Form (MHC-SF) is the most widely used scale to measure the tripartite model of mental well-being.[48][49][50]\n\nAs of 2019, about one in seven of the world's 10–19 year olds experienced a mental health disorder; about 165 million young people in total.[51][52] A person's teenage years are a unique period where much crucial psychological development occurs, and is also a time of increased vulnerability to the development of adverse mental health conditions. More than half of mental health conditions start before a child reaches 20 years of age, with onset occurring in adolescence much more frequently than it does in early childhood or adulthood. Many such cases go undetected and untreated.[53][51][54][55]\n\nIn the United States alone, in 2021, at least roughly 17.5% of the population (ages 18 and older) were recorded as having a mental illness. The comparison between reports and statistics of mental health issues in newer generations (18–25 years old to 26–49 years old) and the older generation (50 years or older) signifies an increase in mental health issues as only 15% of the older generation reported a mental health issue whereas the newer generations reported 33.7% (18-25) and 28.1% (26-49).[56]  The role of caregivers for youth with mental health needs is valuable, and caregivers benefit most when they have sufficient psychoeducation and peer support.[57] Depression is one of the leading causes of illness and disability among adolescents.[51] Suicide is the fourth leading cause of death in 15-19-year-olds.[51] Exposure to childhood trauma can cause mental health disorders and poor academic achievement.[58] Ignoring mental health conditions in adolescents can impact adulthood.[59] 50% of preschool children show a natural reduction in behavioral problems. The remaining experience long-term consequences.[59] It impairs physical and mental health and limits opportunities to live fulfilling lives.[59] A result of depression during adolescence and adulthood may be substance abuse.[59][60] The average age of onset is between 11 and 14 years for depressive disorders.[60] Only approximately 25% of children with behavioral problems refer to medical services.[59] The majority of children go untreated.[59]\n\nMental illness is thought to be highly prevalent among homeless populations, though access to proper diagnoses is limited. An article written by Lisa Goodman and her colleagues summarized Smith's research into PTSD in homeless single women and mothers in St. Louis, Missouri, which found that 53% of the respondents met diagnostic criteria, and which describes homelessness as a risk factor for mental illness.[61] At least two commonly reported symptoms of psychological trauma, social disaffiliation and learned helplessness are highly prevalent among homeless individuals and families.[62]\n\nWhile mental illness is prevalent, people infrequently receive appropriate care.[61] Case management linked to other services is an effective care approach for improving symptoms in people experiencing homelessness.[62] Case management reduced admission to hospitals, and it reduced substance use by those with substance abuse problems more than typical care.[62]\n\nStates that produce refugees are sites of social upheaval, civil war, even genocide.[63] Most refugees experience trauma. It can be in the form of torture, sexual assault, family fragmentation, and death of loved ones.[63][64]\n\nRefugees and immigrants experience psychosocial stressors after resettlement.[65] These include discrimination, lack of economic stability, and social isolation causing emotional distress. For example, Not far into the 1900s, campaigns targeting Japanese immigrants were being formed that inhibited their ability to participate in U.S life, painting them as a threat to the American working-class. They were subject to prejudice and slandered by American media as well as anti-Japanese legislation being implemented.[66][63][64] For refugees family reunification can be one of the primary needs to improve quality of life.[63] Post-migration trauma is a cause of depressive disorders and psychological distress for immigrants.[63][64][65]\n\nMental health is a socially constructed concept; different societies, groups, cultures (both ethnic and national/regional), institutions, and professions have very different ways of conceptualizing its nature and causes, determining what is mentally healthy, and deciding what interventions, if any, are appropriate.[67] Thus, different professionals will have different cultural, class, political and religious backgrounds, which will impact the methodology applied during treatment. In the context of deaf mental health care, it is necessary for professionals to have cultural competency of deaf and hard of hearing people and to understand how to properly rely on trained, qualified, and certified interpreters when working with culturally Deaf clients.\n\nResearch has shown that there is stigma attached to mental illness.[68] Due to such stigma, individuals may resist labeling and may be driven to respond to mental health diagnoses with denialism.[69] Family caregivers of individuals with mental disorders may also suffer discrimination or face stigma.[70]\n\nAddressing and eliminating the social stigma and perceived stigma attached to mental illness has been recognized as crucial to education and awareness surrounding mental health issues. In the United Kingdom, the Royal College of Psychiatrists organized the campaign Changing Minds (1998–2003) to help reduce stigma,[71] while in the United States, efforts by entities such as the Born This Way Foundation and The Manic Monologues specifically focus on removing the stigma surrounding mental illness.[72][73] The National Alliance on Mental Illness (NAMI) is a U.S. institution founded in 1979 to represent and advocate for those struggling with mental health issues. NAMI helps to educate about mental illnesses and health issues, while also working to eliminate stigma[74] attached to these disorders.\n\nMany mental health professionals are beginning to, or already understand, the importance of competency in religious diversity and spirituality, or the lack thereof. They are also partaking in cultural training to better understand which interventions work best for these different groups of people. The American Psychological Association explicitly states that religion must be respected. Education in spiritual and religious matters is also required by the American Psychiatric Association,[75] however, far less attention is paid to the damage that more rigid, fundamentalist faiths commonly practiced in the United States can cause.[76][unreliable source?] This theme has been widely politicized in 2018 such as with the creation of the Religious Liberty Task Force in July of that year.[77] Also, many providers and practitioners in the United States are only beginning to realize that the institution of mental healthcare lacks knowledge and competence of many non-Western cultures, leaving providers in the United States ill-equipped to treat patients from different cultures.[78]\n\nOccupational therapy practitioners aim to improve and enable a client or group's participation in meaningful, everyday occupations.[79] In this sense, occupation is defined as any activity that \"occupies one's time\". Examples of those activities include daily tasks (dressing, bathing, eating, house chores, driving, etc.), sleep and rest, education, work, play, leisure (hobbies), and social interactions. The OT profession offers a vast range of services for all stages of life in a myriad of practice settings, though the foundations of OT come from mental health. Community support for mental health through expert-moderated support groups can aid those who want to recover from mental illness or otherwise improve their emotional well-being.[80]\n\nOT services focused on mental health can be provided to persons, groups, and populations [79] across the lifespan and experiencing varying levels of mental health performance. For example, occupational therapy practitioners provide mental health services in school systems, military environments, hospitals, outpatient clinics, and inpatient mental health rehabilitation settings. Interventions or support can be provided directly through specific treatment interventions or indirectly by providing consultation to businesses, schools, or other larger groups to incorporate mental health strategies on a programmatic level. Even people who are mentally healthy can benefit from the health promotion and additional prevention strategies to reduce the impact of difficult situations.\n\nThe interventions focus on positive functioning, sensory strategies, managing emotions, interpersonal relationships, sleep, community engagement, and other cognitive skills (i.e. visual-perceptual skills, attention, memory, arousal/energy management, etc.).\n\nSocial work in mental health, also called psychiatric social work, is a process where an individual in a setting is helped to attain freedom from overlapping internal and external problems (social and economic situations, family and other relationships, the physical and organizational environment, psychiatric symptoms, etc.). It aims for harmony, quality of life, self-actualization and personal adaptation across all systems. Psychiatric social workers are mental health professionals that can assist patients and their family members in coping with both mental health issues and various economic or social problems caused by mental illness or psychiatric dysfunctions and to attain improved mental health and well-being. They are vital members of the treatment teams in Departments of Psychiatry and Behavioral Sciences in hospitals. They are employed in both outpatient and inpatient settings of a hospital, nursing homes, state and local governments, substance use clinics, correctional facilities, health care services, private practice, etc.[81]\n\nIn the United States, social workers provide most of the mental health services. According to government sources, 60 percent of mental health professionals are clinically trained social workers, 10 percent are psychiatrists, 23 percent are psychologists, and 5 percent are psychiatric nurses.[82]\n\nMental health social workers in Japan have professional knowledge of health and welfare and skills essential for person's well-being. Their social work training enables them as a professional to carry out Consultation assistance for mental disabilities and their social reintegration; Consultation regarding the rehabilitation of the victims; Advice and guidance for post-discharge residence and re-employment after hospitalized care, for major life events in regular life, money and self-management and other relevant matters to equip them to adapt in daily life. Social workers provide individual home visits for mentally ill and do welfare services available, with specialized training a range of procedural services are coordinated for home, workplace and school. In an administrative relationship, Psychiatric social workers provides consultation, leadership, conflict management and work direction. Psychiatric social workers who provides assessment and psychosocial interventions function as a clinician, counselor and municipal staff of the health centers.[83]\n\nThere are many things that can contribute to mental health problems, including biological factors, genetic factors, life experiences (such as psychological trauma or abuse), and a family history of mental health problems.[84]\n\nAccording to the National Institute of Health Curriculum Supplement Series book, most scientists believe that changes in neurotransmitters can cause mental illnesses. In the section \"The Biology of Mental Illnesses\" the issue is explained in detail, \"...there may be disruptions in the neurotransmitters dopamine, glutamate, and norepinephrine in individuals who have schizophrenia\".\n[85]\n\nGender, age, ethnicity, life expectancy, longevity, population density, and community diversity are all demographic characteristics that can increase the risk and severity of mental disorders.[86] Existing evidence demonstrates that the female gender is connected with an elevated risk of depression at different phases of life, commencing in adolescence in different contexts.[87][88] Females, for example, have a higher risk of anxiety[89] and eating disorders,[90] whereas males have a higher chance of substance abuse and behavioral and developmental issues.[91] This does not imply that women are less likely to suffer from developmental disorders such autism spectrum disorder, attention deficit hyperactivity disorder, Tourette syndrome, or early-onset schizophrenia. Ethnicity and ethnic heterogeneity have also been identified as risk factors for the prevalence of mental disorders, with minority groups being at a higher risk due to discrimination and exclusion.[86]\n\nUnemployment has been shown to hurt an individual's emotional well-being, self-esteem, and more broadly their mental health. Increasing unemployment has been shown to have a significant impact on mental health, predominantly depressive disorders.[92] This is an important consideration when reviewing the triggers for mental health disorders in any population survey.[93] According to a 2009 meta-analysis by Paul and Moser, countries with high income inequality and poor unemployment protections experience worse mental health outcomes among the unemployed.[94]\n\nEmotional mental disorders are a leading cause of disabilities worldwide. Investigating the degree and severity of untreated emotional mental disorders throughout the world is a top priority of the World Mental Health (WMH) survey initiative,[95] which was created in 1998 by the World Health Organization (WHO).[96] \"Neuropsychiatric disorders are the leading causes of disability worldwide, accounting for 37% of all healthy life years lost through disease. These disorders are most destructive to low and middle-income countries due to their inability to provide their citizens with proper aid. Despite modern treatment and rehabilitation for emotional mental health disorders, \"even economically advantaged societies have competing priorities and budgetary constraints\".\n\nUnhappily married couples suffer 3–25 times the risk of developing clinical depression.[97][98][99]\n\nThe World Mental Health survey initiative has suggested a plan for countries to redesign their mental health care systems to best allocate resources.\n\"A first step is documentation of services being used and the extent and nature of unmet treatment needs. A second step could be to do a cross-national comparison of service use and unmet needs in countries with different mental health care systems. Such comparisons can help to uncover optimum financing, national policies, and delivery systems for mental health care.\"[This quote needs a citation]\n\nKnowledge of how to provide effective emotional mental health care has become imperative worldwide. Unfortunately, most countries have insufficient data to guide decisions, absent or competing visions for resources, and near-constant pressures to cut insurance and entitlements. WMH surveys were done in Africa (Nigeria, South Africa), the Americas (Colombia, Mexico, United States), Asia and the Pacific (Japan, New Zealand, Beijing and Shanghai in the People's Republic of China), Europe (Belgium, France, Germany, Italy, Netherlands, Spain, Ukraine), and the Middle East (Israel, Lebanon). Countries were classified with World Bank criteria as low-income (Nigeria), lower-middle-income (China, Colombia, South Africa, Ukraine), higher middle-income (Lebanon, Mexico), and high-income.\n\nThe coordinated surveys on emotional mental health disorders, their severity, and treatments were implemented in the aforementioned countries. These surveys assessed the frequency, types, and adequacy of mental health service use in 17 countries in which WMH surveys are complete. The WMH also examined unmet needs for treatment in strata defined by the seriousness of mental disorders. Their research showed that \"the number of respondents using any 12-month mental health service was generally lower in developing than in developed countries, and the proportion receiving services tended to correspond to countries' percentages of gross domestic product spent on health care\".\n\"High levels of unmet need worldwide are not surprising, since WHO Project ATLAS' findings of much lower mental health expenditures than was suggested by the magnitude of burdens from mental illnesses. Generally, unmet needs in low-income and middle-income countries might be attributable to these nations spending reduced amounts (usually <1%) of already diminished health budgets on mental health care, and they rely heavily on out-of-pocket spending by citizens who are ill-equipped for it\".\n\nThe Centre for Addiction and Mental Health discusses how a certain amount of stress is a normal part of daily life. Small doses of stress help people meet deadlines, be prepared for presentations, be productive and arrive on time for important events. However, long-term stress can become harmful. When stress becomes overwhelming and prolonged, the risks for mental health problems and medical problems increase.\"[100] Also on that note, some studies have found language to deteriorate mental health and even harm humans.[101]\n\nThe impact of a stressful environment has also been highlighted by different models. Mental health has often been understood from the lens of the vulnerability-stress model.[102] In that context, stressful situations may contribute to a preexisting vulnerability to negative mental health outcomes being realized. On the other hand, the differential susceptibility hypothesis suggests that mental health outcomes are better explained by an increased sensitivity to the environment than by vulnerability.[103] For example, it was found that children scoring higher on observer-rated environmental sensitivity often derive more harm from low-quality parenting, but also more benefits from high-quality parenting than those children scoring lower on that measure.[104]\n\nA psychological study has been conducted by four scientists during inaugural Convention of Psychological Science. The results find that people who thrive with financial stability or fall under low socioeconomic status (SES) tend to perform worse cognitively due to external pressure imposed upon them. The research found that stressors such as low income, inadequate health care, discrimination, and exposure to criminal activities all contribute to mental disorders. This study also found that children exposed to poverty-stricken environments have slower cognitive thinking.[105] It is seen that children perform better under the care of their parents and that children tend to adopt speaking language at a younger age. Since being in poverty from childhood is more harmful than it is for an adult, it is seen that children in poor households tend to fall behind in certain cognitive abilities compared to other average families.[106]\n\nFor a child to grow up emotionally healthy, the children under three need \"A strong, reliable primary caregiver who provides consistent and unconditional love, guidance, and support. Safe, predictable, stable environments. Ten to 20 hours each week of harmonious, reciprocal interactions. This process, known as attunement, is most crucial during the first 6–24 months of infants' lives and helps them develop a wider range of healthy emotions, including gratitude, forgiveness, and empathy. Enrichment through personalized, increasingly complex activities\".[citation needed]\n\n\n\nThe effects of climate change on mental health and wellbeing are being documented as the consequences of climate change become more tangible and impactful. This is especially the case for vulnerable populations and those with pre-existing serious mental illness.[110] There are three broad pathways by which these effects can take place: directly, indirectly or via awareness.[111] The direct pathway includes stress-related conditions caused by exposure to extreme weather events. These include post-traumatic stress disorder (PTSD). Scientific studies have linked mental health to several climate-related exposures. These include heat, humidity, rainfall, drought, wildfires and floods.[112] The indirect pathway can be disruption to economic and social activities. An example is when an area of farmland is less able to produce food.[112] The third pathway can be of mere awareness of the climate change threat, even by individuals who are not otherwise affected by it.[111] This especially manifests in the form of anxiety over the quality of life for future generations.[113]\n\nAn additional aspect to consider is the detrimental impact climate change can have on green or blue natural spaces, which have been proven to have beneficial impact on mental health.[114][115] Impacts of anthropogenic climate change, such as freshwater pollution or deforestation, degrade these landscapes and reduce public access to them.[116] Even when the green and blue spaces are intact, their accessibility is not equal across society, which is an issue of environmental justice and economic inequality.[117]\n\nMental health outcomes have been measured by several different indicators. These include psychiatric hospital admissions, mortality, self-harm and suicide rates. People with pre-existing mental illness, Indigenous peoples, migrants and refugees, and children and adolescents are especially vulnerable. The emotional responses to the threat of climate change can include eco-anxiety, ecological grief and eco-anger.[118][119] Such emotions can be rational responses to the degradation of the natural world and may lead to adaptive action.[120]\n\n\"The terms mental health promotion and prevention have often been confused. Promotion is defined as intervening to optimize positive mental health by addressing determinants of positive mental health (i.e. protective factors) before a specific mental health problem has been identified, with the ultimate goal of improving the positive mental health of the population. Mental health prevention is defined as intervening to minimize mental health problems (i.e. risk factors) by addressing determinants of mental health problems before a specific mental health problem has been identified in the individual, group, or population of focus with the ultimate goal of reducing the number of future mental health problems in the population.\"[122][123]\n\nIn order to improve mental health, the root of the issue has to be resolved. \"Prevention emphasizes the avoidance of risk factors; promotion aims to enhance an individual's ability to achieve a positive sense of self-esteem, mastery, well-being, and social inclusion.\"[124] Mental health promotion attempts to increase protective factors and healthy behaviors that can help prevent the onset of a diagnosable mental disorder and reduce risk factors that can lead to the development of a mental disorder.[122] Yoga is an example of an activity that calms one's entire body and nerves.[125] According to a study on well-being by Richards, Campania, and Muse-Burke, \"mindfulness is considered to be a purposeful state, it may be that those who practice it belief in its importance and value being mindful, so that valuing of self-care activities may influence the intentional component of mindfulness.\"[39] Akin to surgery, sometimes the body must be further damaged, before it can properly heal [126]\n\nMental health is conventionally defined as a hybrid of the absence of a mental disorder and the presence of well-being. Focus is increasing on preventing mental disorders.\nPrevention is beginning to appear in mental health strategies, including the 2004 WHO report \"Prevention of Mental Disorders\", the 2008 EU \"Pact for Mental Health\" and the 2011 US National Prevention Strategy.[127][128][page needed] Some commentators have argued that a pragmatic and practical approach to mental disorder prevention at work would be to treat it the same way as physical injury prevention.[129]\n\nPrevention of a disorder at a young age may significantly decrease the chances that a child will have a disorder later in life, and shall be the most efficient and effective measure from a public health perspective.[130] Prevention may require the regular consultation of a physician for at least twice a year to detect any signs that reveal any mental health concerns.\n\nAdditionally, social media is becoming a resource for prevention. In 2004, the Mental Health Services Act[131] began to fund marketing initiatives to educate the public on mental health. This California-based project is working to combat the negative perception with mental health and reduce the stigma associated with it. While social media can benefit mental health, it can also lead to deterioration if not managed properly.[132] Limiting social media intake is beneficial.[133]\n\nStudies report that patients in mental health care who can access and read their Electronic Health Records (EHR) or Open Notes online experience increased understanding of their mental health, feeling in control of their care, and enhanced trust in their clinicians. Patients' also reported feelings of greater validation, engagement, remembering their care plan, and acquiring a better awareness of potential side effects of their medications, when reading their mental health notes. Other common experiences were that shared mental health notes enhance patient empowerment and augment patient autonomy.[134][135][136][137][138][139]\n\nFurthermore, recent studies have shown that social media is an effective way to draw attention to mental health issues. By collecting data from Twitter, researchers found that social media presence is heightened after an event relating to behavioral health occurs.[140] Researchers continue to find effective ways to use social media to bring more awareness to mental health issues through online campaigns in other sites such as Facebook and Instagram.[141]\n\nMental health care navigation helps to guide patients and families through the fragmented, often confusing mental health industries. Care navigators work closely with patients and families through discussion and collaboration to provide information on best therapies as well as referrals to practitioners and facilities specializing in particular forms of emotional improvement. The difference between therapy and care navigation is that the care navigation process provides information and directs patients to therapy rather than providing therapy. Still, care navigators may offer diagnosis and treatment planning. Though many care navigators are also trained therapists and doctors. Care navigation is the link between the patient and the below therapies. A clear recognition that mental health requires medical intervention was demonstrated in a study by Kessler et al. of the prevalence and treatment of mental disorders from 1990 to 2003 in the United States. Despite the prevalence of mental health disorders remaining unchanged during this period, the number of patients seeking treatment for mental disorders increased threefold.[142]\n\nPharmacotherapy is a therapy that uses pharmaceutical drugs. Pharmacotherapy is used in the treatment of mental illness through the use of antidepressants, benzodiazepines, and the use of elements such as lithium. It can only be prescribed by a medical professional trained in the field of Psychiatry.\n\nPhysical exercise can improve mental and physical health. Playing sports, walking, cycling, or doing any form of physical activity trigger the production of various hormones, sometimes including endorphins, which can elevate a person's mood.[143]\n\nStudies have shown that in some cases, physical activity can have the same impact as antidepressants when treating depression and anxiety.[144]\n\nMoreover, cessation of physical exercise may have adverse effects on some mental health conditions, such as depression and anxiety. This could lead to different negative outcomes such as obesity, skewed body image and many health risks associated with mental illnesses.[145] Exercise can improve mental health but it should not be used as an alternative to therapy.[146]\n\nActivity therapies also called recreation therapy and occupational therapy, promote healing through active engagement. An example of occupational therapy would be promoting an activity that improves daily life, such as self-care or improving hobbies.[147]\n\nEach of these therapies have proven to improve mental health and have resulted in healthier, happier individuals. In recent years, for example, coloring has been recognized as an activity that has been proven to significantly lower the levels of depressive symptoms and anxiety in many studies.[148]\n\nExpressive therapies or creative arts therapies are a form of psychotherapy that involves the arts or artmaking. These therapies include art therapy, music therapy, drama therapy, dance therapy, and poetry therapy. It has been proven that music therapy is an effective way of helping people with a mental health disorder.[149] Drama therapy is approved by NICE for the treatment of psychosis.[150]\n\nPsychotherapy is the general term for the scientific based treatment of mental health issues based on modern medicine. It includes a number of schools, such as gestalt therapy, psychoanalysis, cognitive behavioral therapy, psychedelic therapy, transpersonal psychology/psychotherapy, and dialectical behavioral therapy.\nGroup therapy involves any type of therapy that takes place in a setting involving multiple people. It can include psychodynamic groups, expressive therapy groups, support groups (including the Twelve-step program), problem-solving and psychoeducation groups.\n\nAccording to Neff, self-compassion consists of three main positive components and their negative counterparts: Self-Kindness versus Self-Judgment, Common Humanity versus Isolation and Mindfulness versus Over-Identification.[151] Furthermore, there is evidence from a study by Shin & Lin suggesting specific components of self-compassion can predict specific dimensions of positive mental health (emotional, social, and psychological well-being).[152]\n\nThe Collaborative for academic, social, emotional learning (CASEL) addresses five broad and interrelated areas of competence and highlights examples for each: self-awareness, self-management, social awareness, relationship skills, and responsible decision-making.[153] A meta-analysis was done by Alexendru Boncu, Iuliana Costeau, & Mihaela Minulescu (2017) looking at social-emotional learning (SEL) studies and the effects on emotional and behavior outcomes. They found a small but significant effect size (across the studies looked into) for externalized problems and social-emotional skills.[154]\n\nThe practice of mindfulness meditation has several potential mental health benefits, such as bringing about reductions in depression, anxiety and stress.[155][156][157][158] Mindfulness meditation may also be effective in treating substance use disorders.[159]\n\nLucid dreaming has been found to be associated with greater mental well-being. It also was not associated with poorer sleep quality nor with cognitive dissociation.[160] There is also some evidence lucid dreaming therapy can help with nightmare reduction.[161]\n\nMental fitness is a mental health movement that encourages people to intentionally regulate and maintain their emotional wellbeing through friendship, regular human contact, and activities that include meditation, calming exercises, aerobic exercise, mindfulness, having a routine and maintaining adequate sleep. Mental fitness is intended to build resilience against every-day mental and potentially physical health challenges to prevent an escalation of anxiety, depression, and suicidal ideation.[162] This can help people, including older adults with health challenges, to more effectively cope with the escalation of those feelings if they occur.[163]\n\nSpiritual counsellors meet with people in need to offer comfort and support and to help them gain a better understanding of their issues and develop a problem-solving relation with spirituality. These types of counselors deliver care based on spiritual, psychological and theological principles.[164]\n\nThere are many factors that influence mental health including:\n\nEmotional mental illnesses is a particular concern in the United States since the U.S. has the highest annual prevalence rates (26 percent) for mental illnesses among a comparison of 14 developing and developed countries.[165] While approximately 80 percent of all people in the United States with a mental disorder eventually receive some form of treatment, on average persons do not access care until nearly a decade following the development of their illness, and less than one-third of people who seek help receive minimally adequate care.[166] The government offers everyone programs and services, but veterans receive the most help, there is certain eligibility criteria that has to be met.[167]\n\nMental health policies in the United States have experienced four major reforms: the American asylum movement led by Dorothea Dix in 1843; the mental hygiene movement inspired by Clifford Beers in 1908; the deinstitutionalization started by Action for Mental Health in 1961; and the community support movement called for by The CMCH Act Amendments of 1975.[168]\n\nIn 1843, Dorothea Dix submitted a Memorial to the Legislature of Massachusetts, describing the abusive treatment and horrible conditions received by the mentally ill patients in jails, cages, and almshouses. She revealed in her Memorial: \"I proceed, gentlemen, briefly to call your attention to the present state of insane persons confined within this Commonwealth, in cages, closets, cellars, stalls, pens! Chained, naked, beaten with rods, and lashed into obedience....\"[169] Many asylums were built in that period, with high fences or walls separating the patients from other community members and strict rules regarding the entrance and exit. In 1866, a recommendation came to the New York State Legislature to establish a separate asylum for chronic mentally ill patients. Some hospitals placed the chronic patients into separate wings or wards, or different buildings.[170]\n\nIn A Mind That Found Itself (1908) Clifford Whittingham Beers described the humiliating treatment he received and the deplorable conditions in the mental hospital.[171] One year later, the National Committee for Mental Hygiene (NCMH) was founded by a small group of reform-minded scholars and scientists—including Beers himself—which marked the beginning of the \"mental hygiene\" movement. The movement emphasized the importance of childhood prevention. World War I catalyzed this idea with an additional emphasis on the impact of maladjustment, which convinced the hygienists that prevention was the only practical approach to handle mental health issues.[172] However, prevention was not successful, especially for chronic illness; the condemnable conditions in the hospitals were even more prevalent, especially under the pressure of the increasing number of chronically ill and the influence of the depression.[168]\n\nIn 1961, the Joint Commission on Mental Health published a report called Action for Mental Health, whose goal was for community clinic care to take on the burden of prevention and early intervention of the mental illness, therefore to leave space in the hospitals for severe and chronic patients. The court started to rule in favor of the patients' will on whether they should be forced to treatment. By 1977, 650 community mental health centers were built to cover 43 percent of the population and serve 1.9 million individuals a year, and the lengths of treatment decreased from 6 months to only 23 days.[173] However, issues still existed. Due to inflation, especially in the 1970s, the community nursing homes received less money to support the care and treatment provided. Fewer than half of the planned centers were created, and new methods did not fully replace the old approaches to carry out its full capacity of treating power.[173] Besides, the community helping system was not fully established to support the patients' housing, vocational opportunities, income supports, and other benefits.[168] Many patients returned to welfare and criminal justice institutions, and more became homeless. The movement of deinstitutionalization was facing great challenges.[174]\n\nAfter realizing that simply changing the location of mental health care from the state hospitals to nursing houses was insufficient to implement the idea of deinstitutionalization, the National Institute of Mental Health (NIMH) in 1975 created the Community Support Program (CSP) to provide funds for communities to set up a comprehensive mental health service and supports to help the mentally ill patients integrate successfully in the society. The program stressed the importance of other supports in addition to medical care, including housing, living expenses, employment, transportation, and education; and set up new national priority for people with serious mental disorders. In addition, the Congress enacted the Mental Health Systems Act of 1980 to prioritize the service to the mentally ill and emphasize the expansion of services beyond just clinical care alone.[175] Later in the 1980s, under the influence from the Congress and the Supreme Court, many programs started to help the patients regain their benefits. A new Medicaid service was also established to serve people who were diagnosed with a \"chronic mental illness\". People who were temporally hospitalized were also provided aid and care and a pre-release program was created to enable people to apply for reinstatement prior to discharge.[173] Not until 1990, around 35 years after the start of the deinstitutionalization, did the first state hospital begin to close. The number of hospitals dropped from around 300 by over 40 in the 1990s, and finally a Report on Mental Health showed the efficacy of mental health treatment, giving a range of treatments available for patients to choose.[175]\n\nHowever, several critics maintain that deinstitutionalization has, from a mental health point of view, been a thoroughgoing failure. The seriously mentally ill are either homeless, or in prison; in either case (especially the latter), they are getting little or no mental health care. This failure is attributed to a number of reasons over which there is some degree of contention, although there is general agreement that community support programs have been ineffective at best, due to a lack of funding.[174]\n\nThe 2011 National Prevention Strategy included mental and emotional well-being, with recommendations including better parenting and early intervention programs, which increase the likelihood of prevention programs being included in future US mental health policies.[127][page needed] The NIMH is researching only suicide and HIV/AIDS prevention, but the National Prevention Strategy could lead to it focusing more broadly on longitudinal prevention studies.[176][failed verification]\n\nIn 2013, United States Representative Tim Murphy introduced the Helping Families in Mental Health Crisis Act, HR2646. The bipartisan bill went through substantial revision and was reintroduced in 2015 by Murphy and Congresswoman Eddie Bernice Johnson. In November 2015, it passed the Health Subcommittee by an 18–12 vote.[177]\n"
    },
    {
        "title": "Nutrition",
        "url": "https://en.wikipedia.org/wiki/Nutrition",
        "content": "\n\nNutrition is the biochemical and physiological process by which an organism uses food to support its life. It provides organisms with nutrients, which can be metabolized to create energy and chemical structures. Failure to obtain the required amount of nutrients causes malnutrition. Nutritional science is the study of nutrition, though it typically emphasizes human nutrition.\n\nThe type of organism determines what nutrients it needs and how it obtains them. Organisms obtain nutrients by consuming organic matter, consuming inorganic matter, absorbing light, or some combination of these. Some can produce nutrients internally by consuming basic elements, while some must consume other organisms to obtain pre-existing nutrients. All forms of life require carbon, energy, and water as well as various other molecules. Animals require complex nutrients such as carbohydrates, lipids, and proteins, obtaining them by consuming other organisms. Humans have developed agriculture and cooking to replace foraging and advance human nutrition. Plants acquire nutrients through the soil and the atmosphere. Fungi absorb nutrients around them by breaking them down and absorbing them through the mycelium.\n\nScientific analysis of food and nutrients began during the chemical revolution in the late 18th century. Chemists in the 18th and 19th centuries experimented with different elements and food sources to develop theories of nutrition.[1] Modern nutrition science began in the 1910s as individual micronutrients began to be identified. The first vitamin to be chemically identified was thiamine in 1926, and vitamin C was identified as a protection against scurvy in 1932.[2] The role of vitamins in nutrition was studied in the following decades. The first recommended dietary allowances for humans were developed to address fears of disease caused by food deficiencies during the Great Depression and the Second World War.[3] Due to its importance in human health, the study of nutrition has heavily emphasized human nutrition and agriculture, while ecology is a secondary concern.[4]\n\nNutrients are substances that provide energy and physical components to the organism, allowing it to survive, grow, and reproduce. Nutrients can be basic elements or complex macromolecules. Approximately 30 elements are found in organic matter, with nitrogen, carbon, and phosphorus being the most important.[5] Macronutrients are the primary substances required by an organism, and micronutrients are substances required by an organism in trace amounts. Organic micronutrients are classified as vitamins, and inorganic micronutrients are classified as minerals.[6]\n\nNutrients are absorbed by the cells and used in metabolic biochemical reactions. These include fueling reactions that create precursor metabolites and energy, biosynthetic reactions that convert precursor metabolites into building block molecules, polymerizations that combine these molecules into macromolecule polymers, and assembly reactions that use these polymers to construct cellular structures.[5]\n\nOrganisms can be classified by how they obtain carbon and energy. Heterotrophs are organisms that obtain nutrients by consuming the carbon of other organisms, while autotrophs are organisms that produce their own nutrients from the carbon of inorganic substances like carbon dioxide. Mixotrophs are organisms that can be heterotrophs and autotrophs, including some plankton and carnivorous plants. Phototrophs obtain energy from light, while chemotrophs obtain energy by consuming chemical energy from matter. Organotrophs consume other organisms to obtain electrons, while lithotrophs obtain electrons from inorganic substances, such as water, hydrogen sulfide, dihydrogen, iron(II), sulfur, or ammonium.[7] Prototrophs can create essential nutrients from other compounds, while auxotrophs must consume preexisting nutrients.[8]\n\nIn nutrition, the diet of an organism is the sum of the foods it eats.[9] A healthy diet improves the physical and mental health of an organism. This requires ingestion and absorption of vitamins, minerals, essential amino acids from protein and essential fatty acids from fat-containing food. Carbohydrates, protein and fat play major roles in ensuring the quality of life, health and longevity of the organism.[10] Some cultures and religions have restrictions on what is acceptable for their diet.[11]\n\nA nutrient cycle is a biogeochemical cycle involving the movement of inorganic matter through a combination of soil, organisms, air or water, where they are exchanged in organic matter.[12] Energy flow is a unidirectional and noncyclic pathway, whereas the movement of mineral nutrients is cyclic. Mineral cycles include the carbon cycle, sulfur cycle, nitrogen cycle, water cycle, phosphorus cycle, and oxygen cycle, among others that continually recycle along with other mineral nutrients into productive ecological nutrition.[12]\n\nBiogeochemical cycles that are performed by living organisms and natural processes are water, carbon, nitrogen, phosphorus, and sulfur cycles.[13] Nutrient cycles allow these essential elements to return to the environment after being absorbed or consumed.[14] Without proper nutrient cycling, there would be risk of change in oxygen levels, climate, and ecosystem function.[citation needed]\n\nForaging is the process of seeking out nutrients in the environment. It may also be defined to include the subsequent use of the resources. Some organisms, such as animals and bacteria, can navigate to find nutrients, while others, such as plants and fungi, extend outward to find nutrients. Foraging may be random, in which the organism seeks nutrients without method, or it may be systematic, in which the organism can go directly to a food source.[15] Organisms are able to detect nutrients through taste or other forms of nutrient sensing, allowing them to regulate nutrient intake.[16] Optimal foraging theory is a model that explains foraging behavior as a cost–benefit analysis in which an animal must maximize the gain of nutrients while minimizing the amount of time and energy spent foraging. It was created to analyze the foraging habits of animals, but it can also be extended to other organisms.[17] Some organisms are specialists that are adapted to forage for a single food source, while others are generalists that can consume a variety of food sources.[18]\n\nNutrient deficiencies, known as malnutrition, occur when an organism does not have the nutrients that it needs. This may be caused by suddenly losing nutrients or the inability to absorb proper nutrients. Not only is malnutrition the result of a lack of necessary nutrients,[19] but it can also be a result of other illnesses and health conditions. When this occurs, an organism will adapt by reducing energy consumption and expenditure to prolong the use of stored nutrients. It will use stored energy reserves until they are depleted, and it will then break down its own body mass for additional energy.[20]\n\nA balanced diet includes appropriate amounts of all essential and non-essential nutrients. These can vary by age, weight, sex, physical activity levels, and more. A lack of just one essential nutrient can cause bodily harm, just as an overabundance can cause toxicity. The Daily Reference Values keep the majority of people from nutrient deficiencies.[21] DRVs are not recommendations but a combination of nutrient references to educate professionals and policymakers on what the maximum and minimum nutrient intakes are for the average person.[22] Food labels also use DRVs as a reference to create safe nutritional guidelines for the average healthy person.[23]\n\nAnimals are heterotrophs that consume other organisms to obtain nutrients. Herbivores are animals that eat plants, carnivores are animals that eat other animals, and omnivores are animals that eat both plants and other animals.[24] Many herbivores rely on bacterial fermentation to create digestible nutrients from indigestible plant cellulose, while obligate carnivores must eat animal meats to obtain certain vitamins or nutrients their bodies cannot otherwise synthesize. Animals generally have a higher requirement of energy in comparison to plants.[25] The macronutrients essential to animal life are carbohydrates, amino acids, and fatty acids.[6][26]\n\nAll macronutrients except water are required by the body for energy, however, this is not their sole physiological function. The energy provided by macronutrients in food is measured in kilocalories, usually called Calories, where 1 Calorie is the amount of energy required to raise 1 kilogram of water by 1 degree Celsius.[27]\n\nCarbohydrates are molecules that store significant amounts of energy. Animals digest and metabolize carbohydrates to obtain this energy. Carbohydrates are typically synthesized by plants during metabolism, and animals have to obtain most carbohydrates from nature, as they have only a limited ability to generate them. They include sugars, oligosaccharides, and polysaccharides. Glucose is the simplest form of carbohydrate.[28] Carbohydrates are broken down to produce glucose and short-chain fatty acids, and they are the most abundant nutrients for herbivorous land animals.[29] Carbohydrates contain 4 calories per gram.\n\nLipids provide animals with fats and oils. They are not soluble in water, and they can store energy for an extended period of time. They can be obtained from many different plant and animal sources. Most dietary lipids are triglycerides, composed of glycerol and fatty acids. Phospholipids and sterols are found in smaller amounts.[30] An animal's body will reduce the amount of fatty acids it produces as dietary fat intake increases, while it increases the amount of fatty acids it produces as carbohydrate intake increases.[31] Fats contain 9 calories per gram.\n\nProtein consumed by animals is broken down to amino acids, which would be later used to synthesize new proteins. Protein is used to form cellular structures, fluids,[32] and enzymes (biological catalysts). Enzymes are essential to most metabolic processes, as well as DNA replication, repair, and transcription.[33] Protein contains 4 calories per gram.\n\nMuch of animal behavior is governed by nutrition. Migration patterns and seasonal breeding take place in conjunction with food availability, and courtship displays are used to display an animal's health.[34] Animals develop positive and negative associations with foods that affect their health, and they can instinctively avoid foods that have caused toxic injury or nutritional imbalances through a conditioned food aversion. Some animals, such as rats, do not seek out new types of foods unless they have a nutrient deficiency.[35]\n\nEarly human nutrition consisted of foraging for nutrients, like other animals, but it diverged at the beginning of the Holocene with the Neolithic Revolution, in which humans developed agriculture to produce food. The Chemical Revolution in the 18th century allowed humans to study the nutrients in foods and develop more advanced methods of food preparation. Major advances in economics and technology during the 20th century allowed mass production and food fortification to better meet the nutritional needs of humans.[36] Human behavior is closely related to human nutrition, making it a subject of social science in addition to biology. Nutrition in humans is balanced with eating for pleasure, and optimal diet may vary depending on the demographics and health concerns of each person.[37]\n\nHumans are omnivores that eat a variety of foods. Cultivation of cereals and production of bread has made up a key component of human nutrition since the beginning of agriculture. Early humans hunted animals for meat, and modern humans domesticate animals to consume their meat and eggs. The development of animal husbandry has also allowed humans in some cultures to consume the milk of other animals and process it into foods such as cheese. Other foods eaten by humans include nuts, seeds, fruits, and vegetables. Access to domesticated animals as well as vegetable oils has caused a significant increase in human intake of fats and oils. Humans have developed advanced methods of food processing that prevent contamination of pathogenic microorganisms and simplify the production of food. These include drying, freezing, heating, milling, pressing, packaging, refrigeration, and irradiation. Most cultures add herbs and spices to foods before eating to add flavor, though most do not significantly affect nutrition. Other additives are also used to improve the safety, quality, flavor, and nutritional content of food.[38]\n\nHumans obtain most carbohydrates as starch from cereals, though sugar has grown in importance.[28] Lipids can be found in animal fat, butterfat, vegetable oil, and leaf vegetables, and they are also used to increase flavor in foods.[30] Protein can be found in virtually all foods, as it makes up cellular material, though certain methods of food processing may reduce the amount of protein in a food.[39] Humans can also obtain energy from ethanol, which is both a food and a drug, but it provides relatively few essential nutrients and is associated with nutritional deficiencies and other health risks.[40]\n\nIn humans, poor nutrition can cause deficiency-related diseases, such as blindness, anemia, scurvy, preterm birth, stillbirth and cretinism,[41] or nutrient-excess conditions, such as obesity[42] and metabolic syndrome.[43] Other conditions possibly affected by nutrition disorders include cardiovascular diseases,[44] diabetes,[45][46] and osteoporosis.[47] Undernutrition can lead to wasting in acute cases, and stunting of marasmus in chronic cases of malnutrition.[41]\n\nIn domesticated animals, such as pets, livestock, and working animals, as well as other animals in captivity, nutrition is managed by humans through animal feed. Fodder and forage are provided to livestock. Specialized pet food has been manufactured since 1860, and subsequent research and development have addressed the nutritional needs of pets. Dog food and cat food in particular are heavily studied and typically include all essential nutrients for these animals. Cats are sensitive to some common nutrients, such as taurine, and require additional nutrients derived from meat. Large-breed puppies are susceptible to overnutrition, as small-breed dog food is more energy dense than they can absorb.[48]\n\nMost plants obtain nutrients through inorganic substances absorbed from the soil or the atmosphere. Carbon, hydrogen, oxygen, nitrogen, and sulfur are essential nutrients that make up organic material in a plant and allow enzymic processes. These are absorbed ions in the soil, such as bicarbonate, nitrate, ammonium, and sulfate, or they are absorbed as gases, such as carbon dioxide, water, oxygen gas, and sulfur dioxide. Phosphorus, boron, and silicon are used for esterification. They are obtained through the soil as phosphates, boric acid, and silicic acid, respectively. Other nutrients used by plants are potassium, sodium, calcium, magnesium, manganese, chlorine, iron, copper, zinc, and molybdenum.[49]\n\nPlants uptake essential elements from the soil through their roots and from the air (consisting of mainly nitrogen and oxygen) through their leaves. Nutrient uptake in the soil is achieved by cation exchange, wherein root hairs pump hydrogen ions (H+) into the soil through proton pumps. These hydrogen ions displace cations attached to negatively charged soil particles so that the cations are available for uptake by the root. In the leaves, stomata open to take in carbon dioxide and expel oxygen.[50] Although nitrogen is plentiful in the Earth's atmosphere, very few plants can use this directly. Most plants, therefore, require nitrogen compounds to be present in the soil in which they grow. This is made possible by the fact that largely inert atmospheric nitrogen is changed in a nitrogen fixation process to biologically usable forms in the soil by bacteria.[51]\n\nAs these nutrients do not provide the plant with energy, they must obtain energy by other means. Green plants absorb energy from sunlight with chloroplasts and convert it to usable energy through photosynthesis.[52]\n\nFungi are chemoheterotrophs that consume external matter for energy. Most fungi absorb matter through the root-like mycelium, which grows through the organism's source of nutrients and can extend indefinitely. The fungus excretes extracellular enzymes to break down surrounding matter and then absorbs the nutrients through the cell wall. Fungi can be parasitic, saprophytic, or symbiotic. Parasitic fungi attach and feed on living hosts, such as animals, plants, or other fungi. Saprophytic fungi feed on dead and decomposing organisms. Symbiotic fungi grow around other organisms and exchange nutrients with them.[53]\n\nProtists include all eukaryotes that are not animals, plants, or fungi, resulting in great diversity between them. Algae are photosynthetic protists that can produce energy from light. Several types of protists use mycelium similar to those of fungi. Protozoa are heterotrophic protists, and different protozoa seek nutrients in different ways. Flagellate protozoa use a flagellum to assist in hunting for food, and some protozoa travel via infectious spores to act as parasites.[54] Many protists are mixotrophic, having both phototrophic and heterotrophic characteristics. Mixotrophic protists will typically depend on one source of nutrients while using the other as a supplemental source or a temporary alternative when its primary source is unavailable.[55]\n\nProkaryotes, including bacteria and archaea, vary greatly in how they obtain nutrients across nutritional groups. Prokaryotes can only transport soluble compounds across their cell envelopes, but they can break down chemical components around them. Some lithotrophic prokaryotes are extremophiles that can survive in nutrient-deprived environments by breaking down inorganic matter.[56] Phototrophic prokaryotes, such as cyanobacteria and Chloroflexia, can engage in photosynthesis to obtain energy from sunlight. This is common among bacteria that form in mats atop geothermal springs. Phototrophic prokaryotes typically obtain carbon from assimilating carbon dioxide through the Calvin cycle.[57]\n\nSome prokaryotes, such as Bdellovibrio and Ensifer, are predatory and feed on other single-celled organisms. Predatory prokaryotes seek out other organisms through chemotaxis or random collision, merge with the organism, degrade it, and absorb the released nutrients. Predatory strategies of prokaryotes include attaching to the outer surface of the organism and degrading it externally, entering the cytoplasm of the organism, or by entering the periplasmic space of the organism. Groups of predatory prokaryotes may forgo attachment by collectively producing hydrolytic enzymes.[58]\n"
    },
    {
        "title": "Pandemic",
        "url": "https://en.wikipedia.org/wiki/Pandemic",
        "content": "\n\nA pandemic (/pænˈdɛmɪk/ pan-DEM-ik) is an epidemic of an infectious disease that has a sudden increase in cases and spreads across a large region, for instance multiple continents or worldwide, affecting a substantial number of individuals. Widespread endemic diseases with a stable number of infected individuals such as recurrences of seasonal influenza are generally excluded as they occur simultaneously in large regions of the globe rather than being spread worldwide.\n\nThroughout human history, there have been a number of pandemics of diseases such as smallpox. The Black Death, caused by the Plague, caused the deaths of up to half of the population of Europe in the 14th century.[2][3][4][5] The term pandemic had not been used then, but was used for later epidemics, including the 1918 H1N1 influenza A pandemic—more commonly known as the Spanish flu—which is the deadliest pandemic in history.[6][7][8] The most recent pandemics include the HIV/AIDS pandemic,[a][9] the 2009 swine flu pandemic and the COVID-19 pandemic. Almost all these diseases still circulate among humans though their impact now is often far less.\n\nIn response to the COVID-19 pandemic, 194 member states of the World Health Organization began negotiations on an International Treaty on Pandemic Prevention, Preparedness and Response, with a requirement to submit a draft of this treaty to the 77th World Health Assembly during its 2024 convention.[10][11] Further, on 6 May 2024, the White House released an official policy to more safely manage medical research projects involving potentially hazardous pathogens, including viruses and bacteria, that may pose a risk of a pandemic.[12][13]\n\nA medical dictionary definition of pandemic is \"an epidemic occurring on a scale that crosses international boundaries, usually affecting people on a worldwide scale\".[14] A disease or condition is not a pandemic merely because it is widespread or kills many people; it must also be infectious. For instance, cancer is responsible for many deaths but is not considered a pandemic because the disease is not contagious—i.e. easily transmissible—and not even simply infectious.[15] This definition differs from colloquial usage in that it encompasses outbreaks of relatively mild diseases.[16][17]\n\nThe World Health Organization (WHO) has a category of Public Health Emergency of International Concern, defined as \"an extraordinary event which is determined to constitute a public health risk to other States through the international spread of disease and to potentially require a coordinated international response\".[18] There is a rigorous process underlying this categorization and a clearly defined trajectory of responses.[19]\n\nA WHO-sponsored international body, tasked with preparing an international agreement on pandemic prevention, preparedness and response has defined a pandemic as \"the global spread of a pathogen or variant that infects human populations with limited or no immunity through sustained and high transmissibility from person to person, overwhelming health systems with severe morbidity and high mortality, and causing social and economic disruptions, all of which require effective national and global collaboration and coordination for its control\".[20]\n\nThe word comes from the Greek παν- pan- meaning \"all\", or \"every\" and δῆμος demos \"people\".\n\nA common early characteristic of a pandemic is a rapid, sometimes exponential, growth in the number of infections, coupled with a widening geographical spread.[21]\n\nWHO utilises different criteria to declare a Public Health Emergency of International Concern (PHEIC), its nearest equivalent to the term pandemic.[22] The potential consequences of an incident are considered, rather than its current status.[23] For example, polio was declared a PHEIC in 2014 even though only 482 cases were reported globally in the previous year;[24] this was justified by concerns that polio might break out of its endemic areas and again become a significant health threat globally.[25] The PHEIC status of polio is reviewed regularly and is ongoing, despite the small number of cases annually.[b][26]\n\nThe end of a pandemic is more difficult to delineate. Generally, past epidemics & pandemics have faded out as the diseases become accepted into people's daily lives and routines, becoming endemic.[27] The transition from pandemic to endemic may be defined based on:\n\nAn endemic disease is always present in a population, but at a relatively low and predictable level. There may be periodic spikes of infections or seasonality, (e.g. influenza) but generally the burden on health systems is manageable.[28]\n\nPandemic prevention comprises activities such as anticipatory research and development of therapies and vaccines, as well as monitoring for pathogens and disease outbreaks which may have pandemic potential.[30] Routine vaccination programs are a type of prevention strategy, holding back diseases such as influenza and polio which have caused pandemics in the past, and could do so again if not controlled.[31] Prevention overlaps with preparedness which aims to curtail an outbreak and prevent it getting out of control - it involves strategic planning, data collection and modelling to measure the spread, stockpiling of therapies, vaccines, and medical equipment, as well as public health awareness campaigning.[32] By definition, a pandemic involves many countries so international cooperation, data sharing, and collaboration are essential; as is universal access to tests and therapies.[30]\n\nCollaboration - In response to the COVID-19 pandemic, WHO established a Pandemic Hub in September 2021 in Berlin, aiming to address weaknesses around the world in how countries detect, monitor and manage public health threats. The Hub's initiatives include using artificial intelligence to analyse more than 35,000 data feeds for indications of emerging health threats, as well as improving facilities and coordination between academic institutions and WHO member countries.[33]\n\nDetection - In May 2023, WHO launched the International Pathogen Surveillance Network (IPSN) (hosted by the Pandemic Hub) aiming to detect and respond to disease threats before they become epidemics and pandemics, and to optimize routine disease surveillance. The network provides a platform to connect countries, improving systems for collecting and analysing samples of potentially harmful pathogens.[34] Wastewater surveillance can for example provide early warnings by detecting pathogens in sewage.[35]\n\nTherapies and vaccines - The Coalition for Epidemic Preparedness Innovations (CEPI) is developing a program to condense new vaccine development timelines to 100 days, a third of the time it took to develop a COVID-19 vaccine. CEPI aims to reduce global epidemic and pandemic risk by developing vaccines against known pathogens as well as enabling rapid response to Disease X.[36][37] In the US, the National Institute of Allergy and Infectious Diseases (NIAID) has developed a Pandemic Preparedness Plan which focuses on identifying viruses of concern and developing diagnostics and therapies (including prototype vaccines) to combat them.[38][39]\n\nModeling is important to inform policy decisions. It helps to predict the burden of disease on healthcare facilities, the effectiveness of control measures, projected geographical spread, and timing and extent of future pandemic waves.[40]\n\nPublic awareness involves disseminating reliable information, ensuring consistency on message, transparency, and steps to discredit misinformation.[41]\n\nAir quality - Enhanced indoor ventilation and air filtration systems are also effective at reducing transmission of airborne pathogens, while providing additional health benefits beyond pandemic control.[42]\n\nStockpiling involves maintaining strategic stockpiles of emergency supplies such as personal protective equipment, drugs and vaccines, and equipment such as respirators. Many of these items have limited shelf life, so they require stock rotation even though they may be rarely used.[43]\n\nThe COVID-19 pandemic highlighted a number of ethical and political issues which must be considered during a pandemic. These included decisions about who should be prioritised for treatment while resources are scarce; whether or not to make vaccination compulsory; the timing and extent of constraints on individual liberty, how to sanction individuals who do not comply with emergency regulations, and the extent of international collaboration and resource sharing.[44][45]\n\nThe basic strategies in the control of an outbreak are containment and mitigation. Containment may be undertaken in the early stages of the outbreak, including contact tracing and isolating infected individuals to stop the disease from spreading to the rest of the population, other public health interventions on infection control, and therapeutic countermeasures such as vaccinations which may be effective if available.[53] When it becomes apparent that it is no longer possible to contain the spread of the disease, management will then move on to the mitigation stage, in which measures are taken to slow the spread of the disease and mitigate its effects on society and the healthcare system. In reality, containment and mitigation measures may be undertaken simultaneously.[54]\n\nA key part of managing an infectious disease outbreak is trying to decrease the epidemic peak, known as \"flattening the curve\".[46][49] This helps decrease the risk of health services being overwhelmed and provides more time for a vaccine and treatment to be developed.[46][49] A broad group of non-pharmaceutical interventions may be taken to manage the outbreak.[49] In a flu pandemic, these actions may include personal preventive measures such as hand hygiene, wearing face-masks, and self-quarantine; community measures aimed at social distancing such as closing schools and canceling mass gatherings; community engagement to encourage acceptance and participation in such interventions; and environmental measures such as cleaning of surfaces.[47]\n\nAnother strategy, suppression, requires more extreme long-term non-pharmaceutical interventions to reverse the pandemic by reducing the basic reproduction number to less than 1. The suppression strategy, which includes stringent population-wide social distancing, home isolation of cases, and household quarantine, was undertaken by China during the COVID-19 pandemic where entire cities were placed under lockdown; such a strategy may carry with it considerable social and economic costs.[55]\n\nFor a novel influenza virus, WHO previously applied a six-stage classification to delineate the process by which the virus moves from the first few infections in humans through to a pandemic. Starting with phase 1 (infections identified in animals only), it moves through phases of increasing infection and spread to phase 6 (pandemic).[56] In February 2020, a WHO spokesperson clarified that the system is no longer in use.[57]\n\nIn 2014, the United States Centers for Disease Control and Prevention (CDC) introduced a framework for characterising the progress of an influenza pandemic titled the Pandemic Intervals Framework.[58] The six intervals of the framework are as follows:\n\nAt the same time, the CDC adopted the Pandemic Severity Assessment Framework (PSAF) to assess the severity of influenza pandemics.[58] The PSAF rates the severity of an influenza outbreak on two dimensions: clinical severity of illness in infected persons; and the transmissibility of the infection in the population.[59] This tool was not applied during the COVID-19 pandemic.[60]\n\nSARS-CoV-2, a new strain of coronavirus, was first detected in the city of Wuhan, Hubei Province, China, in December 2019.[62] The outbreak was characterized as a Public Health Emergency of International Concern (PHEIC) between January 2020 and May 2023 by WHO.[63][64] The number of people infected with COVID-19 has reached more than 767 million worldwide, with a death toll of 6.9 million.[c][65] It is considered likely that the virus will eventually become endemic and, like the common cold, cause less severe disease for most people.[66]\n\nHIV/AIDS was first identified as a disease in 1981, and is an ongoing worldwide public health issue.[67][68] Since then, HIV/AIDS has killed an estimated 40 million people with a further 630,000 deaths annually; 39 million people are currently living with HIV infection.[d][67] HIV has a zoonotic origin, having originated in nonhuman primates in Central Africa and transferred to humans in the early 20th century.[69] The most frequent mode of transmission of HIV is through sexual contact with an infected person. There may be a short period of mild, nonspecific symptoms followed by an asymptomatic (but nevertheless infectious) stage called clinical latency - without treatment, this stage can last between 3 and 20 years. The only way to detect infection is by means of a HIV test.[70] There is no vaccine to prevent HIV infection, but the disease can be held in check by means of antiretroviral therapy.[71]\n\nHistorical accounts of epidemics are often vague or contradictory in describing how victims were affected. A rash accompanied by a fever might be smallpox, measles, scarlet fever, or varicella, and it is possible that epidemics overlapped, with multiple infections striking the same population at once. It is often impossible to know the exact causes of mortality, although ancient DNA studies can sometimes detect residues of certain pathogens.[72]\nIt is assumed that, prior to the Neolithic Revolution around 10,000 BC, disease outbreaks were limited to a single family or clan, and did not spread widely before dying out. The domestication of animals increased human-animal contact, increasing the possibility of zoonotic infections. The advent of agriculture, and trade between settled groups, made it possible for pathogens to spread widely. As population increased, contact between groups became more frequent. A history of epidemics maintained by the Chinese Empire from 243 B.C. to 1911 A.C. shows an approximate correlation between the frequency of epidemics and the growth of the population.[74]\n\nHere is an incomplete list of known epidemics which spread widely enough to merit the title \"pandemic\".\n\nBeginning from the Middle Ages, encounters between European settlers and native populations in the rest of the world often introduced epidemics of extraordinary virulence. Settlers introduced novel diseases which were endemic in Europe, such as smallpox, measles, pertussis and influenza, to which the indigenous peoples had no immunity.[102][103] The Europeans infected with such diseases typically carried them in a dormant state, were actively infected but asymptomatic, or had only mild symptoms.[104]\n\nSmallpox was the most destructive disease that was brought by Europeans to the Native Americans, both in terms of morbidity and mortality. The first well-documented smallpox epidemic in the Americas began in Hispaniola in late 1518 and soon spread to Mexico.[104] Estimates of mortality range from one-quarter to one-half of the population of central Mexico.[105] It is estimated that over the 100 years after European arrival in 1492, the indigenous population of the Americas dropped from 60 million to only 6 million, due to a combination of disease, war, and famine. The majority these deaths are attributed to successive waves of introduced diseases such as smallpox, measles, and typhoid fever.[106][107][108]\n\nIn Australia, smallpox was introduced by European settlers in 1789 devastating the Australian Aboriginal population, killing an estimated 50% of those infected with the disease during the first decades of colonisation.[109] In the early 1800s, measles, smallpox and intertribal warfare killed an estimated 20,000 New Zealand Māori.[110]\n\nIn 1848–49, as many as 40,000 out of 150,000 Hawaiians are estimated to have died of measles, whooping cough and influenza. Measles killed more than 40,000 Fijians, approximately one-third of the population, in 1875,[111] and in the early 19th century devastated the Great Andamanese population.[112] In Hokkaido, an epidemic of smallpox introduced by Japanese settlers is estimated to have killed 34% of the native Ainu population in 1845.[113]\n\nPrevention of future pandemics requires steps to identify future causes of pandemics and to take preventive measures before the disease moves uncontrollably into the human population.\n\nFor example, influenza is a rapidly evolving disease which has caused pandemics in the past and has potential to cause future pandemics. WHO collates the findings of 144 national influenza centres worldwide which monitor emerging flu viruses. Virus variants which are assessed as likely to represent a significant risk are identified and can then be incorporated into the next seasonal influenza vaccine program.[114]\n\nIn a press conference on 28 December 2020, Mike Ryan, head of the WHO Emergencies Program, and other officials said the current COVID-19 pandemic is \"not necessarily the big one\" and \"the next pandemic may be more severe.\" They called for preparation.[115] WHO and the UN have warned the world must tackle the cause of pandemics and not just the health and economic symptoms.[116]\n\nThere is always a possibility that a disease which has caused epidemics in the past may return in the future.[74] It is also possible that little known diseases may become more virulent; in order to encourage research, a number of organisations which monitor global health have drawn up lists of diseases which may have pandemic potential; see table below.[e]\n\nCoronavirus diseases are a family of usually mild illnesses in humans, including those such as the common cold, that have resulted in outbreaks and pandemics such as the 1889-1890 pandemic,[120][121] the 2002–2004 SARS outbreak, Middle East respiratory syndrome–related coronavirus and the COVID-19 pandemic. There is widespread concern that members of the coronavirus family, particularly SARS and MERS have the potential to cause future pandemics.[122] Many human coronaviruses have zoonotic origin, their with natural reservoir in bats or rodents,[123] leading to concerns for future spillover events.[124]\n\nFollowing the end of the COVID-19 pandemic Public Health Emergency of International Concern deceleration by WHO, WHO Director General Tedros Ghebreyesus stated he would not hesitate to re-declare COVID-19 a PHEIC should the global situation worsen in the coming months or years.\n\nInfluenza was first described by the Greek physician Hippocrates in 412 BC.[126] Since the Middle Ages, influenza pandemics have been recorded every 10 to 30 years as the virus mutates to evade immunity.[127][128]\n\nInfluenza is an endemic disease, with a fairly constant number of cases which vary seasonally and can, to a certain extent, be predicted.[129] In a typical year, 5–15% of the population contracts influenza. There are 3–5 million severe cases annually, with up to 650,000 respiratory-related deaths globally each year.[130] The 1889–1890 pandemic is estimated to have caused around a million fatalities,[131] and the \"Spanish flu\" of 1918–1920 eventually infected about one-third of the world's population and caused an estimate 50 million fatalities.[95]\n\nThe Global Influenza Surveillance and Response System is a global network of laboratories that has for purpose to monitor the spread of influenza with the aim to provide WHO with influenza control information.[132] More than two million respiratory specimens are tested by GISRS annually to monitor the spread and evolution of influenza viruses through a network of about 150 laboratories in 114 countries representing 91% of the world's population.[133]\n\nAntibiotic-resistant microorganisms, which sometimes are referred to as \"superbugs\", may contribute to the re-emergence of diseases with pandemic potential that are currently well controlled.[134]\n\nFor example, cases of tuberculosis that are resistant to traditionally effective treatments remain a cause of great concern to health professionals. Every year, nearly half a million new cases of multidrug-resistant tuberculosis (MDR-TB) are estimated to occur worldwide.[135] China and India have the highest rate of MDR-TB.[136] WHO reports that approximately 50 million people worldwide are infected with MDR-TB, with 79 percent of those cases resistant to three or more antibiotics. Extensively drug-resistant tuberculosis (XDR-TB) was first identified in Africa in 2006 and subsequently discovered to exist in 49 countries. During 2021 there were estimated to be around 25,000 cases XDR-TB worldwide.[137]\n\nIn the past 20 years, other common bacteria including Staphylococcus aureus, Serratia marcescens and Enterococcus, have developed resistance to a wide range of antibiotics. Antibiotic-resistant organisms have become an important cause of healthcare-associated (nosocomial) infections.[138]\n\nThere are two groups of infectious disease that may be affected by climate change. The first group are vector-borne diseases which are transmitted via insects such as mosquitos or ticks.[139] Some of these diseases, such as malaria, yellow fever, and dengue fever, can have potentially severe health consequences. Climate can affect the distribution of these diseases due to the changing geographic range of their vectors, with the potential to cause serious outbreaks in areas where the disease has not previously been known.[140] The other group comprises water-borne diseases such as cholera, dysentery, and typhoid which may increase in prevalence due to changes in rainfall patterns.[141]\n\nThe October 2020 'era of pandemics' report by the United Nations' Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services, written by 22 experts in a variety of fields, said the anthropogenic destruction of biodiversity is paving the way to the pandemic era and could result in as many as 850,000 viruses being transmitted from animals—in particular birds and mammals—to humans. The \"exponential rise\" in consumption and trade of commodities such as meat, palm oil, and metals, largely facilitated by developed nations, and a growing human population, are the primary drivers of this destruction. According to Peter Daszak, the chair of the group who produced the report, \"there is no great mystery about the cause of the Covid-19 pandemic or any modern pandemic. The same human activities that drive climate change and biodiversity loss also drive pandemic risk through their impacts on our environment.\" Proposed policy options from the report include taxing meat production and consumption, cracking down on the illegal wildlife trade, removing high-risk species from the legal wildlife trade, eliminating subsidies to businesses that are harmful to the natural world, and establishing a global surveillance network.[142][143][144]\n\nIn June 2021, a team of scientists assembled by the Harvard Medical School Center for Health and the Global Environment warned that the primary cause of pandemics so far, the anthropogenic destruction of the natural world through such activities including deforestation and hunting, is being ignored by world leaders.[145]\n\nPermafrost covers a fifth of the northern hemisphere and is made up of soil that has been kept at temperatures below freezing for long periods. Viable samples of viruses have been recovered from thawing permafrost, after having been frozen for many years, sometimes for millennia. There is a remote possibility that a thawed pathogen could infect humans or animals.[146][147]\n\nExperts have raised concerns that advances in artificial intelligence could facilitate the design of particularly dangerous pathogens with pandemic potential. They recommended in 2024 that governments implement mandatory oversight and testing requirements.[148]\n\nIn 2016, the commission on a Global Health Risk Framework for the Future estimated that pandemic disease events would cost the global economy over $6 trillion in the 21st century—over $60 billion per year.[149] The same report recommended spending $4.5 billion annually on global prevention and response capabilities to reduce the threat posed by pandemic events, a figure that the World Bank Group raised to $13 billion in a 2019 report.[150] It has been suggested that such costs be paid from a tax on aviation rather than from, e.g., income taxes,[151] given the crucial role of air traffic in transforming local epidemics into pandemics (being the only factor considered in state-of-the-art models of long-range disease transmission [152]).\n\nThe COVID-19 pandemic is expected to have a profound negative effect on the global economy, potentially for years to come, with substantial drops in GDP accompanied by increases in unemployment noted around the world.[49] The slowdown of economic activity early in the COVID-19 pandemic had a profound effect on emissions of pollutants and greenhouse gases.[153][154][155] Analysis of ice cores taken from the Swiss Alps have revealed a reduction in atmospheric lead pollution over a four-year period corresponding to the years 1349 to 1353 (when the Black Death was ravaging Europe), indicating a reduction in mining and economic activity generally.[156]\n"
    }
]